@inproceedings{wobbrock2011effects,
  title={The effects of task dimensionality, endpoint deviation, throughput calculation, and experiment design on pointing measures and models},
  author={Wobbrock, Jacob O and Shinohara, Kristen and Jansen, Alex},
  booktitle={Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  pages={1639--1648},
  year={2011}
}

@article{lock2019bone,
  title={Bone-Conduction Audio Interface to Guide People with Visual Impairments},
  journal={Communications in Computer and Information Science},
  author={J.C. Lock and I.D. Gilchrist and G. Cielniak and N. Bellotto},
  year={2019}
}

@article{kanwal2015navigation,
  title={A navigation system for the visually impaired: a fusion of vision and depth sensor},
  author={Kanwal, Nadia and Bostanci, Erkan and Currie, Keith and Clark, Adrian F},
  journal={Applied bionics and biomechanics},
  year={2015},
  publisher={Hindawi}
}

@inproceedings{schwarze2015intuitive,
  title={An intuitive mobility aid for visually impaired people based on stereo vision},
  author={Schwarze, Tobias and Lauer, Martin and Schwaab, Manuel and Romanovas, Michailas and Bohm, Sandra and Jurgensohn, Thomas},
  booktitle={Int. Conf. on Computer Vision Workshops},
  pages={17--25},
  year={2015}
}

@article{barfield1997visual,
  title={Visual and auditory localization as a function of azimuth and elevation},
  author={Barfield, Woodrow and Cohen, Michael and Rosenberg, Craig},
  journal={Int. Journal of Aviation Psychology},
  volume={7},
  number={2},
  pages={123--138},
  year={1997},
  publisher={Taylor \& Francis}
}

@InProceedings{stanley2006lateralization,
  author       = {Stanley, Raymond M and Walker, Bruce N},
  title        = {Lateralization of sounds using bone-conduction headsets},
  booktitle    = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  year         = {2006},
  volume       = {50},
  number       = {16},
  pages        = {1571--1575},
  organization = {SAGE Publications Sage CA: Los Angeles, CA},
}

@Article{gardner1995hrtf,
  author    = {Gardner, William G and Martin, Keith D},
  title     = {{HRTF} measurements of a {KEMAR}},
  journal   = {The Journal of the Acoustical Society of America},
  year      = {1995},
  volume    = {97},
  number    = {6},
  pages     = {3907--3908},
  publisher = {ASA},
}

@Article{blauert1969sound,
  author    = {Blauert, Jens},
  title     = {Sound localization in the median plane},
  journal   = {Acta Acustica united with Acustica},
  year      = {1969},
  volume    = {22},
  number    = {4},
  pages     = {205--213},
  publisher = {S. Hirzel Verlag},
}

@InProceedings{ammirato2017dataset,
  author       = {Ammirato, Phil and Poirson, Patrick and Park, Eunbyung and Kosecka, Jana and Berg, Alexander C.},
  title        = {A Dataset for Developing and Benchmarking Active Vision},
  booktitle    = {Proceedings of the International Conference on Robotics and Automation (ICRA)},
  year         = {2017},
  organization = {IEEE},
  publisher    = {IEEE},
}

@Article{liu2018deep,
  author  = {Li Liu and Wanli Ouyang and Xiaogang Wang and Paul W. Fieguth and Jie Chen and Xinwang Liu and Matti Pietik{\"a}inen},
  title   = {Deep Learning for Generic Object Detection: A Survey},
  journal = {CoRR},
  year    = {2018},
}

@InProceedings{lin2014microsoft,
  author       = {Lin, Tsung-Yi, et. al.},
  title        = {Microsoft coco: Common objects in context},
  booktitle    = {European conference on computer vision},
  year         = {2014},
  pages        = {740--755},
  organization = {Springer},
}

@InProceedings{liu2016ssd,
  author    = {Wei Liu and Dragomir Anguelov and Dumitru Erhan and Christian Szegedy and Scott E. Reed and Cheng-Yang Fu and Alexander C. Berg},
  title     = {{SSD}: Single Shot MultiBox Detector},
  booktitle = {Proc. of ECCV},
  year      = {2016},
}

@Article{redmon2018yolov3,
  author  = {Joseph Redmon and Ali Farhadi},
  title   = {YOLOv3: An Incremental Improvement},
  journal = {CoRR},
  year    = {2018},
}

@Article{ren2015faster,
  author    = {Shaoqing Ren and Kaiming He and Ross B. Girshick and Jian Sun},
  title     = {Faster {R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
  journal   = {Trans. on Pattern Analysis and Machine Intelligence},
  year      = {2015},
  pages     = {1137-1149},
  publisher = {IEEE},
}

@Article{andrewg.howard2017mobilenets,
  author  = {Andrew G. Howard,Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  title   = {{MobileNets}: Efficient Convolutional Neural Networks for Mobile Vision Applications},
  journal = {CoRR},
  year    = {2017},
}

@InProceedings{sandler2018mobilenetv2,
  author       = {Mark B. Sandler and Andrew G. Howard and Menglong Zhu and Andrey Zhmoginov and Liang-Chieh Chen},
  title        = {{MobileNetV2}: Inverted Residuals and Linear Bottlenecks},
  booktitle    = {Proceedings of the Conference of Computer Vision and Pattern Recognition (CVPR)},
  year         = {2018},
  pages        = {4510-4520},
  organization = {IEEE},
  journal      = {Proceedings of CVPR},
}

@InProceedings{li2018tiny,
  author    = {Yuxi Li and Jiuwei Li and Weiyao Lin and Jianguo Li},
  title     = {{Tiny-DSOD}: Lightweight Object Detection for Resource-Restricted Usages},
  booktitle = {Proceedings of the British Machine Vision Conferencec (BMVC)},
  year      = {2018},
}

@InProceedings{freeman2018effnet,
  author    = {Ido Freeman and Lutz Roese-Koerner and Anton Kummert},
  title     = {Effnet: An Efficient Structure for Convolutional Neural Networks},
  booktitle = {International Conference on Image Processing (ICIP)},
  year      = {2018},
  pages     = {6-10},
  publisher = {IEEE},
  journal   = {2018 25th IEEE International Conference on Image Processing (ICIP)},
}

@Article{zhu2018towards,
  author  = {Xizhou Zhu and Jifeng Dai and Xingchi Zhu and Yichen Wei and Lu Yuan},
  title   = {Towards High Performance Video Object Detection for Mobiles},
  journal = {CoRR},
  year    = {2018},
  volume  = {abs/1804.05830},
}

@InProceedings{shen2017dsod,
  author    = {Zhiqiang Shen and Zhuang Liu and Jianguo Li and Yu-Gang Jiang and Yurong Chen and Xiangyang Xue},
  title     = {DSOD: Learning Deeply Supervised Object Detectors from Scratch},
  booktitle = {Proceedings of the International Conference on Computer Vision (ICCV)},
  year      = {2017},
  pages     = {1937-1945},
  publisher = {IEEE},
  journal   = {2017 IEEE International Conference on Computer Vision (ICCV)},
}

@Misc{bargiacchiai,
  author = {Bargiacchi, Eugenio},
  title  = {AI-Toolbox},
  note   = {https://github.com/Svalorzen/AI-Toolbox},
}

@PhdThesis{bargiacchi2016dynamic,
  author = {Eugenio Bargiacchi},
  title  = {Dynamic Resource Allocation for Multi-Camera Systems},
  school = {University of Amsterdam},
  year   = {2016},
}

@Article{kaelbling1998planning,
  author    = {Kaelbling, Leslie Pack and Littman, Michael L and Cassandra, Anthony R},
  title     = {Planning and acting in partially observable stochastic domains},
  journal   = {Artificial intelligence},
  year      = {1998},
  volume    = {101},
  number    = {1-2},
  pages     = {99--134},
  publisher = {Elsevier},
}

@InProceedings{silver2010monte,
  author    = {Silver, David and Veness, Joel},
  title     = {Monte-Carlo planning in large {POMDPs}},
  booktitle = {Advances in neural information processing systems},
  year      = {2010},
  pages     = {2164--2172},
}

@Article{spaan2005perseus,
  author  = {Spaan, Matthijs TJ and Vlassis, Nikos},
  title   = {Perseus: Randomized point-based value iteration for {POMDPs}},
  journal = {Journal of artificial intelligence research},
  year    = {2005},
  volume  = {24},
  pages   = {195--220},
}

@TechReport{eyes2019be,
  author      = {"Be My Eyes"},
  title       = {"Be My Eyes - Help the Blind"},
  institution = {"Google Play"},
  year        = {"2019"},
  note        = {"https://play.google.com/store/apps/details?id=com.bemyeyes.bemyeyes"},
}

@InProceedings{pineau2003point,
  author    = {Pineau, Joelle and Gordon, Geoff and Thrun, Sebastian and others},
  title     = {Point-based value iteration: An anytime algorithm for {POMDPs}},
  booktitle = {Int. Joint Conf. on Artificial Intelligence Organization},
  year      = {2003},
  pages     = {1025--1032},
}

@InProceedings{lock2019active,
  author    = {J.C. Lock and G. Cielniak and N. Bellotto},
  title     = {Active Object Search with a Mobile Device for People with Visual Impairments},
  booktitle = {Int. Conf. on Computer Vision Theory and Applications},
  year      = {2019},
  pages     = {476-485},
}

@Article{dogar2014object,
  author    = {Dogar, Mehmet R and Koval, Michael C and Tallavajhula, Abhijeet and Srinivasa, Siddhartha S},
  title     = {Object search by manipulation},
  journal   = {Autonomous Robots},
  year      = {2014},
  volume    = {36},
  number    = {1-2},
  pages     = {153--167},
  publisher = {Springer},
}

@InProceedings{aydemir2011search,
  author       = {Aydemir, Alper and Sj{\"o}{\"o}, Kristoffer and Folkesson, John and Pronobis, Andrzej and Jensfelt, Patric},
  title        = {Search in the real world: Active visual object search based on spatial relations},
  booktitle    = {Robotics and Automation (ICRA), 2011 IEEE International Conference on},
  year         = {2011},
  pages        = {2818--2824},
  organization = {IEEE},
}

@Article{kuznetsova2018open,
  author  = {Alina Kuznetsova and Hassan Rom and Neil Alldrin and Jasper R. R. Uijlings and Ivan Krasin and Jordi Pont{-}Tuset and Shahab Kamali and Stefan Popov and Matteo Malloci and Tom Duerig and Vittorio Ferrari},
  title   = {The {Open Images Dataset V4:} Unified image classification, object detection, and visual relationship detection at scale},
  journal = {CoRR},
  year    = {2018},
}

@InProceedings{lock2017portable,
  author    = {J.C. Lock and G. Cielniak and N. Bellotto},
  title     = {Portable Navigations System with Adaptive Multimodal Interface for the Blind},
  booktitle = {{AAAI} Spring Symposium -- Designing the User Experience of Machine Learning Systems},
  year      = {2017},
}

@InProceedings{gonzalez-garcia2015active,
  author    = {Gonzalez-Garcia, Abel and Vezhnevets, Alexander and Ferrari, Vittorio},
  title     = {An active search strategy for efficient object class detection},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2015},
  pages     = {3022--3031},
}

@InProceedings{caicedo2015active,
  author    = {Caicedo, Juan C and Lazebnik, Svetlana},
  title     = {Active object localization with deep reinforcement learning},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  year      = {2015},
  pages     = {2488--2496},
}

@Book{rummery1994line,
  title     = {On-line Q-learning using connectionist systems},
  publisher = {University of Cambridge, Department of Engineering Cambridge, England},
  year      = {1994},
  author    = {Rummery, Gavin A and Niranjan, Mahesan},
  volume    = {37},
}

@Article{krasin2017openimages,
  author  = {Krasin, Ivan and Duerig, Tom and Alldrin, Neil and Ferrari, Vittorio and Abu-El-Haija, Sami and Kuznetsova, Alina and Rom, Hassan and Uijlings, Jasper and Popov, Stefan and Kamali, Shahab and Malloci, Matteo and Pont-Tuset, Jordi and Veit, Andreas and Belongie, Serge and Gomes, Victor and Gupta, Abhinav and Sun, Chen and Chechik, Gal and Cai, David and Feng, Zheyun and Narayanan, Dhyanesh and Murphy, Kevin},
  title   = {OpenImages: A public dataset for large-scale multi-label and multi-class image classification.},
  journal = {CoRR},
  year    = {2017},
}

@InProceedings{bigham2010vizwiz,
  author       = {Bigham, Jeffrey P and Jayant, Chandrika and Miller, Andrew and White, Brandyn and Yeh, Tom},
  title        = {VizWiz:: LocateIt-enabling blind people to locate objects in their environment},
  booktitle    = {Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on},
  year         = {2010},
  pages        = {65--72},
  organization = {IEEE},
}

@Article{bellman1957markovian,
  author    = {Bellman, Richard},
  title     = {A Markovian decision process},
  journal   = {Journal of Mathematics and Mechanics},
  year      = {1957},
  pages     = {679--684},
  publisher = {JSTOR},
}

@InProceedings{bay2006surf,
  author       = {Bay, Herbert and Tuytelaars, Tinne and Van Gool, Luc},
  title        = {Surf: Speeded up robust features},
  booktitle    = {European conference on computer vision},
  year         = {2006},
  pages        = {404--417},
  organization = {Springer},
}

@InProceedings{huang2017speedaccuracy,
  author    = {Huang, Jonathan and Rathod, Vivek and Sun, Chen and Zhu, Menglong and Korattikara, Anoop and Fathi, Alireza and Fischer, Ian and Wojna, Zbigniew and Song, Yang and Guadarrama, Sergio and others},
  title     = {Speed/accuracy trade-offs for modern convolutional object detectors},
  booktitle = {IEEE CVPR},
  year      = {2017},
  volume    = {4},
}

@InProceedings{redmon2016you,
  author    = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  title     = {You only look once: Unified, real-time object detection},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year      = {2016},
  pages     = {779--788},
}

@InProceedings{lowe1999object,
  author       = {Lowe, David G},
  title        = {Object recognition from local scale-invariant features},
  booktitle    = {Computer vision, 1999. The proceedings of the seventh IEEE international conference on},
  year         = {1999},
  volume       = {2},
  pages        = {1150--1157},
  organization = {Ieee},
}

@InProceedings{hub2006interactive,
  author    = {Hub, Andreas and Hartter, Tim and Ertl, Thomas},
  title     = {Interactive localization and recognition of objects for the blind},
  booktitle = {California State University, Northridge Center on Disabilities' 21st Annual International Technology and Persons with Disabilities Conference},
  year      = {2006},
}

@InProceedings{kawai2002support,
  author       = {Kawai, Yoshihiro and Tomita, Fumiaki},
  title        = {A support system for visually impaired persons to understand three-dimensional visual information using acoustic interface},
  booktitle    = {Pattern Recognition, 2002. Proceedings. 16th International Conference on},
  year         = {2002},
  volume       = {3},
  pages        = {974--977},
  organization = {IEEE},
}

@InProceedings{iannizzotto2005badge3d,
  author       = {Iannizzotto, Giancarlo and Costanzo, Carlo and Lanzafame, Pietro and La Rosa, Francesco},
  title        = {Badge3D for visually impaired},
  booktitle    = {Computer Vision and Pattern Recognition-Workshops, 2005. CVPR Workshops. IEEE Computer Society Conference on},
  year         = {2005},
  pages        = {29--29},
  organization = {IEEE},
}

@Article{gude2013blind,
  author  = {Gude, R and {\O}sterby, M and Soltveit, S},
  title   = {Blind navigation and object recognition},
  journal = {Laboratory for Computational Stochastics, University of Aarhus, Denmark},
  year    = {2013},
}

@Article{aloimonos1988active,
  author    = {Aloimonos, John and Weiss, Isaac and Bandyopadhyay, Amit},
  title     = {Active vision},
  journal   = {International journal of computer vision},
  year      = {1988},
  volume    = {1},
  number    = {4},
  pages     = {333--356},
  publisher = {Springer},
}

@InProceedings{song2017semantic,
  author    = {Song, Shuran and Yu, Fisher and Zeng, Andy and Chang, Angel X and Savva, Manolis and Funkhouser, Thomas},
  title     = {Semantic Scene Completion from a Single Depth Image},
  booktitle = {Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2017},
  publisher = {IEEE},
  journal   = {IEEE Conference on Computer Vision and Pattern Recognition},
}

@Article{everingham2010pascal,
  author  = {Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.},
  title   = {The Pascal Visual Object Classes (VOC) Challenge},
  journal = {International Journal of Computer Vision},
  year    = {2010},
  volume  = {88},
  number  = {2},
  pages   = {303--338},
  month   = jun,
}

@Article{watkins1992q,
  author   = {Watkins, Christopher J. C. H. and Dayan, Peter},
  title    = {Q-learning},
  journal  = {Machine Learning},
  year     = {1992},
  volume   = {8},
  number   = {3},
  pages    = {279--292},
  month    = {May},
  issn     = {1573-0565},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  day      = {01},
  doi      = {10.1007/BF00992698},
  url      = {https://doi.org/10.1007/BF00992698},
}

@InProceedings{durette2008visuo,
  author    = {Durette, Barth{\'e}l{\'e}my and Louveton, Nicolas and Alleysson, David and H{\'e}rault, Jeanny},
  title     = {Visuo-auditory sensory substitution for mobility assistance: testing {TheVIBE}},
  booktitle = {Workshop on Computer Vision Applications for the Visually Impaired},
  year      = {2008},
}

@InProceedings{sato2017navcog3,
  author       = {Sato, Daisuke and Oh, Uran and Naito, Kakuya and Takagi, Hironobu and Kitani, Kris and Asakawa, Chieko},
  title        = {Navcog3: An evaluation of a smartphone-based blind indoor navigation assistant with semantic features in a large-scale environment},
  booktitle    = {Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility},
  year         = {2017},
  pages        = {270--279},
  organization = {ACM},
}

@InProceedings{schauerte2012assistive,
  author       = {Schauerte, Boris and Martinez, Manel and Constantinescu, Angela and Stiefelhagen, Rainer},
  title        = {An assistive vision system for the blind that helps find lost things},
  booktitle    = {International Conference on Computers for Handicapped Persons},
  year         = {2012},
  pages        = {566--572},
  organization = {Springer},
}

@InProceedings{vazquez2012helping,
  author       = {V{\'a}zquez, Marynel and Steinfeld, Aaron},
  title        = {Helping visually impaired users properly aim a camera},
  booktitle    = {Proceedings of the 14th international ACM SIGACCESS conference on Computers and accessibility},
  year         = {2012},
  pages        = {95--102},
  organization = {ACM},
}

@InProceedings{fiannaca2014headlock,
  author       = {Fiannaca, Alexander and Apostolopoulous, Ilias and Folmer, Eelke},
  title        = {Headlock: a wearable navigation aid that helps blind cane users traverse large open spaces},
  booktitle    = {Proceedings of the 16th international ACM SIGACCESS conference on Computers \& accessibility},
  year         = {2014},
  pages        = {19--26},
  organization = {ACM},
}

@Article{macdonald2006spatial,
  author    = {MacDonald, Justin A and Henry, Paula P and Letowski, Tomasz R},
  title     = {Spatial audio through a bone conduction interface},
  journal   = {International journal of audiology},
  year      = {2006},
  volume    = {45},
  number    = {10},
  pages     = {595--599},
  publisher = {Taylor \& Francis},
}

@Book{welford1968fundamentals,
  title     = {Fundamentals of skill.},
  publisher = {Methuen},
  year      = {1968},
  author    = {Welford, AT},
}

@Article{wightman1992dominant,
  author    = {Wightman, FL and Kistler, DJ},
  title     = {The dominant role of low-frequency interaural time differences in sound localization},
  journal   = {The Journal of the Acoustical Society of America},
  year      = {1992},
  volume    = {91},
  number    = {3},
  pages     = {1648--1661},
  publisher = {ASA},
}

@Online{kelly2016google,
  author  = {Kelly, H},
  title   = {Google is helping this school build a 'Waze' for the blind},
  year    = {2016},
  url     = {http://cnnmon.ie/23rpGso},
  urldate = {2018-03-09},
}

@Article{wetherill1965sequential,
  author    = {Wetherill, GB and Levitt, H},
  title     = {Sequential estimation of points on a psychometric function},
  journal   = {British Journal of Mathematical and Statistical Psychology},
  year      = {1965},
  volume    = {18},
  number    = {1},
  pages     = {1--10},
  publisher = {Wiley Online Library},
}

@Article{levitt1971transformed,
  author    = {Levitt, HCCH},
  title     = {Transformed up-down methods in psychoacoustics},
  journal   = {The Journal of the Acoustical society of America},
  year      = {1971},
  volume    = {49},
  number    = {2B},
  pages     = {467--477},
  publisher = {ASA},
}

@InProceedings{menelas2010audio,
  author       = {Menelas, B and Picinalli, L and Katz, BFG and Bourdot, P},
  title        = {Audio haptic feedbacks for an acquisition task in a multi-target context},
  booktitle    = {3D User Interfaces (3DUI), 2010 IEEE Symposium on},
  year         = {2010},
  pages        = {51--54},
  organization = {IEEE},
}

@Article{golledge1998geographical,
  author    = {Golledge, RG and Klatzky, RL and Loomis, JM and Speigle, J and Tietz, J},
  title     = {A geographical information system for a GPS based personal guidance system},
  journal   = {International Journal of Geographical Information Science},
  year      = {1998},
  volume    = {12},
  number    = {7},
  pages     = {727--749},
  publisher = {Taylor \& Francis},
}

@Article{loomis1998navigation,
  author    = {Loomis, JM and Golledge, RG and Klatzky, RL},
  title     = {Navigation system for the blind: Auditory display modes and guidance},
  journal   = {Presence: Teleoperators and Virtual Environments},
  year      = {1998},
  volume    = {7},
  number    = {2},
  pages     = {193--203},
  publisher = {MIT Press},
}

@Book{blauert1997spatial,
  title     = {Spatial hearing: the psychophysics of human sound localization},
  publisher = {MIT press},
  year      = {1997},
  author    = {Blauert, J},
}

@InProceedings{ross2008bayesian,
  author       = {Ross, S and Chaib-draa, B and Pineau, J},
  title        = {Bayesian reinforcement learning in continuous POMDPs with application to robot navigation},
  booktitle    = {Robotics and Automation, 2008. ICRA 2008. IEEE International Conference on},
  year         = {2008},
  pages        = {2845--2851},
  organization = {IEEE},
}

@TechReport{rnib2016uk,
  author       = {RNIB},
  title        = {UK Vision Strategy},
  institution  = {RNIB},
  year         = {2016},
  note         = {Accessed: 19-07-2016},
  howpublished = {\url{http://www.rnib.org.uk/about-rnib-what-we-do/uk-vision-strategy}},
}

@Book{jagacinski2003control,
  title     = {{Control Theory for Humans}},
  publisher = {{Laurence Erlbaum Associates, Inc.}},
  year      = {2003},
  author    = {Jagacinski, JR and Flach, MJ},
}

@TechReport{who2010global,
  author      = {{World Health Organisation}},
  title       = {{Global Data on Visual Impairment}},
  institution = {{World Health Organisation}},
  year        = {2010},
}

@Conference{willis2005rfid,
  author    = {Willis, S and Helal, S},
  title     = {{RFID Information Grid for Blind Navigation and Wayfinding}},
  booktitle = {Proceedings of the 9th IEEE International Symposium on Wearable Computers},
  year      = {2005},
  pages     = {34 - 37},
}

@Article{dossantosalves2014multimodal,
  author  = {{Dos Santos Alves}, R and Soares de Souza, A},
  title   = {{Multimodal and Alternative Perception for the Visually Impaired: A Survey Structured}},
  journal = {Igarss},
  year    = {2014},
  pages   = {1 - 5},
}

@Unpublished{daecost,
  author = {{Deloitte Access Economics}},
  title  = {{The Cost of Blindness}},
  note   = {Report, to be published in 2016},
}

@InProceedings{merel2013multi,
  author    = {Merel, J and Fox, R and Jebara, T and Paninski, L},
  title     = {A Multi-agent Control Framework for Co-adaptation in Brain-computer Interfaces},
  booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems},
  year      = {2013},
  series    = {NIPS'13},
  pages     = {2841--2849},
  address   = {USA},
  publisher = {Curran Associates Inc.},
  acmid     = {2999929},
  location  = {Lake Tahoe, Nevada},
  numpages  = {9},
  url       = {http://dl.acm.org/citation.cfm?id=2999792.2999929},
}

@InBook{dixon2012human,
  pages     = {137--155},
  title     = {Human Factors in Reliable Design},
  publisher = {John Wiley \& Sons, Inc.},
  year      = {2012},
  author    = {Dixon, J},
  booktitle = {Design for Reliability},
}

@InProceedings{jokela2003standard,
  author    = {Jokela, T and Iivari, N and Matero, J and Karukka, M},
  title     = {The Standard of User-centered Design and the Standard Definition of Usability: Analyzing ISO 13407 Against ISO 9241-11},
  booktitle = {Proceedings of the Latin American Conference on Human-computer Interaction},
  year      = {2003},
  series    = {CLIHC '03},
  pages     = {53--60},
  location  = {Rio de Janeiro, Brazil},
  numpages  = {8},
}

@InProceedings{faria2010electronic,
  author       = {Faria, J and Lopes, S and Fernandes, H and Martins, P and Barroso, J},
  title        = {Electronic white cane for blind people navigation assistance},
  booktitle    = {World Automation Congress (WAC), 2010},
  year         = {2010},
  pages        = {1--7},
  organization = {IEEE},
}

@InProceedings{ran2004drishti,
  author       = {Ran, L and Helal, S and Moore, S},
  title        = {Drishti: an integrated indoor/outdoor blind navigation system and service},
  booktitle    = {Pervasive Computing and Communications, 2004. PerCom 2004. Proceedings of the Second IEEE Annual Conference on},
  year         = {2004},
  pages        = {23--30},
  organization = {IEEE},
}

@InProceedings{ross2000wearable,
  author       = {Ross, D A and Blasch, BB},
  title        = {Wearable interfaces for orientation and wayfinding},
  booktitle    = {Proceedings of the fourth international ACM conference on Assistive technologies},
  year         = {2000},
  pages        = {193--200},
  organization = {ACM},
}

@Manual{hiebert2005openal,
  title  = {{OpenAL 1.1 Specification and Reference}},
  author = {Hiebert, Garin},
  year   = {2005},
}

@Article{pratt1930spatial,
  author    = {Pratt, CC},
  title     = {The spatial character of high and low tones.},
  journal   = {Journal of Experimental Psychology},
  year      = {1930},
  volume    = {13},
  number    = {3},
  pages     = {278},
  publisher = {Psychological Review Company},
}

@Article{shepard1964circularity,
  author    = {Shepard, RN},
  title     = {Circularity in judgments of relative pitch},
  journal   = {The Journal of the Acoustical Society of America},
  year      = {1964},
  volume    = {36},
  number    = {12},
  pages     = {2346--2353},
  publisher = {ASA},
}

@Article{roffler1968factors,
  author    = {Roffler, SK and Butler, RA},
  title     = {Factors that influence the localization of sound in the vertical plane},
  journal   = {The Journal of the Acoustical Society of America},
  year      = {1968},
  volume    = {43},
  number    = {6},
  pages     = {1255--1259},
  publisher = {ASA},
}

@Article{mackenzie1992fitts,
  author    = {MacKenzie, IS},
  title     = {Fitts' law as a research and design tool in human-computer interaction},
  journal   = {Human-computer interaction},
  year      = {1992},
  volume    = {7},
  number    = {1},
  pages     = {91--139},
  publisher = {L. Erlbaum Associates Inc.},
}

@Article{fitts1954information,
  author    = {Fitts, PM},
  title     = {The information capacity of the human motor system in controlling the amplitude of movement.},
  journal   = {Journal of experimental psychology},
  year      = {1954},
  volume    = {47},
  number    = {6},
  pages     = {381},
  publisher = {American Psychological Association},
}

@Article{kruskal1952use,
  author    = {Kruskal, WH and Wallis, WA},
  title     = {Use of ranks in one-criterion variance analysis},
  journal   = {Journal of the American statistical Association},
  year      = {1952},
  volume    = {47},
  number    = {260},
  pages     = {583--621},
  publisher = {Taylor \& Francis},
}

@Book{mood1950introduction,
  title     = {Introduction to the Theory of Statistics.},
  publisher = {McGraw-hill},
  year      = {1950},
  author    = {Mood, AM},
  pages     = {394 - 399},
}

@Article{brewster1998using,
  author    = {Brewster, SA},
  title     = {Using nonspeech sounds to provide navigation cues},
  journal   = {ACM Transactions on Computer-Human Interaction (TOCHI)},
  year      = {1998},
  volume    = {5},
  number    = {3},
  pages     = {224--259},
  publisher = {ACM},
}

@Article{friedman1937use,
  author    = {Friedman, M},
  title     = {The use of ranks to avoid the assumption of normality implicit in the analysis of variance},
  journal   = {Journal of the american statistical association},
  year      = {1937},
  volume    = {32},
  number    = {200},
  pages     = {675--701},
  publisher = {Taylor \& Francis},
}

@Article{bryan2013probabilistic,
  author   = {Bryan, Matthew J and Martin, Stefan A and Cheung, Willy and Rao, Rajesh P N},
  title    = {{Probabilistic co-adaptive brain–computer interfacing}},
  journal  = {Journal of Neural Engineering},
  year     = {2013},
  volume   = {10},
  number   = {6},
  pages    = {066008},
  issn     = {1741-2560},
  abstract = {OBJECTIVE: Brain-computer interfaces (BCIs) are confronted with two fundamental challenges: (a) the uncertainty associated with decoding noisy brain signals, and (b) the need for co-adaptation between the brain and the interface so as to cooperatively achieve a common goal in a task. We seek to mitigate these challenges.$\backslash$n$\backslash$nAPPROACH: We introduce a new approach to brain-computer interfacing based on partially observable Markov decision processes (POMDPs). POMDPs provide a principled approach to handling uncertainty and achieving co-adaptation in the following manner: (1) Bayesian inference is used to compute posterior probability distributions ('beliefs') over brain and environment state, and (2) actions are selected based on entire belief distributions in order to maximize total expected reward; by employing methods from reinforcement learning, the POMDP's reward function can be updated over time to allow for co-adaptive behaviour.$\backslash$n$\backslash$nMAIN RESULTS: We illustrate our approach using a simple non-invasive BCI which optimizes the speed-accuracy trade-off for individual subjects based on the signal-to-noise characteristics of their brain signals. We additionally demonstrate that the POMDP BCI can automatically detect changes in the user's control strategy and can co-adaptively switch control strategies on-the-fly to maximize expected reward.$\backslash$n$\backslash$nSIGNIFICANCE: Our results suggest that the framework of POMDPs offers a promising approach for designing BCIs that can handle uncertainty in neural signals and co-adapt with the user on an ongoing basis. The fact that the POMDP BCI maintains a probability distribution over the user's brain state allows a much more powerful form of decision making than traditional BCI approaches, which have typically been based on the output of classifiers or regression techniques. Furthermore, the co-adaptation of the system allows the BCI to make online improvements to its behaviour, adjusting itself automatically to the user's changing circumstances.},
  doi      = {10.1088/1741-2560/10/6/066008},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bryan et al. - 2013 - Probabilistic co-adaptive brain–computer interfacing.pdf:pdf},
  isbn     = {1741-2560},
  pmid     = {24140680},
  url      = {http://stacks.iop.org/1741-2552/10/i=6/a=066008?key=crossref.90e9929bc46f6853495c39821f57cbb4},
}

@Article{digiovanna2008changing,
  author = {DIGIOVANNA, JOHN F .},
  title  = {{CHANGING THE BRAIN-MACHINE INTERFACE PARADIGM : CO-ADAPTATION BASED ON REINFORCEMENT LEARNING}},
  year   = {2008},
  file   = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/DIGIOVANNA - 2008 - CHANGING THE BRAIN-MACHINE INTERFACE PARADIGM CO-ADAPTATION BASED ON REINFORCEMENT LEARNING.pdf:pdf},
}

@InProceedings{irwansyah2017application,
  author    = {Irwansyah and Usagawa, Tsuyoshi},
  title     = {{Application of active control technique on a bone conduction headphone for estimating a cross-talk compensation filter}},
  booktitle = {Proceedings of the Region 10 Annual International Conference},
  year      = {2017},
  volume    = {2017-Decem},
  pages     = {3099--3104},
  publisher = {IEEE},
  abstract  = {{\textcopyright} 2017 IEEE. When presenting a stereo sound over a pair of transducers on a bone conduction headphone, the sound wave coming from either the left or right side transducer is delivered directly to the cochlea in both ears. This mechanism is often referred to as 'cross-talk,' and it may decrease the ability to sense sound direction perceived through the bone conduction headphone. In this paper, we investigate the application of active control technique on a bone conduction headphone for estimating a cross-talk compensation filter. To reduce perceived loudness in one of the ears, transfer functions on routes from transducers to a cochlea are estimated by using a microphone placed in the outer ear canal under the assumption that otoacoustic emission (OAE) emitted by the cochlea in response to a bone-conducted sound stimulus can be recorded. By using the transfer functions, the compensation filter which produces anti-sound can be obtained as a result of the filtered-x least mean square (FXLMS) algorithm. In this study, sound stimuli with and without cancellation were presented through a bone conduction headphone where three subjects with normal hearing participated in the measurement. Experimental results show that sound reduction was achieved at the location of the microphone and all participants also reported that they could feel a decrease in the perceived loudness.},
  doi       = {10.1109/TENCON.2017.8228394},
  file      = {:home/jaycee/PhD Papers/08228394.pdf:pdf},
  isbn      = {9781509011339},
  issn      = {21593450},
}

@Article{hoey2006tracking,
  author   = {Hoey, J.},
  title    = {{Tracking using Flocks of Features, with Application to Assisted Handwashing}},
  journal  = {Procedings of the British Machine Vision Conference 2006},
  year     = {2006},
  pages    = {38.1--38.10},
  abstract = {This paper describes a method for tracking in the presence of distractors, changes in shape, and occlusions. An object is modeled as a flock of features describing its approximate shape. The flocks dynamics keep it spatially localised and moving in concert, but also well distributed across the object being tracked. A recursive Bayesian estimation of the density of the object is approximated with a set of samples. The method is demonstrated on two simple examples, and is applied to an assistive system that tracks the hands and the towel during a handwashing task},
  doi      = {10.5244/C.20.38},
  file     = {:home/jaycee/PhD Papers/hoeybmvc06.pdf:pdf},
  isbn     = {1-901725-32-4},
  url      = {http://www.bmva.org/bmvc/2006/papers/167.html},
}

@InProceedings{ye2018active,
  author    = {Ye, Xin and Lin, Zhe and Li, Haoxiang and Zheng, Shibin and Yang, Yezhou},
  title     = {{Active Object Perceiver: Recognition-Guided Policy Learning for Object Searching on Mobile Robots}},
  booktitle = {Proceedings of the International Conference on Intelligent Robots and Systems},
  year      = {2018},
  pages     = {6857--6863},
  publisher = {IEEE},
  abstract  = {We study the problem of learning a navigation policy for a robot to actively search for an object of interest in an indoor environment solely from its visual inputs. While scene-driven visual navigation has been widely studied, prior efforts on learning navigation policies for robots to find objects are limited. The problem is often more challenging than target scene finding as the target objects can be very small in the view and can be in an arbitrary pose. We approach the problem from an active perceiver perspective, and propose a novel framework that integrates a deep neural network based object recognition module and a deep reinforcement learning based action prediction mechanism. To validate our method, we conduct experiments on both a simulation dataset (AI2-THOR) and a real-world environment with a physical robot. We further propose a new decaying reward function to learn the control policy specific to the object searching task. Experimental results validate the efficacy of our method, which outperforms competing methods in both average trajectory length and success rate.},
  doi       = {10.1109/IROS.2018.8593720},
  file      = {:home/jaycee/PhD Papers/08593720.pdf:pdf},
  isbn      = {9781538680940},
  issn      = {21530866},
  journal   = {IEEE International Conference on Intelligent Robots and Systems},
}

@Article{flynn2010converging,
  author   = {Flynn, Peter and Yingling, Jessica and Shoemaker, Dan},
  title    = {{Converging technologies to enable induced pluripotent stem cells in drug discovery}},
  journal  = {Regenerative Medicine},
  year     = {2010},
  volume   = {5},
  number   = {4},
  pages    = {489--491},
  issn     = {1746-0751},
  abstract = {In the early decades of the twenty-first century, concentrated efforts can unify science based on the unity of nature, thereby advancing the combination of nanotechnology, biotechnology, information technology, and new technologies based in cognitive science. With proper attention to ethical issues and societal needs, converging technologies could achieve a tremendous improvement in human abilities, societal outcomes, the nation's productivity, and the quality of life. This is a broad, cross- cutting, emerging and timely opportunity of interest to individuals, society and humanity in the long term. The phrase “convergent technologies” refers to the synergistic combination of four major “NBIC” (nano-bio-info-cogno) provinces of science and technology, each of which is currently progressing at a rapid rate: (a) nanoscience and nanotechnology; (b) biotechnology and biomedicine, including genetic engineering; (c) information technology, including advanced computing and communications; (d) cognitive science, including cognitive neuroscience.},
  doi      = {10.2217/rme.10.48},
  file     = {:home/jaycee/PhD Papers/NBIC{\_}pre{\_}publication.pdf:pdf},
}

@Article{velazquez2010wearable,
  author   = {Vel{\'{a}}zquez, Ramiro},
  title    = {{Wearable assistive devices for the blind}},
  journal  = {Lecture Notes in Electrical Engineering},
  year     = {2010},
  volume   = {75 LNEE},
  pages    = {331--349},
  issn     = {18761100},
  abstract = {Abstract Assistive devices are a key aspect in wearable systems for biomedical applications, as they represent potential aids for people with physical and sensory disabilities that might lead to improvements in the quality of life. This chapter focuses on wearable assistive ... $\backslash$n},
  doi      = {10.1007/978-3-642-15687-8-17},
  file     = {:home/jaycee/PhD Papers/10.1007@978-3-642-15687-817.pdf:pdf},
  isbn     = {9783642156861},
  keywords = {assistive technology,reading/mobility aids,wearable devices and systems},
}

@Article{rodriguez-sanchez2014accessible,
  author    = {Rodriguez-Sanchez, M. C. and Moreno-Alvarez, M. A. and Martin, E. and Borromeo, S. and Hernandez-Tamames, J. A.},
  title     = {{Accessible smartphones for blind users: A case study for a wayfinding system}},
  journal   = {Expert Systems with Applications},
  year      = {2014},
  volume    = {41},
  number    = {16},
  pages     = {7210--7222},
  issn      = {09574174},
  abstract  = {While progress on assistive technologies have been made, some blind users still face several problems opening and using basic functionalities when interacting with touch interfaces. Sometimes, people with visual impairments may also have problems navigating autonomously, without personal assistance, especially in unknown environments. This paper presents a complete solution to manage the basic functions of a smartphone and to guide users using a wayfinding application. This way, a blind user could go to work from his home in an autonomous way using an adaptable wayfinding application on his smartphone. The wayfinding application combines text, map, auditory and tactile feedback for providing the information. Eighteen visually impaired users tested the application. Preliminary results from this study show that blind people and limited vision users can effectively use the wayfinding application without help. The evaluation also confirms the usefulness of extending the vibration feedback to convey distance information as well as directional information. The validation was successful for iOS and Android devices. {\textcopyright} 2014 Elsevier Ltd. All rights reserved.},
  doi       = {10.1016/j.eswa.2014.05.031},
  file      = {:home/jaycee/PhD Papers/1-s2.0-S095741741400311X-main.pdf:pdf},
  keywords  = {Accessibility,Touch screens,User interfaces,Visually impaired users,Wayfinding},
  publisher = {Elsevier Ltd},
  url       = {http://dx.doi.org/10.1016/j.eswa.2014.05.031},
}

@Article{pohlmeyer2014using,
  author   = {Pohlmeyer, Eric A. and Mahmoudi, Babak and Geng, Shijia and Prins, Noeline W. and Sanchez, Justin C.},
  title    = {{Using reinforcement learning to provide stable brain-machine interface control despite neural input reorganization}},
  journal  = {PLoS ONE},
  year     = {2014},
  volume   = {9},
  number   = {1},
  issn     = {19326203},
  abstract = {Brain-machine interface (BMI) systems give users direct neural control of robotic, communication, or functional electrical stimulation systems. As BMI systems begin transitioning from laboratory settings into activities of daily living, an important goal is to develop neural decoding algorithms that can be calibrated with a minimal burden on the user, provide stable control for long periods of time, and can be responsive to fluctuations in the decoder's neural input space (e.g. neurons appearing or being lost amongst electrode recordings). These are significant challenges for static neural decoding algorithms that assume stationary input/output relationships. Here we use an actor-critic reinforcement learning architecture to provide an adaptive BMI controller that can successfully adapt to dramatic neural reorganizations, can maintain its performance over long time periods, and which does not require the user to produce specific kinetic or kinematic activities to calibrate the BMI. Two marmoset monkeys used the Reinforcement Learning BMI (RLBMI) to successfully control a robotic arm during a two-target reaching task. The RLBMI was initialized using random initial conditions, and it quickly learned to control the robot from brain states using only a binary evaluative feedback regarding whether previously chosen robot actions were good or bad. The RLBMI was able to maintain control over the system throughout sessions spanning multiple weeks. Furthermore, the RLBMI was able to quickly adapt and maintain control of the robot despite dramatic perturbations to the neural inputs, including a series of tests in which the neuron input space was deliberately halved or doubled.},
  doi      = {10.1371/journal.pone.0087253},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pohlmeyer et al. - 2014 - Using reinforcement learning to provide stable brain-machine interface control despite neural input reorganiza.pdf:pdf},
  isbn     = {10.1371/journal.pone.0087253},
  pmid     = {24498055},
}

@Article{vera2014smartphone,
  author   = {Vera, Pablo and Zenteno, Daniel and Salas, Joaqu{\'{i}}n},
  title    = {{A smartphone-based virtual white cane}},
  journal  = {Pattern Analysis and Applications},
  year     = {2014},
  volume   = {17},
  number   = {3},
  pages    = {623--632},
  issn     = {14337541},
  abstract = {The objective of our research was to develop assistive technology for visually impaired people, with a high appreciation for the human potential to achieve, to learn, and to achieve goals. In this document, we describe a virtual white cane made of a combination of a Smartphone and a laser pointer. In our device, the laser pointer beam reflection is captured by the Smartphone camera. The distance from the virtual white cane to the reflection is computed through active triangulation. Then, a personalized vibration, the magnitude of which corresponds to distance, is generated in the Smartphone. In this way, the users receive information that could prevent collisions with obstacles in the environment. Our contributions include the development of a virtual white cane around a Smartphone and other off-the-shelf accessories and a methodology to provide personalized vibratory feedback to the user. Our experiments show that to navigate, our instrument is better option, in terms of travel time, that the use of the hands. However, the travel time is still better using a traditional white cane than our instrument. {\textcopyright} 2013 Springer-Verlag London.},
  doi      = {10.1007/s10044-013-0328-8},
  file     = {:home/jaycee/PhD Papers/Vera2014{\_}Article{\_}ASmartphone-basedVirtualWhiteC.pdf:pdf},
  isbn     = {1004401303288},
  keywords = {Electronic Travel Aids,Smartphones-based computer vision,Virtual White Cane},
}

@Article{jafri2017visual,
  author    = {Jafri, Rabia and Campos, Rodrigo Louzada and Ali, Syed Abid and Arabnia, Hamid R.},
  title     = {{Visual and Infrared Sensor Data-Based Obstacle Detection for the Visually Impaired Using the Google Project Tango Tablet Development Kit and the Unity Engine}},
  journal   = {IEEE Access},
  year      = {2017},
  volume    = {6},
  pages     = {443--454},
  issn      = {21693536},
  abstract  = {{\textcopyright} 2013 IEEE. A novel visual and infrared sensor data-based system to assist visually impaired users in detecting obstacles in their path while independently navigating indoors is presented. The system has been developed for the recently introduced Google Project Tango Tablet Development Kit equipped with a powerful graphics processor and several sensors which allow it to track its motion and orientation in 3-D space in real-Time. It exploits the inbuilt functionalities of the Unity engine in the Tango SDK to create a 3-D reconstruction of the surrounding environment, then associates a Unity collider component with the user and utilizes it to determine his interaction with the reconstructed mesh in order to detect obstacles. The user is warned about any detected obstacles via audio alerts. An extensive empirical evaluation of the obstacle detection component has yielded favorable results, thus, confirming the potential of this system for future development work.},
  doi       = {10.1109/ACCESS.2017.2766579},
  file      = {:home/jaycee/PhD Papers/08082793.pdf:pdf},
  keywords  = {Project Tango,Unity,Visually impaired,assistive technologies,blind,multimodal sensors,navigation,obstacle avoidance,obstacle detection},
  publisher = {IEEE},
}

@Article{yusif2016older,
  author    = {Yusif, Salifu and Soar, Jeffrey and Hafeez-Baig, Abdul},
  title     = {{Older people, assistive technologies, and the barriers to adoption: A systematic review}},
  journal   = {International Journal of Medical Informatics},
  year      = {2016},
  volume    = {94},
  pages     = {112--116},
  issn      = {18728243},
  abstract  = {Background Older people generally prefer to continue living in their own homes rather than move into residential age care institutions. Assistive technologies and sensors in the home environment and/or bodily worn systems that monitor people's movement might contribute to an increased sense of safety and security at home. However, their use can raise ethical anxieties as little is known about how older persons perceive assistive and monitoring technologies. Objectives To review the main barriers to the adoption of assistive technologies (ATs) by older adults in order to uncover issues of concern from empirical studies and to arrange these issues from the most critical to the least critical. Method A 4-step systematic review was conducted using empirical studies: locating and identifying relevant articles; screening of located articles; examination of full text articles for inclusion/exclusion; and detail examination of the 44 articles included. Results Privacy is a top critical concern to older adults, registering a 34{\%} of the total articles examined. Two other equally potent barriers to the adoption of ATs were trust and functionality/added value representing 27 and 25 per cent each respectively of the total studies examined. Also of serious concerns are cost of ATs and ease of use and suitability for daily use (23{\%}) each respectively, perception of “no need” (20{\%}), stigma (18{\%}), and fear of dependence and lack of training (16{\%}) each respectively. These underlying factors are generation/cohort effects and physical decline relating to aging, and negative attitudes toward technologies such as the so-called “gerontechnologies” specifically targeting older adults. However, more and more older adults adopt different kinds of ATs in order to fit in with the society. Conclusions The identified underlying factors are generation/cohort effects and physical decline relating to aging, and negative attitudes toward technologies. The negative attitudes that are most frequently associated with technologies such as the so-called “gerontechnologies” specifically targeting older adults contain stigmatizing symbolism that might prevent them from adopting them.},
  file      = {:home/jaycee/PhD Papers/1-s2.0-S1386505616301551-main.pdf:pdf},
  keywords  = {Assistive/home monitoring technologies,Ethical issues,ICT,Older adults},
  publisher = {Elsevier Ireland Ltd},
}

@InProceedings{sathyanarayanan2017iot,
  author    = {Sathyanarayanan, E. and {Gokul Deepan}, D. and Nithin, B. P. and Vidhyasagar, P.},
  title     = {{IoT based smart walking cane for typhlotic with voice assistance}},
  booktitle = {Proceedings of the Online International Conference on Green Engineering and Technologies},
  year      = {2017},
  pages     = {1--6},
  publisher = {IEEE},
  abstract  = {Locomotion is inevitable for human beings. The main problem for visually challenged is movement from one place to another, level crossing and identification of their current location. In case of their venture into unfamiliar environment it is a menace for them and for their fellow mates too. Therefore it is essential for the family members, organization of visually challenged to know the current location of the visually challenged. Smart walking cane acts an Electronic Travel Aid (ETA). Smart walking cane solves the challenge by detecting both indoor and outdoor obstacles and by providing level crossing guidance and current position of the visually challenged. Obstacles are detected using Ultrasonic sensors; Global Positioning System (GPS) is used to indicate the current location of the visually challenged; level crossing guidance is provided by Reflective Infra Red (RIR) sensor. The current position of visually challenged is uploaded to cloud through Wi-Fi module. Navigation information and instructions is intimated to the user of the Smart walking cane by text-to-speech converter through stereophonic Headphone. Buzzer and Vibratory circuit assists to find the distance in noisy environment too. This Smart walking cane meets the mandatory requirements of the blind people to navigate as well as to know the position and orientation of the visually challenged. {\&}copy; 2016 IEEE.},
  doi       = {10.1109/GET.2016.7916687},
  file      = {:home/jaycee/PhD Papers/07916687.pdf:pdf},
  isbn      = {9781509045563},
  journal   = {Proceedings of 2016 Online International Conference on Green Engineering and Technologies, IC-GET 2016},
  keywords  = {ETA,GPS,RIR,Travel Aid for the Blind,Ultrasonic Sensor,Wi-Fi},
}

@Article{mocanu2016when,
  author   = {Mocanu, Bogdan and Tapu, Ruxandra and Zaharia, Titus},
  title    = {{When ultrasonic sensors and computer vision join forces for efficient obstacle detection and recognition}},
  journal  = {Sensors},
  year     = {2016},
  volume   = {16},
  number   = {11},
  issn     = {14248220},
  abstract = {{\"{i}}¿½ 2016 by the authors; licensee MDPI, Basel, Switzerland. In the most recent report published by theWorld Health Organization concerning people with visual disabilities it is highlighted that by the year 2020, worldwide, the number of completely blind people will reach 75 million, while the number of visually impaired (VI) people will rise to 250 million. Within this context, the development of dedicated electronic travel aid (ETA) systems, able to increase the safe displacement of VI people in indoor/outdoor spaces, while providing additional cognition of the environment becomes of outmost importance. This paper introduces a novel wearable assistive device designed to facilitate the autonomous navigation of blind and VI people in highly dynamic urban scenes. The system exploits two independent sources of information: ultrasonic sensors and the video camera embedded in a regular smartphone. The underlying methodology exploits computer vision and machine learning techniques and makes it possible to identify accurately both static and highly dynamic objects existent in a scene, regardless on their location, size or shape. In addition, the proposed system is able to acquire information about the environment, semantically interpret it and alert users about possible dangerous situations through acoustic feedback. To determine the performance of the proposed methodology we have performed an extensive objective and subjective experimental evaluation with the help of 21 VI subjects from two blind associations. The users pointed out that our prototype is highly helpful in increasing the mobility, while being friendly and easy to learn.},
  file     = {:home/jaycee/PhD Papers/sensors{\_}16{\_}01807{\_}pdf.pdf:pdf},
  isbn     = {3316076409},
  keywords = {Acoustic feedback,Computer vision techniques,Machine learning algorithms,Object recognition,Obstacle detection,Ultrasonic network,Wearable assistive device},
}

@Article{marston2019nonvisual,
  author   = {Marston, James R. and Loomis, Jack M. and Klatzky, Roberta L. and Golledge, Reginald G.},
  title    = {{Nonvisual Route following with Guidance from a Simple Haptic or Auditory Display}},
  journal  = {Journal of Visual Impairment {\&} Blindness},
  year     = {2019},
  volume   = {101},
  number   = {4},
  pages    = {203--211},
  issn     = {0145-482X},
  abstract = {A path-following experiment, using a global positioning system, was conducted with participants who were legally blind. On- and off-course confir- mations were delivered by either a vibrotactile or an audio stimulus. These simple binary cues were sufficient for guidance and point to the need to offer output options for guidance systems for people who are visually impaired.},
  file     = {:home/jaycee/PhD Papers/0145482x0710100403.pdf:pdf},
}

@Article{vorapatratorn2014isonar,
  author   = {Vorapatratorn, Surapol and Nambunmee, Kowit},
  title    = {{iSonar: an obstacle warning device for the totally blind}},
  journal  = {Journal of Assistive, Rehabilitative {\&} Therapeutic Technologies},
  year     = {2014},
  volume   = {2},
  number   = {1},
  pages    = {23114},
  abstract = {Existing electronic travel aids have not been widely used by blind communities in Thailand due to their low performance, unattractive appearance, impracticality, and high cost. This paper proposes iSonar-a miniaturized, high performance, and low cost obstacle warning device for the blind. Ultrasonic transducer is used to detect obstacles by providing tactile feedback in different vibration frequencies at head and upper body levels to avoid collision. Our prototype devices have been tested with fifteen blind volunteers with simulated obstacles. Experimental results showed that iSonar reduced collision rates from 33.33 percent to 6.67 percent.},
  doi      = {10.3402/jartt.v2.23114},
  file     = {:home/jaycee/PhD Papers/iSonar an obstacle warning device for the totally blind.pdf:pdf},
  keywords = {11 june 2014,15 may 2014,19 october 2013,27 april 2014,ac,accepted,chiang rai 57100,correspondence to,email,impairments,mae fah luang university,mfu,obstacle detection,published,received,revised,school of information technology,surapol,surapol vorapatratorn,th,thailand,travel aid for visual,ultrasonic transducer,visually impaired,vor},
}

@Article{fee2015annual,
  author = {Fee, Non-sterling Cash},
  title  = {{An annual summary of your account charges}},
  year   = {2015},
  number = {14259},
  pages  = {0--3},
  file   = {:home/jaycee/PhD Papers/Statement.pdf:pdf},
}

@Article{younghoonlee2016computer,
  author   = {B, Young Hoon Lee},
  title    = {{Computer Vision – ECCV 2016 Workshops}},
  year     = {2016},
  volume   = {9913},
  pages    = {493--508},
  abstract = {Visual odometry has been used in many fields, especially in robotics and intelligent vehicles. Since local descriptors are robust to background clutter, occlusion and other content variations, they have been receiving more and more attention in the application of the detector-descriptor based visual odometry. To our knowledge, however, there is no extensive, comparative evaluation inves-tigating the performance of the detector-descriptor based methods in the sce-nario of monocular visual-IMU (Inertial Measurement Unit) odometry. In this paper, we therefore perform such an evaluation under a unified framework. We select five typical routes from the challenging KITTI dataset by taking into account the length and shape of routes, the impact of independent motions due to other vehicles and pedestrians. In terms of the five routes, we conduct five different experiments in order to assess the performance of different combina-tions of salient point detector and local descriptor in various road scenes, respectively. The results obtained in this study potentially provide a series of guidelines for the selection of salient point detectors and local descriptors.},
  doi      = {10.1007/978-3-319-46604-0},
  file     = {:home/jaycee/PhD Papers/lee2015.pdf:pdf},
  isbn     = {978-3-319-46603-3},
  url      = {http://link.springer.com/10.1007/978-3-319-46604-0},
}

@Article{hAkansson1984hearing,
  author   = {H{\^{A}}Kansson, B. and Tjellstr{\"{o}}m, A. and Rosenhal, U.},
  title    = {{Hearing Thresholds with Direct Bone Conduction Versus Conventional Bone Conduction}},
  journal  = {Scandinavian Audiology},
  year     = {1984},
  volume   = {13},
  number   = {1},
  pages    = {3--13},
  issn     = {01050397},
  abstract = {Some patients who need hearing aids are unable to use an aid which transmits the sound via the external ear canal but have to use a bone-conduction hearing aid. The pressure needed to apply the transducer often gives the patient discomfort, and the attenuating effect of the skin gives poor electroacoustical function of the aid. A permanent skin penetration has made it possible to develop a bone-anchored hearing aid with all components in one housing. Ten patients have been equipped with such an aid. This paper deals with a comparative hearing threshold measurement on 10 patients. B{\'{e}}k{\'{e}}sy audiometry was performed and a conventional Oticon (A-type) transducer was used. In the frequency range 600 to 6 000 Hz, there was a lowering of 10-20 dB in thresholds when skin penetration was performed. This lowering in thresholds means lower transducer distortion, lower electrical gain, and lower power consumption to produce a given sensation level.},
  doi      = {10.3109/01050398409076252},
  file     = {:home/jaycee/PhD Papers/hkansson1984.pdf:pdf},
  isbn     = {0105039840907},
}

@Article{gori2016devices,
  author    = {Gori, Monica and Cappagli, Giulia and Tonelli, Alessia and Baud-Bovy, Gabriel and Finocchietti, Sara},
  title     = {{Devices for visually impaired people: High technological devices with low user acceptance and no adaptability for children}},
  journal   = {Neuroscience and Biobehavioral Reviews},
  year      = {2016},
  volume    = {69},
  pages     = {79--88},
  issn      = {18737528},
  abstract  = {Considering that cortical plasticity is maximal in the child, why are the majority of technological devices available for visually impaired users meant for adults and not for children? Moreover, despite high technological advancements in recent years, why is there still no full user acceptance of existing sensory substitution devices? The goal of this review is to create a link between neuroscientists and engineers by opening a discussion about the direction that the development of technological devices for visually impaired people is taking. Firstly, we review works on spatial and social skills in children with visual impairments, showing that lack of vision is associated with other sensory and motor delays. Secondly, we present some of the technological solutions developed to date for visually impaired people. Doing this, we highlight the core features of these systems and discuss their limits. We also discuss the possible reasons behind the low adaptability in children.},
  doi       = {10.1016/j.neubiorev.2016.06.043},
  file      = {:home/jaycee/PhD Papers/1-s2.0-S0149763415302864-main.pdf:pdf},
  publisher = {Elsevier Ltd},
  url       = {http://dx.doi.org/10.1016/j.neubiorev.2016.06.043},
}

@Article{conference2010sonification,
  author = {Conference, International and Display, Auditory},
  title  = {{SONIFICATION OF COLOR AND DEPTH IN A MOBILITY AID FOR BLIND PEOPLE Guido Bologna University of Geneva , Computer Science Department , Route de Drize 7 , 1227 Carouge , University of Geneva , Route de Drize 7 , 1227 University of Geneva , Route de Drize 7 }},
  year   = {2010},
  pages  = {9--13},
  file   = {:home/jaycee/PhD Papers/BolognaDevillePun2010.pdf:pdf},
}

@Article{mohammad2008human,
  author   = {Mohammad, Yasser and Nishida, Toyoaki},
  title    = {{Human adaptation to a miniature robot: Precursors of mutual adaptation}},
  journal  = {Proceedings of the 17th IEEE International Symposium on Robot and Human Interactive Communication, RO-MAN},
  year     = {2008},
  pages    = {124--129},
  issn     = {1944-9445},
  abstract = {Mutual adaptation is an important phenomenon in human-human communications. Traditionally HRI research was more interested in investigating adaptation of the robot to the human using machine learning techniques but the possibility of utilizing the natural ability of humans to adapt to other humans and artifacts including robots is recently becoming increasingly attractive. This paper presents some of the results from an experiment conducted to investigate the interaction patterns and effectiveness of motion cues as a feedback modality between a human operator and a miniature robot in a confined collaborative navigation task. The results presented in this paper show evidence of human adaptation to the robot and moreover suggest that the adaptation rate is not constant or continuous in time but is discontinuous and nonlinear. The results also show evidence of a starting exploration stage before the adaptation with duration dependent on the expectations of the human regarding the capabilities of the robot in the given task. The paper investigates how to utilize these and related findings for building robots not only capable of adapting to human operators but can also help those operators adapt to them.},
  doi      = {10.1109/ROMAN.2008.4600654},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohammad, Nishida - 2008 - Human adaptation to a miniature robot Precursors of mutual adaptation.pdf:pdf},
  isbn     = {9781424422135},
  keywords = {Human Factors and Ergonomics,Monitoring of Behaviour and Internal States of Hum,User-centered Design of Robots},
}

@Article{walker2005thresholds,
  author   = {Walker, Bruce N and Stanley, R},
  title    = {{Thresholds of audibility for bone-conduction headsets}},
  journal  = {International conference on auditory display, Limerick, Ireland},
  year     = {2005},
  pages    = {218--222},
  abstract = {Despite advantages of using headphones, including privacy and portability, headphones have one essential drawback: they cover the ears of the listener, thus deteriorating detection and localization of ambient sounds. Bone-conduction headsets leave the ears uncovered, yet maintain portability and privacy. An initial step in establishing guidelines for using these “bonephones” is taken in the present research. The input into the bonephones necessary to reach a 71{\%} detection threshold is measured at critical band centers ranging from 150 Hz to 13500 Hz. These thresholds were measured with an open ear canal, a plugged ear canal, and a masking noise. Results were consistent with other bone-conduction threshold measurements. The utility of this information in the context of equalization for the audio presented through the bonephones is discussed.},
  file     = {:home/jaycee/PhD Papers/2005ICAD-WalkerStanley.pdf:pdf},
}

@Book{wikipediaencyclopedia2013mobile,
  title  = {{Mobile Computing}},
  year   = {2013},
  author = {{Wikipedia The free Encyclopedia}},
  isbn   = {9783642323201},
  file   = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wikipedia The free Encyclopedia - 2013 - Mobile Computing.pdf:pdf},
  url    = {http://en.wikipedia.org/wiki/Mobile{\_}computing},
}

@Article{burgess1992techniques,
  author  = {Burgess, David a.},
  title   = {{Techniques for low cost spatial audio}},
  journal = {Proceedings of the 5th annual ACM symposium on User interface software and technology - UIST '92},
  year    = {1992},
  pages   = {53--59},
  doi     = {10.1145/142621.142628},
  file    = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burgess - 1992 - Techniques for low cost spatial audio.pdf:pdf},
  isbn    = {0897915496},
  url     = {http://portal.acm.org/citation.cfm?doid=142621.142628},
}

@Article{todd2015haptic,
  author   = {Todd, Catherine and Mallya, Swati and Majeed, Sara and Rojas, Jude and Naylor, Katy},
  title    = {{Haptic-audio simulator for visually impaired indoor exploration}},
  journal  = {Journal of Assistive Technologies},
  year     = {2015},
  volume   = {9},
  number   = {2},
  pages    = {71--85},
  issn     = {20428723},
  abstract = {Purpose VirtuNav is a haptic-, audio- enabled Virtual Reality (VR) simulator that facilitates persons with visual impairment to explore a 3D computer model of a real-life indoor location, such as a room or building. This is to aid in pre-planning and spatial awareness, for a user to become more familiar with the environment prior to experiencing it in reality. Design/methodology/approach The system offers two unique interfaces: a free-roam interface where the user can navigate, and an edit mode where the administrator can manage test users, maps and retrieve test data. Findings System testing reveals that spatial awareness and memory mapping improve with user iterations within VirtuNav. Research limitations/implications VirtuNav is a research tool for investigation of user familiarity developed after repeated exposure to the simulator, to determine the extent to which haptic and/or sound cues improve a visually impaired user's ability to navigate a room or building with or without occlusion. Practical imp...},
  doi      = {10.1108/JAT-06-2014-0016},
  file     = {:home/jaycee/PhD Papers/JAT-06-2014-0016.pdf:pdf},
  keywords = {3D reconstruction,Haptics,Indoor navigation,Simulation,Virtual environment,Visually impaired},
}

@Article{bajcsy2018revisiting,
  author   = {Bajcsy, Ruzena and Aloimonos, Yiannis and Tsotsos, John K.},
  title    = {{Revisiting active perception}},
  journal  = {Autonomous Robots},
  year     = {2018},
  volume   = {42},
  number   = {2},
  pages    = {177--196},
  issn     = {15737527},
  abstract = {{\textcopyright} 2017 The Author(s) Despite the recent successes in robotics, artificial intelligence and computer vision, a complete artificial agent necessarily must include active perception. A multitude of ideas and methods for how to accomplish this have already appeared in the past, their broader utility perhaps impeded by insufficient computational power or costly hardware. The history of these ideas, perhaps selective due to our perspectives, is presented with the goal of organizing the past literature and highlighting the seminal contributions. We argue that those contributions are as relevant today as they were decades ago and, with the state of modern computational tools, are poised to find new life in the robotic perception systems of the next decade.},
  file     = {:home/jaycee/PhD Papers/1603.02729v2.pdf:pdf},
  keywords = {Attention,Control,Perception,Sensing},
}

@Article{faria2010electronica,
  author   = {Faria, J. and Lopes, S. and Fernandes, H. and Martins, P. and Barroso, J.},
  title    = {{Electronic white cane for blind people navigation assistance}},
  journal  = {World Automation Congress (WAC), 2010},
  year     = {2010},
  pages    = {1--7},
  issn     = {2154-4824},
  abstract = {In modern daily life people need to move, whether in business or leisure, sightseeing or addressing a meeting. Often this is done in familiar environments, but in some cases we need to find our way in unfamiliar scenarios. Visual impairment is a factor that greatly reduces mobility. Currently, the most widespread and used means by the visually impaired people are the white stick and the guide dog; however both present some limitations. With the recent advances in inclusive technology it is possible to extend the support given to people with visual impairment during their mobility. In this context we propose a system, named SmartVision, whose global objective is to give blind users the ability to move around in unfamiliar environments, whether indoor or outdoor, through a user friendly interface that is fed by a geographic information system (GIS). In this paper we propose the development of an electronic white cane that helps moving around, in both indoor and outdoor environments, providing contextualized geographical information using RFID technology.},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Faria et al. - 2010 - Electronic white cane for blind people navigation assistance.pdf:pdf},
  isbn     = {978-1-4244-9673-0},
  keywords = {Accessibility,Blind Navigation,GIS,RFID},
}

@Article{marentakis2005comparison,
  author   = {Marentakis, Georgios and Brewster, Stephen},
  title    = {{A comparison of feedback cues for enhancing pointing efficiency in interaction with spatial audio displays}},
  journal  = {Proceedings of the 7th international conference on Human computer interaction with mobile devices {\&} services - MobileHCI '05},
  year     = {2005},
  pages    = {55--62},
  abstract = {An empirical study that compared six different feedback cue types to enhance pointing efficiency in deictic spatial audio displays is presented. Participants were asked to select a sound using a physical pointing gesture, with the help of a loudness cue, a tim- bre cue and an orientation update cue as well as with combina- tions of these cues. Display content was varied systematically to investigate the effect of increasing display population. Speed, accuracy and throughput ratings are provided as well as effective target widths that allow for minimal error rates. The results showed direct pointing to be the most efficient interaction tech- nique; however large effective target widths reduce the applicabil- ity of this technique. Movement-coupled cues were found to sig- nificantly reduce display element size, but resulted in slower in- teraction and were affected by display content due to the require- ment of continuous target attainment. The results show that, with appropriate design, it is possible to overcome interaction uncer- tainty and provide solutions that are effective in mobile human computer interaction.},
  doi      = {10.1145/1085777.1085787},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marentakis, Brewster - 2005 - A comparison of feedback cues for enhancing pointing efficiency in interaction with spatial audio displays.pdf:pdf},
  isbn     = {1595930892},
  keywords = {gestures,multimodal interaction,spatial audio display design},
  url      = {http://portal.acm.org/citation.cfm?doid=1085777.1085787},
}

@Article{elmannai2017sensor,
  author   = {Elmannai, Wafa and Elleithy, Khaled},
  title    = {{Sensor-based assistive devices for visually-impaired people: Current status, challenges, and future directions}},
  journal  = {Sensors (Switzerland)},
  year     = {2017},
  volume   = {17},
  number   = {3},
  issn     = {14248220},
  abstract = {{\textcopyright} 2017 by the authors. Licensee MDPI, Basel, Switzerland. The World Health Organization (WHO) reported that there are 285 million visually-impaired people worldwide. Among these individuals, there are 39 million who are totally blind. There have been several systems designed to support visually-impaired people and to improve the quality of their lives. Unfortunately, most of these systems are limited in their capabilities. In this paper, we present a comparative survey of the wearable and portable assistive devices for visually-impaired people in order to show the progress in assistive technology for this group of people. Thus, the contribution of this literature survey is to discuss in detail the most significant devices that are presented in the literature to assist this population and highlight the improvements, advantages, disadvantages, and accuracy. Our aim is to address and present most of the issues of these systems to pave the way for other researchers to design devices that ensure safety and independent mobility to visually-impaired people.},
  doi      = {10.3390/s17030565},
  file     = {:home/jaycee/PhD Papers/sensors-17-00565-v2.pdf:pdf},
  keywords = {Assistive devices,Navigation and orientation systems,Obstacles avoidance,Obstacles detection,Visually-impaired people},
}

@Article{salas-moreno2013slam,
  author   = {Salas-Moreno, Renato F. and Newcombe, Richard A. and Strasdat, Hauke and Kelly, Paul H.J. and Davison, Andrew J.},
  title    = {{SLAM++: Simultaneous localisation and mapping at the level of objects}},
  journal  = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  year     = {2013},
  pages    = {1352--1359},
  issn     = {10636919},
  abstract = {We present the major advantages of a new ‘object ori- ented' 3D SLAM paradigm, which takes full advantage in the loop of prior knowledge that many scenes consist of repeated, domain-specific objects and structures. As a hand-held depth camera browses a cluttered scene, real- time 3D object recognition and tracking provides 6DoF camera-object constraints which feed into an explicit graph of objects, continually refined by efficient pose-graph opti- misation. This offers the descriptive and predictive power of SLAM systems which perform dense surface reconstruc- tion, but with a huge representation compression. The ob- ject graph enables predictions for accurate ICP-based cam- era to model tracking at each live frame, and efficient ac- tive search for new objects in currently undescribed image regions. We demonstrate real-time incremental SLAM in large, cluttered environments, including loop closure, relo- calisation and the detection of moved objects, and of course the generation of an object level scene description with the potential to enable interaction.},
  doi      = {10.1109/CVPR.2013.178},
  file     = {:home/jaycee/PhD Papers/Salas-Moreno{\_}SLAM{\_}Simultaneous{\_}Localisation{\_}2013{\_}CVPR{\_}paper.pdf:pdf},
  keywords = {GPGPU,ICP,KinectFusion,SLAM,augmented reality,object recognition,object-oriented,objects,real-time,scene understanding},
}

@Article{international2009co,
  author = {International, S R I and Park, Menlo},
  title  = {{Co-adaptation: adaptive co-training for semi-supervised learning}},
  year   = {2009},
  pages  = {3721--3724},
  file   = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/International, Park - 2009 - Co-adaptation adaptive co-training for semi-supervised learning.pdf:pdf},
  isbn   = {9781424423545},
}

@InProceedings{kallara2018indriya,
  author    = {Kallara, Sujith B. and Raj, Mitu and Raju, Roshan and Mathew, Nidhin Jacob and Padmaprabha, V. R. and Divya, D. S.},
  title     = {{Indriya - A smart guidance system for the visually impaired}},
  booktitle = {Proceedings of the International Conference on Inventive Computing and Informatics (ICICI)},
  year      = {2018},
  number    = {Icici},
  pages     = {26--29},
  abstract  = {In this paper, we propose a smart assistive technology named Indriya, for aiding the visually impaired. The word ``Indriya{\{}''{\}} is derived from Sanskrit language, meaning ``Organ{\{}''{\}}. Indriya is a mini handheld device for the blind, which may be used with walking cane as a smart companion. The device features complete voice assistance for easy navigation, through simple button clicks. It can also detect obstacles ahead up to three meters and can differentiate between objects and humans with guaranteed 80{\%} accuracy. Both vibratory and voice feedbacks are provided for accurate collision alerts. Indriya uses less number of sensors and extracts its smartness by incorporating Android and Internet of Things (IOT) support. In this work, we intend to put forth a novel, low cost and reliable approach to help the blind to explore the possibilities and power of smart phone technology in navigation.},
  doi       = {10.1109/ICICI.2017.8365359},
  file      = {:home/jaycee/PhD Papers/08365359.pdf:pdf},
  isbn      = {9781538640319},
  keywords  = {Assistive technology,Electronic travel aid,GPS,Navigation,Smart systems},
}

@Article{bujacz2016sonification,
  author   = {Bujacz, Micha?? and Strumi????o, Pawe??},
  title    = {{Sonification: Review of Auditory Display Solutions in Electronic Travel Aids for the Blind}},
  journal  = {Archives of Acoustics},
  year     = {2016},
  volume   = {41},
  number   = {3},
  pages    = {401--414},
  issn     = {01375075},
  abstract = {{\textcopyright} 2016 Polish Academy of Sciences {\&} Institute of Fundamental Technological Research (IPPT PAN).Sonification is defined as presentation of information by means of non-speech audio. In assistive technologies for the blind, sonification is most often used in electronic travel aids (ETAs) - devices which aid in independent mobility through obstacle detection or help in orientation and navigation. The presented review contains an authored classification of various sonification schemes implemented in the most widely known ETAs. The review covers both those commercially available and those in various stages of research, according to the input used, level of signal processing algorithm used and sonification methods. Additionally, a sonification approach developed in the Naviton project is presented. The prototype utilizes stereovision scene reconstruction, obstacle and surface segmentation and spatial HRTF filtered audio with discrete musical sounds and was successfully tested in a pilot study with blind volunteers in a controlled environment, allowing to localize and navigate around obstacles.},
  doi      = {10.1515/aoa-2016-0040},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bujacz, Strumio - 2016 - Sonification Review of Auditory Display Solutions in Electronic Travel Aids for the Blind.pdf:pdf},
  keywords = {assistive technologies,auditory display,blindness,electronic travel aid,sonification,visual impairment},
}

@Article{heuten2007interactive,
  author   = {Heuten, Wilko and Wichmann, Daniel and Boll, Susanne},
  title    = {{Interactive 3D sonification for the exploration of city maps}},
  year     = {2007},
  number   = {October},
  pages    = {155--164},
  abstract = {Blind or visually impaired people usually do not leave their homes without any assistance, in order to visit unknown cities or places. One reason for this dilemma is, that it is hardly possible for them to gain a non-visual overview about the new place, its landmarks and geographic entities already at home. Sighted people can use a printed or digital map to perform this task. Existing haptic and acoustic approaches today do not provide an economic way to mediate the understanding of a map and relations between objects like distance, direction, and object size. We are providing an interactive three-dimensional sonification interface to explore city maps. A blind person can build a mental model of an area's structure by virtually exploring an auditory map at home. Geographic objects and landmarks are presented by sound areas, which are placed within a sound room. Each type of object is associated with a different sound and can therefore be identified. By investigating the auditory map, the user perceives an idea of the various objects, their directions and relative distances. First user tests show, that users are able to reproduce a sonified city map, which comes close to the original visual city map. With our approach exploring a map with non-speech sound areas provide a new user interface metaphor that offers its potential not only for blind and visually impaired persons but also to applications for sighted persons.},
  doi      = {10.1145/1182475.1182492},
  file     = {:home/jaycee/PhD Papers/p155-heuten.pdf:pdf},
  isbn     = {1595933255},
}

@Article{shepard2015judgments,
  author = {Shepard, N},
  title  = {{in Judgments of Relative Pitch construction of}},
  year   = {2015},
  volume = {180},
  number = {1929},
  pages  = {2346--2353},
  file   = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shepard - 2015 - in Judgments of Relative Pitch construction of.pdf:pdf},
}

@InProceedings{alam2017indoor,
  author    = {Alam, Md Mazidul and Arefin, Sayed Erfan and Alim, Miraj Al and Adib, Samiul Islam and Rahman, Md Abdur},
  title     = {{Indoor localization system for assisting visually impaired people}},
  booktitle = {Proceedings of the International Conference on Electrical, Computer and Communication Engineering (ECCE)},
  year      = {2017},
  pages     = {333--338},
  publisher = {IEEE},
  abstract  = {In this paper we developed an image processing based indoor localization system with color detection technique of connected objects. By using color detection technique our system can pinpoint a user's location with very high accuracy for real-time applications. We used our system to filter out an image for a specific color and extracted pixel co-ordinates for that image. User's location is then determined by comparing the matrix for those values against a pre-created matrix of training images. We successfully conducted experiments in indoor environments as well and they yielded very good results. After analyzing these results, we propose to integrate our localization system with indoor navigation systems which can be used for blind people where accuracy is the most important element. To further facilitate the navigation process we also developed an android based application.},
  doi       = {10.1109/ECACE.2017.7912927},
  file      = {:home/jaycee/PhD Papers/07912927.pdf:pdf},
  isbn      = {9781509056262},
  journal   = {ECCE 2017 - International Conference on Electrical, Computer and Communication Engineering},
  keywords  = {Color Segmentation,Connected object detection,Image Processing,Indoor localization,Wireless communication},
}

@InProceedings{damen2018scaling,
  author    = {Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and Wray, Michael},
  title     = {{Scaling Egocentric Vision: The Epic-Kitchens Dataset}},
  booktitle = {Proceedings of the European Conference on Computer Vision},
  year      = {2018},
  pages     = {753--771},
  abstract  = {First-person vision is gaining interest as it offers a unique viewpoint on people's interaction with objects, their attention, and even intention. However, progress in this challenging domain has been relatively slow due to the lack of sufficiently large datasets. In this paper, we introduce EPIC-KITCHENS, a large-scale egocentric video benchmark recorded by 32 participants in their native kitchen environments. Our videos depict nonscripted daily activities: we simply asked each participant to start recording every time they entered their kitchen. Recording took place in 4 cities (in North America and Europe) by participants belonging to 10 different nationalities, resulting in highly diverse cooking styles. Our dataset features 55 hours of video consisting of 11.5M frames, which we densely labeled for a total of 39.6K action segments and 454.3K object bounding boxes. Our annotation is unique in that we had the participants narrate their own videos (after recording), thus reflecting true intention, and we crowd-sourced ground-truths based on these. We describe our object, action and anticipation challenges, and evaluate several baselines over two test splits, seen and unseen kitchens. Dataset and Project page: http://epic-kitchens.github.io},
  doi       = {10.1007/978-3-030-01225-0_44},
  file      = {:home/jaycee/PhD Papers/Dima{\_}Damen{\_}Scaling{\_}Egocentric{\_}Vision{\_}ECCV{\_}2018{\_}paper.pdf:pdf},
  keywords  = {action recognition and anticipation,benchmarks,dataset,egocentric object detection,egocentric vision,first-person vi-,sion},
}

@Article{zegarraflores2016indoor,
  author   = {{Zegarra Flores}, Jesus Victor and Rasseneur, Laurence and Galani, Rodrigue and Rakitic, Fabienne and Farcy, Ren{\'{e}}},
  title    = {{Indoor navigation with smart phone IMU for the visually impaired in university buildings}},
  journal  = {Journal of Assistive Technologies},
  year     = {2016},
  volume   = {10},
  number   = {3},
  pages    = {133--139},
  issn     = {20428723},
  abstract = {Purpose The purpose of this paper is to design and test effective indoor navigation solutions for visually impaired people in situations where GPS, bluetooth or Wi-Fi signals are unavailable. The authors use the inertial measurement units (IMU), the compass and the barometer of a smart phone. Design/methodology/approach The authors have used commercial Android smart phones with IMU, compass and barometer to record a path and to give navigation instructions in an adapted way using a mobility-specific vocabulary. The method proposed is to save paths taking into account different indoor waypoints such as the stairs (change from one floor to another) and the change of direction of the trajectory of the path (e.g. one-fourth turn right or left), recording data from the IMU sensor's, compass and barometer of the smart phone. Having this information and the characteristics of the each segment (distance, azimuth to the north and pressure) of the path, it is possible to provide functional navigation guidance to th...},
  doi      = {10.1108/JAT-05-2015-0018},
  file     = {:home/jaycee/PhD Papers/JAT-05-2015-0018.pdf:pdf},
  keywords = {Barometer,IMU,Indoor navigation,Smart phone,University buildings,Visually impaired person},
}

@Article{chickering2004uncertainty,
  author   = {Chickering, Max. and Halpern, Joseph Y. and {Boeing Aerospace Company.} and {Association for Uncertainty in Artificial Intelligence.}},
  title    = {{Uncertainty in Artificial Intelligence: Proceedings of the Twentieth Conference}},
  journal  = {Proceedings of the 20th conference on Uncertainty in artificial intelligence},
  year     = {2004},
  pages    = {59--66},
  abstract = {"Conference on Uncertainty in Artificial Intelligence"--Preface. Published by AUAI Press for the Association for Uncertainty in Artificial Intelligence.},
  file     = {:home/jaycee/PhD Papers/p520-smith.pdf:pdf},
  isbn     = {0974903906},
  url      = {https://dl-acm-org.proxy.lib.miamioh.edu/citation.cfm?id=1036851},
}

@Article{friedlander1998bullseye,
  author   = {Friedlander, Naomi and Schlueter, Kevin and Mantei, Marilyn},
  title    = {{Bullseye! when Fitts' law doesn't fit}},
  journal  = {Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '98},
  year     = {1998},
  pages    = {257--264},
  abstract = {{\_} ., .f-,: {\_} - -. .r. -{\_} -{\_}- I{\_} {\_}. ';.-1dL ., I?..;. I- .:I ,'z'.w -,-- ,. : -.-?. i----',:. ':A. {\_} ,.'.L ..-{\_} .-j{\_}{\_} Bullseye! When Fit-k' Doesn't Fit Naomi Friedlander, Kevin Schlueter aud Marilyn Mantei Departmint of Computer Science University of Toronto 10 Kings College Road Toronto, },
  doi      = {10.1145/274644.274681},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedlander, Schlueter, Mantei - 1998 - Bullseye! when Fitts' law doesn't fit.pdf:pdf},
  isbn     = {0201309874},
  url      = {http://dl.acm.org/citation.cfm?id=274644.274681},
}

@Book{greibach1975lecture,
  title     = {{Lecture Notes in Computer Science}},
  publisher = {Springer},
  year      = {1975},
  author    = {Greibach, SA},
  isbn      = {9783642027093},
  doi       = {10.1007/978-3-540-77051-0},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Greibach - 1975 - Lecture Notes in Computer Science.pdf:pdf},
  url       = {http://www.ulb.tu-darmstadt.de/tocs/59142804.pdf},
}

@Article{lee2016impact,
  author   = {Lee, Chang Gyu and Oakley, Ian and Kim, Eun Soo and Ryu, Jeha},
  title    = {{Impact of visual-haptic spatial discrepancy on targeting performance}},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  year     = {2016},
  volume   = {46},
  number   = {8},
  pages    = {1098--1108},
  issn     = {10834427},
  abstract = {This paper presents a comprehensive study of the impact of visual-haptic spatial discrepancies on human performance in a targeting task conducted in a visual-haptic virtual and augmented environment. Moreover, it explores whether the impact of this effect varies with two additional variables: 1) haptic wall stiffness and 2) visual cursor diameter. Finally, we discuss the relative dominance of visual and haptic cues during a targeting task. The results indicate that while the spatial discrepancies studied exerted a small effect on the time required to perform targeting, they impacted the absolute errors considerably. Additionally, we report that haptic wall stiffness has a significant effect on absolute errors while the visual cursor diameter has a significant effect on movement time. Finally, we conclude that while both visual and haptic cues are important during targeting tasks, haptic cues played a more dominant role than visual cues. The results of this paper can be used to predict how human targeting performance will vary between systems, such as those using haptically enabled virtual reality or augmented reality technologies that feature visual-haptic spatial discrepancies.},
  doi      = {10.1109/TSMC.2015.2468675},
  file     = {:home/jaycee/PhD Papers/07247759.pdf:pdf},
  keywords = {Augmented reality (AR),force feedback,haptic interfaces,performance evaluation,surgery,virtual reality (VR)},
}

@Article{aladren2016navigation,
  author   = {Aladren, A. and Lopez-Nicolas, G. and Puig, Luis and Guerrero, Josechu J.},
  title    = {{Navigation Assistance for the Visually Impaired Using RGB-D Sensor with Range Expansion}},
  journal  = {IEEE Systems Journal},
  year     = {2016},
  volume   = {10},
  number   = {3},
  pages    = {922--932},
  issn     = {19379234},
  abstract = {Navigation assistance for visually impaired (NAVI) refers to systems that are able to assist or guide people with vision loss, ranging from partially sighted to totally blind, by means of sound commands. In this paper, a new system for NAVI is presented based on visual and range information. Instead of using several sensors, we choose one device, a consumer RGB-D camera, and take advantage of both range and visual information. In particular, the main contribution is the combination of depth information with image intensities, resulting in the robust expansion of the range-based floor segmentation. On one hand, depth information, which is reliable but limited to a short range, is enhanced with the long-range visual information. On the other hand, the difficult and prone-to-error image processing is eased and improved with depth information. The proposed system detects and classifies the main structural elements of the scene providing the user with obstacle-free paths in order to navigate safely across unknown scenarios. The proposed system has been tested on a wide variety of scenarios and data sets, giving successful results and showing that the system is robust and works in challenging indoor environments.},
  doi      = {10.1109/JSYST.2014.2320639},
  file     = {:home/jaycee/PhD Papers/06819807.pdf:pdf},
  keywords = {Navigation assistance for visually impaired (NAVI),RGB-D camera,range and vision,visually impaired assistance,wearable system},
}

@Article{madole19953,
  author   = {Madole, Dave and Begault, Durand},
  title    = {{3-D Sound for Virtual Reality and Multimedia}},
  journal  = {Computer Music Journal},
  year     = {1995},
  volume   = {19},
  number   = {4},
  pages    = {99},
  issn     = {01489267},
  abstract = {One of the key underlying technologies of immersive virtual reality (VR) is 3-D sound. This is the first introduction to 3-D sound theory and applications aimed at the commercial engineer. It will provide the reader with an understanding of the communication chain between source and listener.},
  doi      = {10.2307/3680997},
  file     = {:home/jaycee/PhD Papers/20010044352.pdf:pdf},
  url      = {https://www.jstor.org/stable/3680997?origin=crossref},
}

@Article{bestick2015personalized,
  author    = {Bestick, Aaron M. and Burden, Samuel A. and Willits, Giorgia and Naikal, Nikhil and Sastry, S. Shankar and Bajcsy, Ruzena},
  title     = {{Personalized kinematics for human-robot collaborative manipulation}},
  journal   = {IEEE International Conference on Intelligent Robots and Systems},
  year      = {2015},
  volume    = {2015-Decem},
  pages     = {1037--1044},
  issn      = {21530866},
  abstract  = {{\textcopyright} 2015 IEEE. We present a framework for parameter and state estimation of personalized human kinematic models from motion capture data. These models can be used to optimize a variety of human-robot collaboration scenarios for the comfort or ergonomics of an individual human collaborator. Our approach offers two main advantages over prior approaches from the literature and commercial software: the kinematic models are estimated for a specific individual without a priori assumptions on limb dimensions or range of motion, and our kinematic formalism explicitly encodes the natural kinematic constraints of the human body. The personalized models are tested in a human-robot collaborative manipulation experiment. We find that human subjects with a restricted range of motion rotate their torso significantly less during bimanual object handoffs if the robot uses a personalized kinematic model to plan the handoff configuration, as compared to previous approaches using generic human kinematic models.},
  doi       = {10.1109/IROS.2015.7353498},
  file      = {:home/jaycee/PhD Papers/07353498.pdf:pdf},
  isbn      = {9781479999941},
  publisher = {IEEE},
}

@Article{araya-lopez2010pomdp,
  author = {Araya-L{\'{o}}pez, Mauricio and Buffet, Olivier and Thomas, Vincent and Charpillet, Fran{\c{c}}ois},
  title  = {{A POMDP Extension with Belief-dependent Rewards (Extended Version) A POMDP Ex-tension with Belief-dependent Rewards (Extended Version)}},
  year   = {2010},
  file   = {:home/jaycee/PhD Papers/RR-7433.pdf:pdf},
  url    = {https://hal.inria.fr/inria-00529498v2},
}

@MastersThesis{linhthai2018deep,
  author   = {{Linh Thai}, Hong},
  title    = {Deep Reinforcement Learning for {POMDPs}},
  school   = {Technical University of Darmstadt},
  year     = {2018},
  abstract = {Deep reinforcement learning has made a big impact in recent years by achieving human level game play in the Atari 2600 games just using images as input and by learning robot control policies end-to-end from images to motor signals - tasks, which were previously intractable for classical reinforcement learning. For learning these control policies based on deep neural networks policy search methods, such as deep deterministic policy gradient (Lillicrap et al., 2015) and especially trust region policy optimization (Schulman et al., 2015), have been very successful, also in transferring insights and past approaches of classical reinforcement learning to deep neural networks. In this thesis we build up this previous work and derive a new algorithm for deep neural networks, called compatible policy search (COPS), based on the idea of the natural gradient, compatible value function approximation, entropy regularization and relative entropy policy search (Peters et al., 2010). In our experiment we investigated the capability of COPS and other policy search methods in challenging partially observable environments: RockSample, where the agent needs to take information gathering into account and Pocman, a large scale partially observable Markov decision processes (POMDP) with ca. 10{\^{}}56 underlying states. We present results where COPS outperforms all other six policy search methods and where the additional entropy regularization constraint, bounding the entropy of the new policy, is essential for exploration and for finding a good policy in these partially observable environments. Furthermore, to encourage additional exploration in these partially observable environments we will propose a factored context tree switching model for POMDPs, which we use for a pseudocount based exploration bonus and leads to additional performance gains.},
  file     = {:home/jaycee/PhD Papers/master{\_}thesis{\_}hong{\_}linh{\_}thai{\_}2018.pdf:pdf},
  keywords = {Deep reinforcement learning,POMDP,entropy regularization,exploration,relative entropy},
  url      = {https://pdfs.semanticscholar.org/cc43/349d4fd3676ec299c9a9613da39f4392f995.pdf{\%}0Ahttp://www.ausy.tu-darmstadt.de/uploads/Team/JoniPajarinen/master{\_}thesis{\_}hong{\_}linh{\_}thai{\_}2018.pdf},
}

@Article{yamagishi2001evidence,
  author   = {Yamagishi, Noriko and Anderson, Stephen J. and Ashida, Hiroshi},
  title    = {{Evidence for dissociation between the perceptual and visuomotor systems in humans}},
  journal  = {Proceedings of the Royal Society B: Biological Sciences},
  year     = {2001},
  volume   = {268},
  number   = {1470},
  pages    = {973--977},
  issn     = {14712970},
  abstract = {When a visual stimulus is continuously moved behind a small stationary window, the window appears displaced in the direction of motion of the stimulus. In this study we showed that the magnitude of this illusion is dependent on (i) whether a perceptual or visuomotor task is used for judging the location of the window (ii) the directional signature of the stimulus, and (iii) whether or not there is a significant delay between the end of the visual presentation and the initiation of the localization measure. Our stimulus was a drifting sinusoidal grating windowed in space by a stationary, two-dimensional, Gaussian envelope (sigma=1 cycle of sinusoid). Localization measures were made following either a short (200 ms) or long (4.2 s) post-stimulus delay. The visuomotor localization error was up to three times greater than the perceptual error for a short delay. However, the visuomotor and perceptual localization measures were similar for a long delay. Our results provide evidence in support of the hypothesis that separate cortical pathways exist for visual perception and visually guided action and that delayed actions rely on stored perceptual information.},
  doi      = {10.1098/rspb.2001.1603},
  file     = {:home/jaycee/PhD Papers/973.full.pdf:pdf},
  keywords = {Action,Illusion,Motion,Vision},
}

@Article{katz2011spatial,
  author   = {Katz, Brian F G and Picinali, Lorenzo},
  title    = {{Spatial audio applied to research with the Blind.}},
  journal  = {Advances in Sound Localization},
  year     = {2011},
  pages    = {225--250},
  abstract = {Spatial audio technology has long been used for studies relating to human perception, primarily in the area of auditory source localisation. The ability to render individual sounds at desired positions or complex spatial audio scenes, without the need to manipulate any physical equipment, has offered researchers many advantages. Recently, the use of spatial audio has expanded beyond the study of such low level processes as localisation, and has been used as a tool to investigate higher-level cognitive functions. This work presents several recent studies where spatial audio technology has been used in order to expand our understanding of spatial cognition, with a specific focus on the abilities of the visually impaired, in both free-field and interior space exploration. These types of works provide for both an improved understanding within cognitive science and for the research and development into improved high resolution renderings with appropriate auditory cues.},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Katz, Picinali - 2011 - Spatial audio applied to research with the Blind.pdf:pdf},
  isbn     = {978-953-307-224-1},
}

@Article{siegle2009measurement,
  author   = {Siegle, Joshua H. and Campos, Jennifer L. and Mohler, Betty J. and Loomis, Jack M. and B{\"{u}}lthoff, Heinrich H.},
  title    = {{Measurement of instantaneous perceived self-motion using continuous pointing}},
  journal  = {Experimental Brain Research},
  year     = {2009},
  volume   = {195},
  number   = {3},
  pages    = {429--444},
  issn     = {00144819},
  abstract = {In order to optimally characterize full-body self-motion perception during passive translations, changes in perceived location, velocity, and acceleration must be quantified in real time and with high spatial resolution. Past methods have failed to effectively measure these critical variables. Here, we introduce continuous pointing as a novel method with several advantages over previous methods. Participants point continuously to the mentally updated location of a previously viewed target during passive, full-body movement. High-precision motion-capture data of arm angle provide a measure of a participant's perceived location and, in turn, perceived velocity at every moment during a motion trajectory. In two experiments, linear movements were presented in the absence of vision by passively translating participants with a robotic wheelchair or an anthropomorphic robotic arm (MPI Motion Simulator). The movement profiles included constant-velocity trajectories, two successive movement intervals separated by a brief pause, and reversed-motion trajectories. Results indicate a steady decay in perceived velocity during constant-velocity travel and an attenuated response to mid-trial accelerations. {\textcopyright} 2009 The Author(s).},
  doi      = {10.1007/s00221-009-1805-6},
  file     = {:home/jaycee/PhD Papers/10.1007{\%}2Fs00221-009-1805-6.pdf:pdf},
  keywords = {Continuous pointing,Inertial cues,Passive transport,Self-motion perception,Spatial updating},
}

@Article{blum2013spatialized,
  author   = {Blum, Jeffrey R. and Bouchard, Mathieu and Cooperstock, Jeremy R.},
  title    = {{Spatialized audio environmental awareness for blind users with a smartphone}},
  journal  = {Mobile Networks and Applications},
  year     = {2013},
  volume   = {18},
  number   = {3},
  pages    = {295--309},
  issn     = {1383469X},
  abstract = {Numerous projects have investigated assistive navigation technologies for the blind community, tackling challenges ranging from interface design to sensory substitution. However, none of these have successfully integrated what we consider to be the three factors necessary for a widely deployable system that delivers a rich experience of one's environment: implementation on a commodity device, use of a pre-existing worldwide point of interest (POI) database, and a means of rendering the environment that is superior to a naive playback of spoken text. Our "In Situ Audio Services" (ISAS) application responds to these needs, allowing users to explore an urban area without necessarily having a particular destination in mind. We describe the technical aspects of its implementation, user requirements, interface design, safety concerns, POI data source issues, and further requirements to make the system practical on a wider basis. Initial qualitative feedback from blind users is also discussed.},
  annote   = {Spacial audio interface for VIPs. Pretty good and noteworthy},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blum, Bouchard, Cooperstock - 2013 - Spatialized audio environmental awareness for blind users with a smartphone.pdf:pdf},
  isbn     = {1383-469X},
  keywords = {Audio augmented reality,Blind navigation,GPS,Smartphone,Spatialized audio},
}

@Article{kurniawan2004design,
  author  = {Kurniawan, Sri Η and Sporka, Adam J and Nĕmecand, Vladislav and Slavik, Pavel},
  title   = {{Design and user evaluation of a spatial audio rendering system for blind users}},
  journal = {5th International Conference on Disability, Virtual Reality and Associated Technologies},
  year    = {2004},
  pages   = {175--182},
  issn    = {2191-1231},
  doi     = {10.1515/IJDHD.2005.4.4.317},
  file    = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kurniawan et al. - 2004 - Design and user evaluation of a spatial audio rendering system for blind users.pdf:pdf},
}

@Article{adebiyi2017assessment,
  author   = {Adebiyi, Aminat and Sorrentino, Paige and Bohlool, Shadi and Zhang, Carey and Arditti, Mort and Goodrich, Gregory and Weiland, James D.},
  title    = {{Assessment of feedback modalities for wearable visual AIDS in blind mobility}},
  journal  = {PLoS ONE},
  year     = {2017},
  volume   = {12},
  number   = {2},
  pages    = {1--17},
  issn     = {19326203},
  abstract = {Sensory substitution devices engage sensory modalities other than vision to communicate information typically obtained through the sense of sight. In this paper, we examine the ability of subjects who are blind to follow simple verbal and vibrotactile commands that allow them to navigate a complex path. A total of eleven visually impaired subjects were enrolled in the study. Prototype systems were developed to deliver verbal and vibrotactile commands to allow an investigator to guide a subject through a course. Using this mode, subjects could follow commands easily and navigate significantly faster than with their cane alone (p {\textless}0.05). The feedback modes were similar with respect to the increased speed for course completion. Subjects rated usability of the feedback systems as "above average" with scores of 76.3 and 90.9 on the system usability scale.},
  doi      = {10.1371/journal.pone.0170531},
  file     = {:home/jaycee/PhD Papers/journal.pone.0170531.pdf:pdf},
  isbn     = {1111111111},
}

@Article{szpiro2016finding,
  author   = {Szpiro, Sarit and Zhao, Yuhang and Azenkot, Shiri},
  title    = {{Finding a store, searching for a product: a study of daily challenges of low vision people}},
  journal  = {UbiComp '16},
  year     = {2016},
  pages    = {61--72},
  abstract = {{\textcopyright} 2016 ACM. Visual impairments encompass a range of visual abilities. People with low vision have functional vision and thus their experiences are likely to be different from people with no vision. We sought to answer two research questions: (1) what challenges do low vision people face when performing daily activities and (2) what AIDS (high- and low-tech) do low vision people use to alleviate these challenges? Our goal was to reveal gaps in current technologies that can be addressed by the UbiComp community. Using contextual inquiry, we observed 11 low vision people perform a wayfinding and shopping task in an unfamiliar environment. The task involved wayfinding and searching and purchasing a product. We found that, although there are low vision AIDS on the market, participants mostly used their smartphones, despite interface accessibility challenges. While smartphones helped them outdoors, participants were overwhelmed and frustrated when shopping in a store. We discuss the inadequacies of existing AIDS and highlight the need for systems that enhance visual information, rather than convert it to audio or tactile.},
  doi      = {10.1145/2971648.2971723},
  file     = {:home/jaycee/PhD Papers/p61-szpiro.pdf:pdf},
  isbn     = {9781450344616},
  url      = {http://dl.acm.org/citation.cfm?doid=2971648.2971723},
}

@Article{hoey2010automated,
  author    = {Hoey, Jesse and Poupart, Pascal and von Bertoldi, Axel and Craig, Tammy and Boutilier, Craig and Mihailidis, Alex},
  title     = {{Automated handwashing assistance for persons with dementia using video and a partially observable Markov decision process}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2010},
  volume    = {114},
  number    = {5},
  pages     = {503--519},
  issn      = {10773142},
  abstract  = {This paper presents a real-time vision-based system to assist a person with dementia wash their hands. The system uses only video inputs, and assistance is given as either verbal or visual prompts, or through the enlistment of a human caregiver's help. The system combines a Bayesian sequential estimation framework for tracking hands and towel, with a decision-theoretic framework for computing policies of action. The decision making system is a partially observable Markov decision process, or POMDP. Decision policies dictating system actions are computed in the POMDP using a point-based approximate solution technique. The tracking and decision making systems are coupled using a heuristic method for temporally segmenting the input video stream based on the continuity of the belief state. A key element of the system is the ability to estimate and adapt to user psychological states, such as awareness and responsiveness. We evaluate the system in three ways. First, we evaluate the hand-tracking system by comparing its outputs to manual annotations and to a simple hand-detection method. Second, we test the POMDP solution methods in simulation, and show that our policies have higher expected return than five other heuristic methods. Third, we report results from a ten-week trial with seven persons moderate-to-severe dementia in a long-term care facility in Toronto, Canada. The subjects washed their hands once a day, with assistance given by our automated system, or by a human caregiver, in alternating two-week periods. We give two detailed case study analyses of the system working during trials, and then show agreement between the system and independent human raters of the same trials. {\textcopyright} 2010 Elsevier Inc. All rights reserved.},
  doi       = {10.1016/j.cviu.2009.06.008},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoey et al. - 2010 - Automated handwashing assistance for persons with dementia using video and a partially observable Markov decision p.pdf:pdf},
  isbn      = {9783000209338},
  keywords  = {Assistive technology,Behavior monitoring,Decision theory,Human tracking,POMDP,Particle filters,User trials},
  pmid      = {18992135},
  publisher = {Elsevier Inc.},
  url       = {http://dx.doi.org/10.1016/j.cviu.2009.06.008},
}

@Article{araya-l2010pomdp,
  author  = {Araya-l, Mauricio and Thomas, Vincent and Universit, Nancy},
  title   = {{A POMDP Extension with Belief-dependent Rewards}},
  journal = {Nips},
  year    = {2010},
  number  = {i},
  pages   = {1--9},
  file    = {:home/jaycee/PhD Papers/3971-a-pomdp-extension-with-belief-dependent-rewards.pdf:pdf},
  url     = {https://hal.inria.fr/inria-00529498v2},
}

@Article{young2010cognitive,
  author  = {Young, Steve},
  title   = {{Cognitive Human Interaction}},
  journal = {IEEE Signal Processing Magazine},
  year    = {2010},
  number  = {May},
  pages   = {128--140},
  file    = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Young - 2010 - Cognitive Human Interaction.pdf:pdf},
}

@Article{hartmann1999how,
  author   = {Hartmann, W M},
  title    = {{How We Localize Sound}},
  journal  = {Physics today},
  year     = {1999},
  volume   = {52},
  number   = {11},
  pages    = {24--29},
  issn     = {00319228},
  abstract = {Relying on a variety of cues, including intensity, timing, and spectrum, our brains recreate a three-dimensional image of the acoustic landscape from the sound we hear},
  doi      = {10.1063/1.882727},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hartmann - 1999 - How We Localize Sound.pdf:pdf},
  isbn     = {0031922899},
  url      = {http://www.aip.org/pt/nov99/locsound.html{\%}5CnC:{\%}5CUsers{\%}5Cdv13282{\%}5CDropbox{\%}5CPdfLibrary{\%}5Cpdfs{\%}5Chow we localize sound.pdf},
}

@Article{bulling2012multimodal,
  author   = {Bulling, Andreas and Ward, Jamie A. and Gellersen, Hans},
  title    = {{Multimodal recognition of reading activity in transit using body-worn sensors}},
  journal  = {ACM Transactions on Applied Perception},
  year     = {2012},
  volume   = {9},
  number   = {1},
  pages    = {1--21},
  issn     = {15443558},
  abstract = {Reading is one of the most well studied visual activities. Vision research traditionally focuses on understanding the perceptual and cognitive processes involved in reading. In this work we recognise reading activity by jointly analysing eye and head movements of people in an everyday environment. Eye movements are recorded using an electrooculography (EOG) system; body movements using body-worn inertial measurement units. We compare two approaches for continuous recognition of reading: String matching (STR) that explicitly models the characteristic horizontal saccades during reading, and a support vector machine (SVM) that relies on 90 eye movement features extracted from the eye movement data. We evaluate both methods in a study performed with eight participants reading while sitting at a desk, standing, walking indoors and outdoors, and riding a tram. We introduce a method to segment reading activity by exploiting the sensorimotor coordination of eye and head movements during reading. Using person-independent training, we obtain an average precision for recognising reading of 88.9$\backslash${\%} (recall 72.3$\backslash${\%}) using STR and of 87.7$\backslash${\%} (recall 87.9$\backslash${\%}) using SVM over all participants. We show that the proposed segmentation scheme improves the performance of recognising reading events by more than 24$\backslash${\%}. Our work demonstrates that the joint analysis of multiple modalities is beneﬁcial for reading recognition and opens up discussion on the wider applicability of this recognition approach to other visual and physical activities.},
  doi      = {10.1145/2134203.2134205},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bulling, Ward, Gellersen - 2012 - Multimodal recognition of reading activity in transit using body-worn sensors.pdf:pdf},
  isbn     = {1544-3558(Print)},
  url      = {http://dl.acm.org/citation.cfm?doid=2134203.2134205},
}

@Article{bierkandt2013novel,
  author   = {Bierkandt, K. and Gepp, M. and Zimmermann, H.},
  title    = {{A novel bioactive implant material based on a porous silicone-}},
  journal  = {Biomed Tech},
  year     = {2013},
  volume   = {58},
  pages    = {13--14},
  doi      = {10.1515/bmt-2013-4},
  file     = {:home/jaycee/PhD Papers/[Biomedical Engineering  Biomedizinische Technik] Concept of a Co-Adaptive Training Environment for Human-Machine Interfaces Based on EMG-Control.pdf:pdf},
  keywords = {bioactive mate-,composite,hydrogel,silicone},
}

@Article{schonstein2008comparison,
  author   = {Schonstein, David and Ferr{\'{e}}, Laurent and Katz, Brian F.},
  title    = {{Comparison of headphones and equalization for virtual auditory source localization}},
  journal  = {The Journal of the Acoustical Society of America},
  year     = {2008},
  number   = {5},
  pages    = {3724--3724},
  issn     = {0001-4966},
  abstract = {This study investigates the variation in localization performance between different headphone styles. Eight different headphones (including various in-ear, circumaural open and closed, and bone conduction headphones) were tested. In addition, the effect of headphone equalization (aiming to produce an approximately flat frequency response) was investigated. Localization was examined for 24 locations distributed on a sphere surrounding the listener. A single subject participated in the study using a single chosen non-individual HRTF set. Each location was repeated 6 times, resulting in a total of 144 localization reports. Overall, results were relatively consistent for 3 out of the 8 headphones tested. For these headphones, there was no significant difference in lateral angle error, associated with ITD and ILD cues. Polar angle errors, associated with the cone of confusion, however did vary significantly for these headphones. The headphone equalization had varying effects on localization accuracy depending on the headphone. Globally, headphone equalization showed no significant effect on localization accuracy. The results serve as a preliminary investigation, highlighting consistent results for only a select group of headphones tested for effective sound rendering in virtual auditory space. In addition, the results suggest that headphone equalization has a minimal influence on localization accuracy under these conditions.},
  file     = {:home/jaycee/PhD Papers/001080.pdf:pdf},
}

@Article{sanchez2009exploiting,
  author    = {Sanchez, Justin C. and Mahmoudi, Babak and DiGiovanna, Jack and Principe, Jose C.},
  title     = {{Exploiting co-adaptation for the design of symbiotic neuroprosthetic assistants}},
  journal   = {Neural Networks},
  year      = {2009},
  volume    = {22},
  number    = {3},
  pages     = {305--315},
  issn      = {08936080},
  abstract  = {The success of brain-machine interfaces (BMI) is enabled by the remarkable ability of the brain to incorporate the artificial neuroprosthetic 'tool' into its own cognitive space and use it as an extension of the user's body. Unlike other tools, neuroprosthetics create a shared space that seamlessly spans the user's internal goal representation of the world and the external physical environment enabling a much deeper human-tool symbiosis. A key factor in the transformation of 'simple tools' into 'intelligent tools' is the concept of co-adaptation where the tool becomes functionally involved in the extraction and definition of the user's goals. Recent advancements in the neuroscience and engineering of neuroprosthetics are providing a blueprint for how new co-adaptive designs based on reinforcement learning change the nature of a user's ability to accomplish tasks that were not possible using conventional methodologies. By designing adaptive controls and artificial intelligence into the neural interface, tools can become active assistants in goal-directed behavior and further enhance human performance in particular for the disabled population. This paper presents recent advances in computational and neural systems supporting the development of symbiotic neuroprosthetic assistants. {\textcopyright} 2009 Elsevier Ltd. All rights reserved.},
  doi       = {10.1016/j.neunet.2009.03.015},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanchez et al. - 2009 - Exploiting co-adaptation for the design of symbiotic neuroprosthetic assistants.pdf:pdf},
  isbn      = {0893-6080},
  keywords  = {Brain-computer symbiosis,Brain-machine interface,Perception-action cycle,Reinforcement learning,Tool use},
  pmid      = {19403263},
  publisher = {Elsevier Ltd},
  url       = {http://dx.doi.org/10.1016/j.neunet.2009.03.015},
}

@Article{ko2017vision,
  author   = {Ko, Eunjeong and Kim, Eun Yi},
  title    = {{A vision-based wayfinding system for visually impaired people using situation awareness and activity-based instructions}},
  journal  = {Sensors (Switzerland)},
  year     = {2017},
  volume   = {17},
  number   = {8},
  issn     = {14248220},
  abstract = {A significant challenge faced by visually impaired people is ‘wayfinding', which is the ability to find one's way to a destination in an unfamiliar environment. This study develops a novel wayfinding system for smartphones that can automatically recognize the situation and scene objects in real time. Through analyzing streaming images, the proposed system first classifies the current situation of a user in terms of their location. Next, based on the current situation, only the necessary context objects are found and interpreted using computer vision techniques. It estimates the motions of the user with two inertial sensors and records the trajectories of the user toward the destination, which are also used as a guide for the return route after reaching the destination. To efficiently convey the recognized results using an auditory interface, activity-based instructions are generated that guide the user in a series of movements along a route. To assess the effectiveness of the proposed system, experiments were conducted in several indoor environments: the sit in which the situation awareness accuracy was 90{\%} and the object detection false alarm rate was 0.016. In addition, our field test results demonstrate that users can locate their paths with an accuracy of 97{\%}.},
  doi      = {10.3390/s17081882},
  file     = {:home/jaycee/PhD Papers/sensors-17-01882-v2.pdf:pdf},
  keywords = {Activity-based instruction,Situation awareness,User trajectory recording,Visually impaired people,Wayfinding system},
}

@Article{rovira2017reinforcement,
  author    = {Rovira, Aitor and Slater, Mel},
  title     = {{Reinforcement Learning as a tool to make people move to a specific location in Immersive Virtual Reality}},
  journal   = {International Journal of Human Computer Studies},
  year      = {2017},
  volume    = {98},
  number    = {July 2015},
  pages     = {89--94},
  issn      = {10959300},
  abstract  = {This paper describes the use of Reinforcement Learning in Immersive Virtual Reality to make a person move to a specific location in a virtual environment. Reinforcement Learning is a sub-area of Machine Learning in which an active entity called an agent interacts with its environment and learns how to act in order to achieve a pre-determined goal. The Reinforcement Learning had no prior model of behaviour and the participants no prior knowledge that their task was to move to and stay in a specific place. The participants were placed in a virtual environment where they had to avoid collisions with virtual projectiles. Following each projectile the agent analysed the movement made by the participant to determine paths of future projectiles in order to increase the chance of driving participants to the goal position and make them stay there as long as possible. The experiment was carried out with 30 participants, 10 were guided towards the leftmost part of the environment, 10 to the rightmost area, and 10 were used as control group where the projectiles were shot randomly throughout the game. Our results show that people tended to stay close to the target area in both the Left and Right conditions, but not in the Random condition.},
  doi       = {10.1016/j.ijhcs.2016.10.007},
  file      = {:home/jaycee/PhD Papers/1-s2.0-S1071581916301513-main.pdf:pdf},
  keywords  = {Immersive Virtual Reality,Reinforcement Learning},
  publisher = {Elsevier},
  url       = {http://dx.doi.org/10.1016/j.ijhcs.2016.10.007},
}

@Article{katz2012navig,
  author   = {Katz, Brian F.G. and Kammoun, Slim and Parseihian, Ga{\"{e}}tan and Gutierrez, Olivier and Brilhault, Adrien and Auvray, Malika and Truillet, Philippe and Denis, Michel and Thorpe, Simon and Jouffrais, Christophe},
  title    = {{NAVIG: Augmented reality guidance system for the visually impaired: Combining object localization, GNSS, and spatial audio}},
  journal  = {Virtual Reality},
  year     = {2012},
  volume   = {16},
  number   = {4},
  pages    = {253--269},
  issn     = {13594338},
  abstract = {Navigating complex routes and finding objects of interest are challenging tasks for the visually impaired. The project NAVIG (Navigation Assisted by artificial VIsion and GNSS) is directed toward increasing personal autonomy via a virtual augmented reality system. The system integrates an adapted geographic information system with different classes of objects useful for improving route selection and guidance. The database also includes models of important geolocated objects that may be detected by real-time embedded vision algorithms. Object localization (relative to the user) may serve both global positioning and sensorimotor actions such as heading, grasping, or piloting. The user is guided to his desired destination through spatialized semantic audio rendering, always maintained in the head-centered reference frame. This paper presents the overall project design and architecture of the NAVIG system. In addition, details of a new type of detection and localization device are presented. This approach combines a bio-inspired vision system that can recognize and locate objects very quickly and a 3D sound rendering system that is able to perceptually position a sound at the location of the recognized object. This system was developed in relation to guidance directives developed through participative design with potential users and educators for the visually impaired. {\textcopyright} 2012 Springer-Verlag London Limited.},
  file     = {:home/jaycee/PhD Papers/10.1007{\%}2Fs10055-012-0213-6.pdf:pdf},
  isbn     = {1005501202136},
  keywords = {Assisted navigation,Guidance,Need analysis,Spatial audio,Visually impaired assistive device},
}

@Article{spaan2010cooperative,
  author   = {Spaan, Matthijs T.J.},
  title    = {{Cooperative Active Perception using POMDPs}},
  journal  = {AAAI 2008 workshop on advancements in POMDP solvers},
  year     = {2010},
  pages    = {4800--4805},
  issn     = {21530858},
  abstract = {This paper studies active perception in an urban sce- nario, focusing on the cooperation between a set of surveillance cameras and mobile robots. The fixed cameras provide a global but incomplete and possi- bly inaccurate view of the environment, which can be enhanced by a robot's local sensors. Active percep- tion means that the robot considers the effects of its actions on its sensory capabilities. In particular, it tries to improve its sensors' performance, for instance by pointing a pan-and-tilt camera. In this paper, we present a decision-theoretic approach to cooperative ac- tive perception, by formalizing the problem as a Par- tially Observable Markov Decision Process (POMDP). POMDPs provide an elegant way to model the interac- tion of an active sensor with its environment. The goal of this paper is to provide first steps towards an inte- grated decision-theoretic approach of cooperative active perception.},
  file     = {:home/jaycee/PhD Papers/Spaan08pomdp.pdf:pdf},
  isbn     = {9781424466764},
  keywords = {ai reasoning methods,autonomous agents,networked robots},
}

@Article{general2016listener,
  author = {General, I},
  title  = {{a Listener Is Not Permitted Free Movement of His}},
  year   = {2016},
  number = {December 1967},
  pages  = {1255--1259},
  file   = {:home/jaycee/PhD Papers/roffler1968.pdf:pdf},
}

@Article{zwiers2001spatial,
  author   = {Zwiers, M P and {Van Opstal}, A J and Cruysberg, J R M and Opstal, A J Van and Cruysberg, J R M},
  title    = {{A spatial hearing deficit in early-blind humans}},
  journal  = {J.Neurosci.},
  year     = {2001},
  volume   = {21},
  number   = {1529-2401},
  pages    = {RC142--RC145},
  issn     = {1529-2401},
  abstract = {An important issue in neuroscience is the effect of visual loss on the remaining senses. Two opposing views have been advanced. On the one hand, visual loss may lead to compensatory plasticity and sharpening of the remaining senses. On the other hand, early blindness may also prevent remaining sensory modalities from a full development. In the case of sound localization, it has been reported recently that, under certain conditions, early-blind humans can localize sounds better than sighted controls. However, these studies were confined to a single sound source in the horizontal plane. This study compares sound localization of early-blind and sighted subjects in both the horizontal and vertical domain, whereas background noise was added to test more complex hearing conditions. The data show that for high signal-to-noise (S/N) ratios, localization by blind and sighted subjects is similar for both azimuth and elevation. At decreasing S/N ratios, the accuracy of the elevation response components deteriorated earlier than the accuracy of the azimuth component in both subject groups. However, although azimuth performance was identical for the two groups, elevation accuracy deteriorated much earlier in the blind subject group. These results indicate that auditory hypercompensation in early-blind humans does not extend to the frontal target domain, where the potential benefit of vision is maximal. Moreover, the results demonstrate for the first time that in this domain the human auditory system may require vision to optimally calibrate the elevation-related spectral pinna cues. Sensitivity to azimuth-encoding binaural difference cues, however, may be adequately calibrated in the absence of vision},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zwiers et al. - 2001 - A spatial hearing deficit in early-blind humans.pdf:pdf},
  isbn     = {1529-2401 (Electronic)},
  keywords = {Acoustic Stimulation,Adult,Blindness,Cues,Feedback,Female,Hand,Head Movements,Hearing,Hearing Disorders,Human,Linear Models,Male,Neuronal Plasticity,Noise,Non-U.S.Gov't,Perceptual Masking,Sound,Sound Localization,Support,Vision,auditory,auditory system,calibration of spatial maps,compensatory plastic-,complications,diagnosis,early blindness,human,ity,methods,more robust against background,noise than the detection,of,physiology,physiopathology,scene analysis,signal-to-noise,sound localization,visual feedback},
  pmid     = {11312316},
}

@Article{walker2006navigation,
  author   = {Walker, Bruce N. and Lindsay, Jeffrey},
  title    = {{Navigation Performance With a Virtual Auditory Display: Effects of Beacon Sound, Capture Radius, and Practice}},
  journal  = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
  year     = {2006},
  volume   = {48},
  number   = {2},
  pages    = {265--278},
  issn     = {0018-7208},
  abstract = {OBJECTIVE: We examined whether spatialized nonspeech beacons could guide navigation and how sound timbre, waypoint capture radius, and practice affect performance. BACKGROUND: Auditory displays may assist mobility and wayfinding for those with temporary or permanent visual impairment, but they remain understudied. Previous systems have used speech-based interfaces. METHOD: Participants (108 undergraduates) navigated three maps, guided by one of three beacons (pink noise, sonar ping, or 1000-Hz pure tone) spatialized by a virtual reality engine. Dependent measures were efficiency of time and path length. RESULTS: Overall navigation was very successful, with significant effects of practice and capture radius, and interactions with beacon sound. Overshooting and subsequent hunting for waypoints was exacerbated for small radius conditions. A human-scale capture radius (1.5 m) and sonar-like beacon yielded the optimal combination for safety and efficiency. CONCLUSION: The selection of beacon sound and capture radius depend on the specific application, including whether speed of travel or adherence to path are of primary concern. Extended use affects sound preferences and quickly leads to improvements in both speed and accuracy. APPLICATION: These findings should lead to improved wayfinding systems for the visually impaired as well as for first responders (e.g., firefighters) and soldiers.},
  doi      = {10.1518/001872006777724507},
  file     = {:home/jaycee/PhD Papers/001872006777724507.pdf:pdf},
}

@InProceedings{wang2017enabling,
  author    = {Wang, Hsueh-Cheng and Katzschmann, Robert K and Araki, Brandon and Teng, Santani and Giarre, Laura and Rus, Daniela},
  title     = {{Enabling Independent Navigation for Visually Impaired People through a Wearable Vision-Based Feedback System}},
  booktitle = {Proceedings of the International Conference on Robotics and Automation (ICRA)},
  year      = {2017},
  pages     = {6533--6540},
  publisher = {IEEE},
  abstract  = {This work introduces a wearable system to provide situational awareness for blind and visually impaired people. The system includes a camera, an embedded computer and a haptic device to provide feedback when an obstacle is detected. The system uses techniques from computer vision and motion planning to (1) identify walkable space; (2) plan step-by-step a safe motion in the space, and (3) recognize and locate certain types of objects, for example the location of an empty chair. These descriptions are communicated to the person wearing the device through vibrations. We present results from user studies with low- and high-level tasks, including walking through a maze without collisions, locating a chair, and walking through a crowded environment while avoiding people.},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2017 - Enabling Independent Navigation for Visually Impaired People through a Wearable Vision-Based Feedback System.pdf:pdf},
  isbn      = {9781509046324},
  journal   = {IEEE International Conference on Robotics and Automation (ICRA)},
}

@Article{david2014navigation,
  author    = {David, Sina and Schmitt, Syn and Utz, Julianne and Hub, Andreas and Schlicht, Wolfgang},
  title     = {{Navigation within buildings: Novel movement detection algorithms supporting people with visual impairments}},
  journal   = {Research in Developmental Disabilities},
  year      = {2014},
  volume    = {35},
  number    = {9},
  pages     = {2026--2034},
  issn      = {18733379},
  abstract  = {This study aimed at finding simple algorithms to identify three different movements registered by accelerometer and to detect differences in the acceleration signals of people with and without visual impairments.The Tactile Acoustical Navigation and Information Assistant (TANIA) is construed to provide persons suffering from visual impairments support for an independent navigation indoors and outdoors. Attaining this goal, TANIA uses vertical acceleration signal extrema to assess its user's walking distance. This study investigated first the sit-to-stand movement, stumbling and walking up- and down stairs of 25 subjects with visual impairments using TANIA sensor system. The objective was to improve the user's movement detection using sensors to get valid and reliable data. In a second step of the study it was investigated if there is a difference between the above-mentioned movements in people with or without visual impairments (n= 10). The acceleration signals of the subjects were compared.Three simple algorithms were found, which are able to separate the movement signals based on accelerometers of the respective daily movements. The second step analysis revealed a detectable difference in the second phase of stumbling (p= .034), where the subjects had to get back into walking forward. No differences in the other acceleration signals were found. {\textcopyright} 2014 Elsevier Ltd.},
  doi       = {10.1016/j.ridd.2014.04.032},
  file      = {:home/jaycee/PhD Papers/1-s2.0-S0891422214002005-main.pdf:pdf},
  keywords  = {Accelerometer,Movement detection,Navigation,Visual impairment},
  publisher = {Elsevier Ltd.},
  url       = {http://dx.doi.org/10.1016/j.ridd.2014.04.032},
}

@Article{rebillat2009smart,
  author   = {R{\'{e}}billat, Marc and Katz, Brian F.G. and Corteel, Etienne},
  title    = {{SMART-I2: "Spatial multi-user audio-visual real-time interactive interface", a broadcast application context}},
  journal  = {3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, Proceedings},
  year     = {2009},
  pages    = {2--5},
  abstract = {SMART-I2 is a high quality 3D audio-visual interactive ren- dering system. In SMART-I2, the screen is also used as a multichannel loudspeaker. The spatial audio rendering is based onWave Field Synthesis, an approach that creates a coherent spatial perception of a spatial sound scene over a large listen- ing area. The azimuth localization accuracy of the system has been verified by a perceptual experiment. Contrary to conventional systems, SMART-I2 is able to realize a high degree of 3D audio-visual integration with almost no compromise on either the audio or the graphics rendering quality. Such a system can provide benefits to a wide range of applications.},
  file     = {:home/jaycee/PhD Papers/05069682.pdf:pdf},
  isbn     = {9781424443185},
  keywords = {Audio-visual integration,Large multi-actuator panels,Virtual reality environments,Wave field synthesis},
}

@Article{mitsunaga2005robot,
  author   = {Mitsunaga, N. and Smith, C. and Kanda, T. and Ishiguro, H. and Hagita, N.},
  title    = {{Robot behavior adaptation for human-robot interaction based on policy gradient reinforcement learning}},
  journal  = {2005 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year     = {2005},
  pages    = {218--225},
  issn     = {0289-1824},
  abstract = {In this paper, we propose an adaptation mechanism for robot behaviors to make robot-human interactions run more smoothly. We propose such a mechanism based on reinforcement learning, which reads minute body signals from a human partner, and uses this information to adjust interaction distances, gaze meeting, and motion speed and timing in human-robot interaction. We show that this enables autonomous adaptation to individual preferences by an experiment with twelve subjects.},
  doi      = {10.1109/IROS.2005.1545206},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitsunaga et al. - 2005 - Robot behavior adaptation for human-robot interaction based on policy gradient reinforcement learning.pdf:pdf},
  isbn     = {0-7803-8912-3},
  pmid     = {8734645},
  url      = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1545206},
}

@Article{manduchi2012mobile,
  author  = {Manduchi, Roberto},
  title   = {{Mobile Vision as Assistive Technology for the Blind : An Experimental Study}},
  journal = {Conference on Computers for Handicapped Persons},
  year    = {2012},
  file    = {:home/jaycee/PhD Papers/qt3m14r6gb.pdf:pdf},
}

@Article{ammi2015intermodal,
  author    = {Ammi, Mehdi and Katz, Brian F.G.},
  title     = {{Intermodal audio–haptic intermodal display: improvement of communication and interpersonal awareness for collaborative search tasks}},
  journal   = {Virtual Reality},
  year      = {2015},
  volume    = {19},
  number    = {3-4},
  pages     = {235--252},
  issn      = {14349957},
  abstract  = {This paper studies a new sensorial approach to improving communication between partners during collaborative tasks taking place in abstract and non-visual virtual reality environments. The sensorial approach was investigated in the context of the search and identification of targets in a simplified 2D environment. It consists in finding a spatial configuration corresponding to a defined criterion such as a maximum, minimum, or defined score. During the collaborative search, users need to be aware of their results and also the results of their partners. In addition, they need to compare examined scores (e.g., docking score or physical value) with other results to make decisions. To support these features, an audio–haptic display was developed employing binaural audio with an intermodal stimuli synthesis design to improve the collaborative search. This rendering tool allows for simultaneous use of the audio and haptic channels which enables an efficient individual search and comparison of results. In addition, it improves the communication and activity coordination between the partners. An experiment was carried out to evaluate the contribution of the tool to improve the collaborative search of targets in a 2D non-visual environment. The results clearly show a significant improvement in performance and working efficiency with the audio–haptic display as compared to a traditional haptic-only condition. Moreover, we observed a reduction in the need for verbal communication during some steps of the search process. However, this approach introduces some communication conflicts during the steps presenting high-level interactions between partners which reduce the working efficiency of some groups. {\textcopyright} 2015, Springer-Verlag London.},
  file      = {:home/jaycee/PhD Papers/10.1007{\%}2Fs10055-015-0273-5.pdf:pdf},
  keywords  = {Audio–haptic interaction,Collaborative virtual environment,Search of targets,Sonification},
  publisher = {Springer London},
}

@InProceedings{siddhartha2018electronic,
  author    = {Siddhartha, B. and Chavan, Arunkumar P. and Uma, B. V.},
  title     = {{An electronic smart jacket for the navigation of visually impaired society}},
  booktitle = {Proceedings of Materials Today},
  year      = {2018},
  volume    = {5},
  number    = {4},
  pages     = {10665--10669},
  publisher = {Elsevier Ltd},
  abstract  = {According to World Health Organisation (WHO) study, 90{\%} of the info to the human brain is sent through eyes alone. The Study also says that there are about 285 million visually impaired or blind people across world. In this paper, we proposed an efficient, reliable and low-cost wearable jacket for the people suffering from visual impaired. A smart jacket is designed by embedding the sensor on the jacket, that enables the user to detect an obstacle and safely navigate. The prototype model has an accuracy of 98{\%} for obstacle with in 200cm. The smart jacket requires low power hence can be used for real time navigation for visually impaired people.},
  doi       = {10.1016/j.matpr.2017.12.344},
  file      = {:home/jaycee/PhD Papers/1-s2.0-S2214785317333503-main.pdf:pdf},
  issn      = {22147853},
  keywords  = {Navigational Aid,Smart Jacket,Visually Impaired},
  url       = {https://doi.org/10.1016/j.matpr.2017.12.344},
}

@Article{stanley2009measurement,
  author = {Stanley, Raymond M},
  title  = {{MEASUREMENT AND VALIDATION OF BONE-CONDUCTION .pdf}},
  year   = {2009},
  number = {August},
  file   = {:home/jaycee/PhD Papers/fc41b527f7eae5d0dd5ce0c78b172f58045e.pdf:pdf},
}

@Article{schalk2008two,
  author   = {Schalk, G. and Miller, K. J. and Anderson, N. R. and Wilson, J. A. and Smyth, M. D. and Ojemann, J. G. and Moran, D. W. and Wolpaw, J. R. and Leuthardt, E. C.},
  title    = {{Two-dimensional movement control using electrocorticographic signals in humans}},
  journal  = {Journal of Neural Engineering},
  year     = {2008},
  volume   = {5},
  number   = {1},
  pages    = {75--84},
  issn     = {17412560},
  abstract = {We show here that a brain-computer interface (BCI) using electrocorticographic activity (ECoG) and imagined or overt motor tasks enables humans to control a computer cursor in two dimensions. Over a brief training period of 12-36 min, each of five human subjects acquired substantial control of particular ECoG features recorded from several locations over the same hemisphere, and achieved average success rates of 53-73{\%} in a two-dimensional four-target center-out task in which chance accuracy was 25{\%}. Our results support the expectation that ECoG-based BCIs can combine high performance with technical and clinical practicality, and also indicate promising directions for further research.},
  doi      = {10.1088/1741-2560/5/1/008},
  file     = {:home/jaycee/PhD Papers/schalk2008.pdf:pdf},
}

@Article{nicol2014roadmap,
  author   = {Nicol, Rozenn and Gros, Laetitia and Colomes, Cathy and Warusfel, Olivier and Noisternig, Markus and Bahu, H{\'{e}}l{\`{e}}ne and Katz, Brian FG and Simon, Laurent S. R.},
  title    = {{A Roadmap for Assessing the Quality of Experience of 3D Audio Binaural Rendering}},
  journal  = {Proceedings of the EAA Joint Symposium on Auralization and Ambisonics},
  year     = {2014},
  pages    = {100--106},
  issn     = {1532-1827},
  abstract = {Today there are 2 major evolutions in spatial audio. First, an enhanced 3D audio experience, where virtual sound sources can be accurately synthesized in any direction, is possible with technolo- gies such as binaural, Wave Field Synthesis, Higher Order Am- bisonics or Vector Base Amplitude Panning. Second, 3D audio is on the way to being democratized through binaural adaptation for headphone listening. These evolutions call for revisiting the methods and tools used to assess the perception of spatial sound reproduction. The first objective of this paper is to delineate the problem, by exploring the potential dimensions and the related at- tributes underlying the perception of spatial sound, mainly within the context of binaural reproduction. Secondly, assessment meth- ods, including both standard and less conventional ones, are listed, and their relevance for the measure of the attributes previously identified is discussed.},
  doi      = {10.1038/bjc.2011.405},
  file     = {:home/jaycee/PhD Papers/16.pdf:pdf},
  isbn     = {0007-0920},
  pmid     = {21970878},
}

@Article{giudice2014touch,
  author   = {Giudice, Nicholas A. and Loomis, Jack M. and Klatzky, Roberta L. and Bennett, Christopher R.},
  title    = {{Touch-Screen Technology for the Dynamic Display of 2D Spatial Information Without Vision: Promise and Progress}},
  journal  = {Multisensory Research},
  year     = {2014},
  volume   = {27},
  number   = {5-6},
  pages    = {359--378},
  issn     = {2213-4794},
  abstract = {Many developers wish to capitalize on touch-screen technology for developing aids for the blind, particularly by incorporating vibrotactile stimulation to convey patterns on their surfaces, which otherwise are featureless. Our belief is that they will need to take into account basic research on haptic perception in designing these graphics interfaces. We point out constraints and limitations in haptic processing that affect the use of these devices. We also suggest ways to use sound to augment basic information from touch, and we include evaluation data from users of a touch-screen device with vibrotactile and auditory feedback that we have been developing, called a vibro-audio interface.},
  doi      = {10.1163/22134808-00002447},
  file     = {:home/jaycee/PhD Papers/klatzky2014.pdf:pdf},
  isbn     = {978-1-60558-733-2},
  keywords = {Perception,auditory,blind,graphics,graphs,multi-modal,sensory substitution,touch,touch-screen,vibration},
  url      = {http://booksandjournals.brillonline.com/content/journals/10.1163/22134808-00002447},
}

@TechReport{day2005using,
  author      = {Day, R. and Holland, S. and Bowers, D. and Dil, A.},
  title       = {{Using Spatial Audio in Minimal Attention Interfaces: Towards An Effective Audio GPS Navigation System}},
  institution = {The Open University},
  year        = {2005},
  file        = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Day et al. - 2005 - Using Spatial Audio in Minimal Attention Interfaces Towards An Effective Audio GPS Navigation System.pdf:pdf},
  pages       = {2004/08},
}

@Article{digiovanna2009coadaptive,
  author   = {DiGiovanna, J. and Mahmoudi, Babak and Fortes, Jose and Principe, J.C. and Sanchez, J.C.},
  title    = {{Coadaptive Brain - Machine Interface via Reinforcement Learning}},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2009},
  volume   = {56},
  number   = {1},
  pages    = {54--64},
  issn     = {0018-9294},
  abstract = {This paper introduces and demonstrates a novel brain-machine interface (BMI) architecture based on the concepts of reinforcement learning (RL), coadaptation, and shaping. RL allows the BMI control algorithm to learn to complete tasks from interactions with the environment, rather than an explicit training signal. Coadaption enables continuous, synergistic adaptation between the BMI control algorithm and BMI user working in changing environments. Shaping is designed to reduce the learning curve for BMI users attempting to control a prosthetic. Here, we present the theory and in vivo experimental paradigm to illustrate how this BMI learns to complete a reaching task using a prosthetic arm in a 3-D workspace based on the user's neuronal activity. This semisupervised learning framework does not require user movements. We quantify BMI performance in closed-loop brain control over six to ten days for three rats as a function of increasing task difficulty. All three subjects coadapted with their BMI control algorithms to control the prosthetic significantly above chance at each level of difficulty.},
  doi      = {10.1109/TBME.2008.926699},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/DiGiovanna et al. - 2009 - Coadaptive Brain - Machine Interface via Reinforcement Learning.pdf:pdf},
  keywords = {Brain{\&}{\#}x2013,Brain-Machine Interfaces,Co-adaptation,Neuroprosthetic,Reinforcement Learning,coadaptation,machine interface (BMI),neuroprosthetic,reinforcement learning (RL)},
  pmid     = {19224719},
}

@Article{xiao2015assistive,
  author    = {Xiao, Jizhong and Joseph, Samleo L. and Zhang, Xiaochen and Li, Bing and Li, Xiaohai and Zhang, Jianwei},
  title     = {{An Assistive Navigation Framework for the Visually Impaired}},
  journal   = {IEEE Trans. on Human-Machine Systems},
  year      = {2015},
  volume    = {45},
  number    = {5},
  pages     = {635--640},
  issn      = {21682291},
  abstract  = {This paper provides a framework for context-aware navigation services for vision impaired people. Integrating advanced intelligence into navigation requires knowledge of the semantic properties of the objects around the user's environment. This interaction is required to enhance communication about objects and places to improve travel decisions. Our intelligent system is a human-in-the-loop cyber-physical system that interprets ubiquitous semantic entities by interacting with the physical world and the cyber domain, viz., 1) visual cues and distance sensing of material objects as line-of-sight interaction to interpret location-context information, and 2) data (tweets) from social media as event-based interaction to interpret situational vibes. The case study elaborates our proposed localization methods (viz., topological, landmark, metric, crowdsourced, and sound localization) for applications in way finding, way confirmation, user tracking, socialization, and situation alerts. Our pilot evaluation provides a proof of concept for an assistive navigation system. {\textcopyright} 2014 IEEE.},
  file      = {:home/jaycee/PhD Papers/07010003.pdf:pdf},
  keywords  = {Cyber-physical system,navigation,situation awareness,visually impaired},
  publisher = {IEEE},
}

@Article{radmard2017active,
  author    = {Radmard, Sina and Croft, Elizabeth A.},
  title     = {{Active target search for high dimensional robotic systems}},
  journal   = {Autonomous Robots},
  year      = {2017},
  volume    = {41},
  number    = {1},
  pages     = {163--180},
  issn      = {15737527},
  abstract  = {When a robotic visual servoing/tracking system loses sight of the target, the servo fails due to loss of input. To resolve this problem a search method, namely a lost target search (LTS) which will generate efficient actions to bring the target back into the camera field of view (FoV) as soon as possible, is required. For high dimensional platforms, like a camera-mounted manipulator or an eye-in-hand system, such a search must address the difficult challenge of generating efficient actions in an online manner while avoiding kinematic constraints. In this work, we utilize the latest available information from the target just prior to leaving the FoV to initiate an optimal online search. We explain various features of our overall LTS algorithm and provide simulation comparisons with common methods existing in the literature. Finally, we implement and demonstrate the capabilities of our general algorithm on a laboratory scale 7 degree of freedom (DoF) eye-in-hand system tracking a fast moving target.},
  doi       = {10.1007/s10514-015-9539-8},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Radmard, Croft - 2017 - Active target search for high dimensional robotic systems.pdf:pdf},
  keywords  = {High dimensional robot,Lost target search,Online sensor planning},
  publisher = {Springer US},
}

@Article{shubina2010visual,
  author    = {Shubina, Ksenia and Tsotsos, John K.},
  title     = {{Visual search for an object in a 3D environment using a mobile robot}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2010},
  volume    = {114},
  number    = {5},
  pages     = {535--547},
  issn      = {10773142},
  abstract  = {Consider the problem of visually finding an object in a mostly unknown space with a mobile robot. It is clear that all possible views and images cannot be examined in a practical system. Visual attention is a complex phenomenon; we view it as a mechanism that optimizes the search processes inherent in vision (Tsotsos, 2001; Tsotsos et al., 2008) [1,2]. Here, we describe a particular example of a practical robotic vision system that employs some of these attentive processes. We cast this as an optimization problem, i.e., optimizing the probability of finding the target given a fixed cost limit in terms of total number of robotic actions required to find the visual target. Due to the inherent intractability of this problem, we present an approximate solution and investigate its performance and properties. We conclude that our approach is sufficient to solve this problem and has additional desirable empirical characteristics. {\textcopyright} 2010 Elsevier Inc. All rights reserved.},
  doi       = {10.1016/j.cviu.2009.06.010},
  file      = {:home/jaycee/PhD Papers/1-s2.0-S1077314210000378-main.pdf:pdf},
  keywords  = {Active object recognition,Active vision,Mobile robot,Search,Sensor planning,Visual attention,Visual search},
  publisher = {Elsevier Inc.},
  url       = {http://dx.doi.org/10.1016/j.cviu.2009.06.010},
}

@Article{rizzo2018new,
  author    = {Rizzo, John Ross and Conti, Kyle and Thomas, Teena and Hudson, Todd E. and {Wall Emerson}, Robert and Kim, Dae Shik},
  title     = {{A new primary mobility tool for the visually impaired: A white cane—adaptive mobility device hybrid}},
  journal   = {Assistive Technology},
  year      = {2018},
  volume    = {30},
  number    = {5},
  pages     = {219--225},
  issn      = {19493614},
  abstract  = {This article describes pilot testing of an adaptive mobility device-hybrid (AMD-H) combining properties of two primary mobility tools for people who are blind: the long cane and adaptive mobility devices (AMDs). The long cane is the primary mobility tool used by people who are blind and visually impaired for independent and safe mobility and AMDs are adaptive devices that are often lightweight frames approximately body width in lateral dimension that are simply pushed forward to clear the space in front of a person. The prototype cane built for this study had a wing apparatus that could be folded around the shaft of a cane but when unfolded, deployed two wheeled wings 25 cm (9.8 in) to each side of the canetip. This project explored drop-off and obstacle detection for 6 adults with visual impairment using the deployed AMD-H and a standard long cane. The AMD-H improved obstacle detection overall, and was most effective for the smallest obstacles (2 and 6 inch diameter). The AMD-H cut the average drop off threshold from 1.79 inches (4.55 cm) to .96 inches (2.44 cm). All participants showed a decrease in drop off detection threshold and an increase in detection rate (13.9{\%} overall). For drop offs of 1 in (2.54 cm) and 3 in (7.62 cm), all participants showed large improvements with the AMD-H, ranging from 8.4 to 50{\%}. The larger drop offs of 5 in (12.7 cm) and 7 in (17.8 cm) were well detected by both types of canes.},
  doi       = {10.1080/10400435.2017.1312634},
  file      = {:home/jaycee/PhD Papers/A new primary mobility tool for the visually impaired A white cane adaptive mobility device hybrid.pdf:pdf},
  keywords  = {adaptive cane,blind,cane,mobility},
  publisher = {Taylor {\&} Francis},
  url       = {https://doi.org/10.1080/10400435.2017.1312634},
}

@Article{data2006double,
  author = {Data, Technical},
  title  = {{Double Coated Urethane Foam Tapes}},
  year   = {2006},
  pages  = {1--5},
  file   = {:home/jaycee/PhD Papers/01423934.pdf:pdf},
}

@Article{kramer1999sonification,
  author   = {Kramer, G and Walker, B and Bonebright, T and Cook, P and Flowers, J and Miner, N and Neuhoff, J and Bargar, R and Barrass, S and Berger, J and Evreinov, G and Fitch, W and Gr{\"{o}}hn, M and Handel, S and Kaper, H and Levkowitz, H and Lodha, S and Shinn-Cunningham, B and Simoni, M and Tipei, S},
  title    = {{Sonification Report: Status of the Field and Research Agenda}},
  journal  = {Psychiatry and Psychology Commons},
  year     = {1999},
  number   = {March},
  pages    = {1--30},
  abstract = {The purpose of this paper is to provide an overview of sonification research, including the current status of the field and a proposed research agenda. This paper was prepared by an interdisciplinary group of researchers gathered at the request of the National Science Foundation in the fall of 1997 in association with the International Conference on Auditory Display (ICAD).},
  file     = {:home/jaycee/PhD Papers/fulltext.pdf:pdf},
  keywords = {Auditory Display,Cognition,Perception,Sonification},
  url      = {http://sonify.psych.gatech.edu/publications/pdfs/1999-NSF-Report.pdf},
}

@Article{schinazi2016spatial,
  author   = {Schinazi, Victor R. and Thrash, Tyler and Chebat, Daniel Robert},
  title    = {{Spatial navigation by congenitally blind individuals}},
  journal  = {Wiley Interdisciplinary Reviews: Cognitive Science},
  year     = {2016},
  volume   = {7},
  number   = {1},
  pages    = {37--58},
  issn     = {19395086},
  abstract = {{\textcopyright} 2016 Wiley Periodicals, Inc.Spatial navigation in the absence of vision has been investigated from a variety of perspectives and disciplines. These different approaches have progressed our understanding of spatial knowledge acquisition by blind individuals, including their abilities, strategies, and corresponding mental representations. In this review, we propose a framework for investigating differences in spatial knowledge acquisition by blind and sighted people consisting of three longitudinal models (i.e., convergent, cumulative, and persistent). Recent advances in neuroscience and technological devices have provided novel insights into the different neural mechanisms underlying spatial navigation by blind and sighted people and the potential for functional reorganization. Despite these advances, there is still a lack of consensus regarding the extent to which locomotion and wayfinding depend on amodal spatial representations. This challenge largely stems from methodological limitations such as heterogeneity in the blind population and terminological ambiguity related to the concept of cognitive maps. Coupled with an over-reliance on potential technological solutions, the field has diffused into theoretical and applied branches that do not always communicate. Here, we review research on navigation by congenitally blind individuals with an emphasis on behavioral and neuroscientific evidence, as well as the potential of technological assistance. Throughout the article, we emphasize the need to disentangle strategy choice and performance when discussing the navigation abilities of the blind population. WIREs Cogn Sci 2016, 7:37-58. doi: 10.1002/wcs.1375 For further resources related to this article, please visit the WIREs website.},
  doi      = {10.1002/wcs.1375},
  file     = {:home/jaycee/PhD Papers/Schinazi{\_}et{\_}al-2016-Wiley{\_}Interdisciplinary{\_}Reviews{\_}{\_}Cognitive{\_}Science.pdf:pdf},
}

@Article{tapu2015survey,
  author    = {Tapu, Ruxandra and Mocanu, Bogdan and Tapu, Ermina},
  title     = {{A survey on wearable devices used to assist the visual impaired user navigation in outdoor environments}},
  journal   = {2014 11th International Symposium on Electronics and Telecommunications, ISETC 2014 - Conference Proceedings},
  year      = {2015},
  pages     = {1--4},
  abstract  = {{\textcopyright} 2014 IEEE. In this paper we introduce a comprehensive survey of wearable systems designed to assist the visual impaired users navigation in everyday life outdoor scenarios. We focus on presenting the main advantages and limitations of each technique in effort to inform the scientific community about the progress in the area of assistive devices and also offer users a review about the capabilities of each system. Various performance parameters are introduced in order to classify different systems by giving qualitative and quantitative measures for evaluation. At the end of the study conclusions are presented along with some perspectives for future work and development.},
  doi       = {10.1109/ISETC.2014.7010793},
  file      = {:home/jaycee/PhD Papers/07010793.pdf:pdf},
  isbn      = {9781479972654},
  keywords  = {assistive outdoor navigation,blind and visually impaired users,wearable devices},
  publisher = {IEEE},
}

@Article{loomis2016navigation,
  author   = {Loomis, Jack M and Golledge, Reginald G and Klatzky, Roberta L},
  title    = {{Navigation System for the Blind}},
  journal  = {Interdisciplinary Research Journal - IJIR},
  year     = {2016},
  volume   = {7},
  number   = {2},
  pages    = {193--204},
  issn     = {10547460},
  abstract = {The research we are reporting here is part of our effort to develop a navigation sys- tem for the blind. Our long-term goal is to create a portable, self-contained system that will allow visually impaired individuals to travel through familiar and unfamiliar environments without the assistance of guides. The system, as it exists now, consists of the following functional components: (1) a module for determining the traveler's posi- tion and orientation in space, (2) a Geographic Information System comprising a de- tailed database of our test site and software for route planning and for obtaining infor- mation from the database, and (3) the user interface. The experiment reported here is concerned with one function of the navigation system: guiding the traveler along a predefined route. We evaluate guidance performance as a function of four different display modes: one involving spatialized sound from a virtual acoustic display, and three involving verbal commands issued by a synthetic speech display. The virtual dis- play mode fared best in terms of both guidance performance and user preferences.},
  file     = {:home/jaycee/PhD Papers/loomis1998.pdf:pdf},
}

@Article{kantarci2012inter,
  author   = {Kantarci, Burak and Foschini, Luca and Corradi, Antonio and Mouftah, Hussein T.},
  title    = {{Inter-and-intra data center VM-placement for energy-efficient large-Scale cloud systems}},
  journal  = {2012 IEEE Globecom Workshops, GC Wkshps 2012},
  year     = {2012},
  pages    = {708--713},
  abstract = {Data centers play the crucial role in the delivery of Cloud services by enabling on-demand access to the shared resources such as software, platform, and infrastructure. Virtual Machine (VM) allocation is one of the challenging tasks in the data center management since user requirements, typically expressed as SLAs, have to be met with the minimum operational expenditure. Despite their huge processing and storage facilities, data centers are among the major contributors of the GreenHouse Gas (GHG) emissions of the IT services. In this paper, we propose a holistic approach for a large-scale Cloud system where the Cloud services are provisioned by several data centers interconnected over the backbone network. We propose a Mixed Integer Linear Programming (MILP) formulation that aims at virtualizing the backbone topology and placing the VMs in data centers with the objective of minimum power consumption. We compare our proposed solution with a benchmark MILP model which selects the closest data centers that can accommodate the user requests taking into account the CPU, memory, and bandwidth capacities of the residing physical hosts. Collected experimental results show the benefit of the proposed management scheme in terms of power consumption, resource utilization, and fairness for medium size data centers. {\textcopyright} 2012 IEEE.},
  doi      = {10.1109/GLOCOMW.2012.6477661},
  file     = {:home/jaycee/PhD Papers/live-301-1562-jair.pdf:pdf},
  isbn     = {9781467349413},
  keywords = {Cloud computing,VM placement,data center management,energy efficiency,virtual topology},
}

@Article{wachsmuth2007metamodel,
  author   = {Wachsmuth, Guido},
  title    = {{Metamodel Adaptation and Model Co-adaptation}},
  journal  = {ECOOP 2007 – Object-Oriented Programming},
  year     = {2007},
  pages    = {600--624},
  issn     = {03029743},
  abstract = {Like other software artefacts, metamodels evolve over time. We propose a transformational approach to assist metamodel evolution by stepwise adaptation. In the first part of the paper, we adopt ideas from grammar engineering to define several semantics- and instance-preservation properties in terms of metamodel relations. This part is not restricted to any metamodel formalism. In the second part, we present a library of QVT Relations for the stepwise adaptation of MOF compliant metamodels. Transformations from this library separate preservation properties. We distinguish three kinds of adaptation according to these properties; namely refactoring, construction, and destruction. Co-adaptation of models is discussed with respect to instance-preservation. In most cases, co-adaptation is achieved automatically. Finally, we point out applications in the areas of metamodel design, implementation, refinement, maintenance, and recovery.},
  doi      = {10.1007/978-3-540-73589-2_28},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wachsmuth - 2007 - Metamodel Adaptation and Model Co-adaptation.pdf:pdf},
  isbn     = {3-540-73588-7, 978-3-540-73588-5},
  url      = {http://link.springer.com/10.1007/978-3-540-73589-2{\_}28},
}

@Article{kim2016interaction,
  author    = {Kim, Hyun K. and Han, Sung H. and Park, Jaehyun and Park, Joohwan},
  title     = {{The interaction experiences of visually impaired people with assistive technology: A case study of smartphones}},
  journal   = {International Journal of Industrial Ergonomics},
  year      = {2016},
  volume    = {55},
  pages     = {22--33},
  issn      = {18728219},
  abstract  = {Globally, the number of visually impaired people is large and increasing. Many assistive technologies are being developed to help visually impaired people, because they still have difficulty accessing assistive technologies that have been developed from a technology-driven perspective. This study applied a user-centered perspective to get different and hopefully deeper understanding of the interaction experiences. More specifically, this study focused on identifying the unique interaction experiences of visually impaired people when they use a camera application on a smartphone. Twenty participants conducted usability testing using the retrospective think aloud technique. The unique interaction experiences of visually impaired people with the camera application, and relevant implications for designing assistive technologies were analyzed. Relevance to industry The considerations for conducting usability testing and the results of this study are expected to contribute to the design and evaluation of new assistive technologies based on smartphones.},
  doi       = {10.1016/j.ergon.2016.07.002},
  file      = {:home/jaycee/PhD Papers/1-s2.0-S0169814116300634-main.pdf:pdf},
  keywords  = {Accessibility design and evaluation methods,Design guidelines,Empirical studies in accessibility,Interaction experiences,Smartphones,Visually impaired people},
  publisher = {Elsevier B.V},
  url       = {http://dx.doi.org/10.1016/j.ergon.2016.07.002},
}

@Article{ye2018gaple,
  author        = {Ye, Xin and Lin, Zhe and Lee, Joon-Young and Zhang, Jianming and Zheng, Shibin and Yang, Yezhou},
  title         = {{GAPLE}: Generalizable Approaching Policy LEarning for Robotic Object Searching in Indoor Environment},
  journal       = {CoRR},
  year          = {2018},
  abstract      = {We study the problem of learning a generalizable action policy for an intelligent agent to actively approach an object of interest in indoor environment solely from its visual inputs. While scene-driven or recognition-driven visual navigation has been widely studied, prior efforts suffer severely from the limited generalization capability. In this paper, we first argue the object searching task is environment dependent while the approaching ability is general. To learn a generalizable approaching policy, we present a novel solution dubbed as GAPLE which adopts two channels of visual features: depth and semantic segmentation, as the inputs to the policy learning module. The empirical studies conducted on the House3D dataset as well as on a physical platform in a real world scenario validate our hypothesis, and we further provide in-depth qualitative analysis.},
  archiveprefix = {arXiv},
  arxivid       = {1809.08287},
  eprint        = {1809.08287},
  file          = {:home/jaycee/PhD Papers/1809.08287.pdf:pdf},
  url           = {http://arxiv.org/abs/1809.08287},
}

@Article{menelas2014non,
  author   = {Menelas, Bob Antoine J. and Picinali, Lorenzo and Bourdot, Patrick and Katz, Brian F.G.},
  title    = {{Non-visual identification, localization, and selection of entities of interest in a 3D environment}},
  journal  = {Journal on Multimodal User Interfaces},
  year     = {2014},
  volume   = {8},
  number   = {3},
  pages    = {243--256},
  issn     = {17838738},
  abstract = {This paper addresses the use of audio and haptics as a mean to reduce the load of the visual channel in interaction tasks within virtual environments. An examination is made regarding the exploitation of audio and/or haptic interactions for the acquisition of a target of interest in an environment containing multiple and obscured distractors. A first study compares means for identifying and locating a specified target among others employing either audio, haptic, or both sensori-motor channels activated simultaneously. Following an analysis of the results and subject comments, an improved multimodal approach is proposed and evaluated in a second study, combining advantages offered by each sensory channel. Results confirm the efficiency and effectiveness of the proposed multimodal approach.},
  doi      = {10.1007/s12193-014-0148-1},
  file     = {:home/jaycee/PhD Papers/10.1007{\%}2Fs12193-014-0148-1.pdf:pdf},
  keywords = {Audio,Haptic audio interaction,Haptic identification,Haptic selection,Haptics,Multi-target,Multimodal interaction,Non-visual interaction,Sonification},
}

@Article{jansaputramullercarmenvidaurremartijnschreuderfrankc.meineckepaulvonbunau2017mathematical,
  author  = {{Jan Saputra Muller, Carmen Vidaurre, Martijn Schreuder, Frank C. Meinecke, Paul von Bunau}, and Klaus-Robert Muller},
  title   = {{A mathematical model for the two-learners problem}},
  journal = {Journal of Neural Engineering},
  year    = {2017},
  volume  = {14},
  number  = {3},
  file    = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jan Saputra Muller, Carmen Vidaurre, Martijn Schreuder, Frank C. Meinecke, Paul von Bunau - 2017 - A mathematical model for the two-lear.pdf:pdf},
}

@Article{bullock1995co,
  author   = {Bullock, Seth},
  title    = {{Co-evolutionary design: Implications for evolutionary robotics}},
  journal  = {Cognitive Science Research Paper 384},
  year     = {1995},
  number   = {384},
  pages    = {1--10},
  abstract = {Genetic Algorithms (GAs) typically work on static fitness landscapes. In contrast, natural evolution works on fitness landscapes that change over evolutionary time as a result of (amongst other things) co-evolution. The attractions of co-evolutionary design techniques are discussed, and attempts to utilise co-evolution in the use of GAs as design tools are reviewed, before the implications of natural predator-prey co-evolution are considered. Utilising strict definitions of true and diffuse co-evolution provided by Janzen (1980), a distinction is drawn between two styles of evolutionary niche, Predator and Parasite. The former niche is robust with respect to environmental change and features systems that have had to solve evolutionary problems in ways that reveal general purpose design principles, whilst the nature of the latter is such that, despite being fragile and unsatisfactory in these respects, it is nevertheless evolutionarily successful. It is contested that if co-e...},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bullock - 1995 - Co-evolutionary design Implications for evolutionary robotics.pdf:pdf},
  keywords = {4,6 june 1995,and,and a bursary from,cial life,co-evolution,european conference on arti,evolutionary robotics,granada,niches,parasitism,predator-prey evolution,presented at the 3rd,research was supported by,spain,the baldwin bewdley trust,the school of cognitive,this},
  url      = {http://eprints.ecs.soton.ac.uk/11471/},
}

@Article{shani2005adaptation,
  author  = {Shani, G and Brafman, R and Shimony, S},
  title   = {{{\{}A{\}}daptation for changing stochastic environments through online {\{}POMDP{\}} policy learning}},
  journal = {{\{}P{\}}roceedings of the {\{}W{\}}orkshop on {\{}R{\}}einforcement {\{}L{\}}earning in {\{}N{\}}on-{\{}S{\}}tationary {\{}E{\}}nvironments},
  year    = {2005},
  pages   = {61--70},
  file    = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shani, Brafman, Shimony - 2005 - {\{}A{\}}daptation for changing stochastic environments through online {\{}POMDP{\}} policy learning.pdf:pdf},
}

@Article{taha2011pomdp,
  author   = {Taha, Tarek and Miro, Jaime Valls and Dissanayake, Gamini},
  title    = {{A POMDP framework for modelling human interaction with assistive robots}},
  journal  = {Proceedings - IEEE International Conference on Robotics and Automation},
  year     = {2011},
  pages    = {544--549},
  issn     = {10504729},
  abstract = {This paper presents a framework for modelling the interaction between a human operator and a robotic device, that enables the robot to collaborate with the human to jointly accomplish tasks. States of the system are captured in a model based on a partially observable Markov decision process (POMDP). States representing the human operator are motivated by behaviours from the psychology of the human action cycle. Hierarchical nature of these states allows the exploitation of data structures based on algebraic decision diagrams (ADD) to efficiently solve the resulting POMDP. The proposed framework is illustrated using two examples from assistive robotics; a robotic wheel chair and an intelligent walking device. Experimental results from trials conducted in an office environment with the wheelchair is used to demonstrate the proposed technique.},
  doi      = {10.1109/ICRA.2011.5980323},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Taha, Miro, Dissanayake - 2011 - A POMDP framework for modelling human interaction with assistive robots.pdf:pdf},
  isbn     = {9781612843865},
}

@Article{innalaahlmark2015initial,
  author   = {{Innala Ahlmark}, Daniel and Prellwitz, Maria and R{\"{o}}ding, Jenny and Nyberg, Lars and Hyypp{\"{a}}, Kalevi},
  title    = {{An initial field trial of a haptic navigation system for persons with a visual impairment}},
  journal  = {Journal of Assistive Technologies},
  year     = {2015},
  volume   = {9},
  number   = {4},
  pages    = {199--206},
  issn     = {20428723},
  abstract = {Purpose   - The purpose of this paper is to describe conceptions of feasibility of a haptic navigation system for persons with a visual impairment (VI).   Design/methodology/approach   - Six persons with a VI who were white cane users were tasked with traversing a predetermined route in a corridor environment using the haptic navigation system. To see whether white cane experience translated to using the system, the participants received no prior training. The procedures were video-recorded, and the participants were interviewed about their conceptions of using the system. The interviews were analyzed using content analysis, where inductively generated codes that emerged from the data were clustered together and formulated into categories.   Findings   - The participants quickly figured out how to use the system, and soon adopted their own usage technique. Despite this, locating objects was difficult. The interviews highlighted the desire to be able to feel at a distance, with several scenarios presented to illustrate current problems. The participants noted that their previous white cane experience helped, but that it nevertheless would take a lot of practice to master using this system. The potential for the device to increase security in unfamiliar environments was mentioned. Practical problems with the prototype were also discussed, notably the lack of auditory feedback.   Originality/value   - One novel aspect of this field trial is the way it was carried out. Prior training was intentionally not provided, which means that the findings reflect immediate user experiences. The findings confirm the value of being able to perceive things beyond the range of the white cane; at the same time, the participants expressed concerns about that ability. Another key feature is that the prototype should be seen as a navigation aid rather than an obstacle avoidance device, despite the interaction similarities with the white cane. As such, the intent is not to replace the white cane as a primary means of detecting obstacles.},
  doi      = {10.1108/JAT-01-2015-0002},
  file     = {:home/jaycee/PhD Papers/JAT-01-2015-0002.pdf:pdf},
  keywords = {Feasibility,Field trial,Haptics,Navigation aid,Usability,Visual impairment},
}

@Article{lewis2015hearing,
  author    = {Lewis, Laura and Sharples, Sarah and Chandler, Ed and Worsfold, John},
  title     = {{Hearing the way: Requirements and preferences for technology-supported navigation aids}},
  journal   = {Applied Ergonomics},
  year      = {2015},
  volume    = {48},
  pages     = {56--69},
  issn      = {18729126},
  abstract  = {Many systems have been developed to assist wayfinding for people with sight problems. There is a need for user requirements for such systems to be defined. This paper presents a study which aimed to determine such user requirements. An experiment was also conducted to establish the best way of guiding users between locations. The focus group results indicated that users require systems to provide them with information about their surroundings, to guide them along their route and to provide progress information. They also showed that users with sight conditions interact with systems differently to sighted users, thereby highlighting the importance of designing systems for the needs of these users. Results of the experiment found that the preferred method of guiding users was a notification when they were both on and off track. However, performance was best when only provided with the off track notification, implying that this cue is particularly important. Technology has the potential to support navigation for people with sight problems. Users should have control over cues provided and for these cues should supplement environmental cues rather than replacing them.},
  doi       = {10.1016/j.apergo.2014.11.004},
  file      = {:home/jaycee/PhD Papers/1-s2.0-S0003687014002555-main.pdf:pdf},
  keywords  = {Blind and partially sighted,Navigation,Wayfinding},
  publisher = {Elsevier Ltd},
  url       = {http://dx.doi.org/10.1016/j.apergo.2014.11.004},
}

@Article{nguyen2016recognition,
  author   = {Nguyen, Thi Hoa Cuc and Nebel, Jean Christophe and Florez-Revuelta, Francisco},
  title    = {{Recognition of activities of daily living with egocentric vision: A review}},
  journal  = {Sensors (Switzerland)},
  year     = {2016},
  volume   = {16},
  number   = {1},
  issn     = {14248220},
  abstract = {Video-based recognition of activities of daily living (ADLs) is being used in ambient assisted living systems in order to support the independent living of older people. However, current systems based on cameras located in the environment present a number of problems, such as occlusions and a limited field of view. Recently, wearable cameras have begun to be exploited. This paper presents a review of the state of the art of egocentric vision systems for the recognition of ADLs following a hierarchical structure: motion, action and activity levels, where each level provides higher semantic information and involves a longer time frame. The current egocentric vision literature suggests that ADLs recognition is mainly driven by the objects present in the scene, especially those associated with specific tasks. However, although object-based approaches have proven popular, object recognition remains a challenge due to the intra-class variations found in unconstrained scenarios. As a consequence, the performance of current systems is far from satisfactory.},
  doi      = {10.3390/s16010072},
  file     = {:home/jaycee/PhD Papers/nguyen2016.pdf:pdf},
  isbn     = {4477568266},
  keywords = {Activity recognition,Ambient assisted living,Egocentric vision,Wearable cameras},
}

@InProceedings{quintas2017context,
  author    = {Quintas, Joao and Menezes, Paulo and Dias, Jorge},
  title     = {{Context-based decision system for human-machine interaction applications}},
  booktitle = {Proceedings of the International Conference on Systems, Man, and Cybernetics},
  year      = {2017},
  pages     = {3906--3911},
  publisher = {IEEE},
  doi       = {10.1109/SMC.2016.7844844},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Quintas, Menezes, Dias - 2017 - Context-based decision system for human-machine interaction applications.pdf:pdf},
  isbn      = {9781509018970},
  journal   = {2016 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2016 - Conference Proceedings},
  keywords  = {Auto-adaptive interfaces,Context-awareness,Decision processes,Human detection,Human-Machine Interaction},
}

@Article{kan-kilic2017way,
  author   = {Kan-Kilic, Didem and Dogan, Fehmi},
  title    = {{Way-finding strategies of blind persons in urban scale}},
  journal  = {PsyCh Journal},
  year     = {2017},
  volume   = {6},
  number   = {4},
  pages    = {303--315},
  issn     = {20460260},
  abstract = {{\textcopyright} 2017 The Institute of Psychology, Chinese Academy of Sciences and John Wiley  {\&}  Sons Australia, Ltd The aim of this study was to determine whether urban environments with different prominent sensory inputs have an impact on the way-finding strategies of blind people and to identify these impacts, where applicable. We specifically investigated how blind people use their senses to compensate for the lack of visual information and how the priority of senses changes according to the urban context. The participants of the study consisted of nine congenitally blind individuals and the study took place in two urban settings: a dense urban district, Kemeralti district in İzmir; and an urban park, the İzmir Fair Park. During the learning phase, a first trial along the selected routes was conducted for each participant individually along with one of the researchers. In the test phase, the participants were requested to re-walk the route and verbally report the environmental cues they attended to. The participants' verbal reports were recorded and transcripts of the recordings were coded according to the environmental sensory inputs. In addition, the short-term memory of each participant was also evaluated. The results show that the characteristics of the urban environment seem to have an impact on way-finding strategies of blind individuals. It was found that the sound of the city and the echo from the environment are the most important factors for blind participants in the dense urban environment. Environmental boundaries provided echoes and gave a sense of enclosure that helped them orient themselves, whereas, in the park environment, the sense of enclosure was not enhanced due to a lack of boundaries in the environment.},
  doi      = {10.1002/pchj.187},
  file     = {:home/jaycee/PhD Papers/Kan-Kilic{\_}et{\_}al-2017-PsyCh{\_}Journal.pdf:pdf},
  keywords = {blind persons,sense of enclosure,senses,way-finding},
}

@Article{yang1997human,
  author   = {Yang, Jie and Xu, Yangsheng and Chen, Chiou S.},
  title    = {{Human action learning via hidden Markov model}},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans.},
  year     = {1997},
  volume   = {27},
  number   = {1},
  pages    = {34--44},
  issn     = {10834427},
  abstract = {To successfully interact with and learn from humans in cooperative modes, robots need a mechanism for recognizing, characterizing, and emulating human skills. In particular, it is our interest to develop the mechanism for recognizing and emulating simple human actions, i.e., a simple activity in a manual operation where no sensory feedback is available. To this end, we have developed a method to model such actions using a hidden Markov model (HMM) representation. We proposed an approach to address two critical problems in action modeling: classifying human action-intent, and learning human skill, for which we elaborated on the method, procedure, and implementation issues in this paper. This work provides a framework for modeling and learning human actions from observations. The approach can be applied to intelligent recognition of manual actions and high-level programming of control input within a supervisory control paradigm, as well as automatic transfer of human skills to robotic systems. {\textcopyright} 1997 IEEE.},
  doi      = {10.1109/3468.553220},
  file     = {:home/jaycee/PhD Papers/00553220.pdf:pdf},
}

@Article{lewald2002vertical,
  author   = {Lewald, J{\"{o}}rg},
  title    = {{Vertical sound localization in blind humans}},
  journal  = {Neuropsychologia},
  year     = {2002},
  volume   = {40},
  number   = {12},
  pages    = {1868--1872},
  issn     = {00283932},
  abstract = {It is widely held that early-blind people compensate their visual loss by a general sharpening of spatial hearing. The present study reports a possible exception to this view: when the vertical position (elevation) of a sound source had to be localized, four out of six early-blind subjects exhibited systematic deviations in pointing, while two early-blind subjects were as accurate as sighted controls. On the other hand, blind and sighted individuals were able to judge relative positions of different sound locations with similar precision. These results suggest that visual experience may be used to accurately calibrate the relation between the vertical coordinates of auditory space and body, but is not needed to develop sufficiently high resolution of spatial hearing. {\textcopyright} 2002 Elsevier Science Ltd. All rights reserved.},
  doi      = {10.1016/S0028-3932(02)00071-4},
  file     = {:home/jaycee/PhD Papers/1-s2.0-S0028393202000714-main.pdf:pdf},
  isbn     = {4923432020},
  keywords = {Auditory system,Blindness,Compensatory plasticity,Hearing,Sound elevation,Space perception,Visual calibration},
}

@Article{spaan2010active,
  author   = {Spaan, Matthijs T.J. and Veiga, Tiago S. and Lima, Pedro U.},
  title    = {{Active cooperative perception in network robot systems using POMDPs}},
  journal  = {IEEE/RSJ 2010 International Conference on Intelligent Robots and Systems, IROS 2010 - Conference Proceedings},
  year     = {2010},
  pages    = {4800--4805},
  abstract = {Network robot systems (NRS) provide many scientific and technological challenges, given that robots interact with each other as well as with sensors present in the environment to accomplish certain tasks. In this work, we consider an essential problem in NRS, namely how to perform task planning given the limitations both in on-board sensing as well as in the environment's sensors. Partially observable Markov decisions processes (POMDPs) form an attractive framework to address planning in the uncertain environments that typify NRS. We show how to model a typical cooperative perception task in a NRS, namely tracking and classifying people, and we present experiments that show how the proposed approach results in an effective interplay between robot and environment sensors.},
  doi      = {10.1109/IROS.2010.5648856},
  file     = {:home/jaycee/PhD Papers/spaan2010.pdf:pdf},
  isbn     = {9781424466757},
  keywords = {AI Reasoning Methods,Autonomous Agents,Networked Robots},
}

@Article{rossetti2002new,
  author   = {Rossetti, Yves and Pisella, Laure and P{\'{e}}lisson, Denis},
  title    = {{New insights on eye blindness and hand sight: Temporal constraints of visuo-motor networks}},
  journal  = {Visual Cognition},
  year     = {2002},
  volume   = {7},
  number   = {6},
  pages    = {785--809},
  issn     = {1350-6285},
  abstract = {Pioneer experiments on saccadic suppression have shown that this effect is not followed by motor disorientation: Conscious perception of a target displacement can be dissociated from correct manual target reaching. It has subsequently been demonstrated that movement corrections with the same latency and spatial characteristics can be produced in conditions allowing perceptual awareness of perturbation of a target as in condition inducing saccadic suppression. In addition to the qualitative dissociation between motor performance and conscious awareness, quantitative temporal dissociations in action can be observed by manipulating different features of the visual target. When the target of an ongoing simple action is perturbed, a temporal advantage is found for responses to perturbations of location relative to colour and shape. Furthermore, there seems to be a temporal advantage for automatic motor corrections made in response to a target displacement as compared to other responses (other ongoing movement adjustments, movement interruption, conditional motor response such as pressing a key, verbal response, delayed matching-to-sample tasks). Thus, this paper reviews evidence for the fact that the temporal characteristics of any given response to a stimulus are dependent both on the sensory processes and on the type of response generated. Accordingly, identification responses (such as verbal report) to a visual stimulus are much slower than motor corrections of an ongoing movement in response to a target location change because of different processing times of the stimulus features ('What' compared to 'Where') and of the response itself ('What' compared to 'How'). The existence of two continua (What/Where and What/How) is proposed between these two extreme stimulus response combinations. This model may be a useful framework to better understand visuo-motor transformations and the network of connections between visual and motor areas.},
  doi      = {10.1080/13506280050144434},
  file     = {:home/jaycee/PhD Papers/New insights on eye blindness and hand sight Temporal constraints of visuo motor networks.pdf:pdf},
}

@Article{boger2005decision,
  author   = {Boger, Jennifer and Poupart, Pascal and Hoey, Jesse and Boutilier, Craig and Fernie, Geoff and Mihailidis, Alex},
  title    = {{A decision-theoretic approach to task assistance for persons with dementia}},
  journal  = {IJCAI International Joint Conference on Artificial Intelligence},
  year     = {2005},
  pages    = {1293--1299},
  issn     = {10450823},
  abstract = {Cognitive assistive technologies that aid people with dementia (such as Alzheimer's disease) hold the promise to provide such people with an increased level of independence. However, to realize this promise, such systems must account for the specific needs and preferences of individuals. We argue that this form of customization requires a sequential, decision-theoretic model of interaction. We describe both fully and partially observable Markov decision process (POMDP) models of$\backslash$na handwashing task, and show that, despite the potential computational complexity, these can be effectively solved and produce policies that are evaluated as useful by professional caregivers.},
  file     = {:home/jaycee/PhD Papers/10.1.1.72.9543.pdf:pdf},
  isbn     = {0938075934},
  pmid     = {3696022},
}

@Article{vision2013uk,
  author = {2020, Vision},
  title  = {{UK Vision Strategy 2013-2018}},
  year   = {2013},
  file   = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2020 - 2013 - UK Vision Strategy 2013-2018.pdf:pdf},
}

@Article{ahmaniemi2009augmented,
  author   = {Ahmaniemi, Teemu Tuomas and Lantz, Vuokko Tuulikki},
  title    = {{Augmented Reality Target Finding Based on Tactile Cues}},
  journal  = {Proceedings of the 2009 International Conference on Multimodal Interfaces},
  year     = {2009},
  pages    = {335--342},
  abstract = {Abstract This study is based on a user scenario where augmented reality targets could be found by scanning the environment with a mobile device and getting a tactile feedback exactly in the direction of the target . In order to understand how accurately and quickly the ... $\backslash$n},
  doi      = {10.1145/1647314.1647383},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahmaniemi, Lantz - 2009 - Augmented Reality Target Finding Based on Tactile Cues.pdf:pdf},
  isbn     = {978-1-60558-772-1},
  keywords = {Fitts' Law,augmented reality,haptics,pointing},
  url      = {http://doi.acm.org/10.1145/1647314.1647383},
}

@Article{lessard1998early,
  author   = {Lessard, N. and Par{\'{e}}, M. and Lepore, F. and Lassonde, M.},
  title    = {{Early-blind human subjects localize sound sources better than sighted subjects}},
  journal  = {Nature},
  year     = {1998},
  volume   = {395},
  number   = {6699},
  pages    = {278--280},
  issn     = {00280836},
  abstract = {Do blind persons develop capacities of their remaining senses that exceed those of sighted individuals? Besides anecdotal suggestions, two views based on experimental studies have been advanced. The first proposes that blind individuals should be severely impaired, given that vision is essential to develop spatial concepts. The second suggests that compensation occurs through the remaining senses, allowing them to develop an accurate concept of space. Here we investigate how an ecologically critical function, namely three-dimensional spatial mapping, is carried out by early-blind individuals with or without residual vision. Subjects were tested under monaural and binaural listening conditions. We find that early-blind subjects can map the auditory environment with equal or better accuracy than sighted subjects. Furthermore, unlike sighted subjects, they can correctly localize sounds monaurally. Surprisingly, blind individuals with residual peripheral vision localized sounds less precisely than sighted or totally blind subjects, confirming that compensation varies according to the aetiology and extent of blindness. Our results resolve a long-standing controversy in that they provide behavioural evidence that totally blind individuals have better auditory ability than sighted subjects, enabling them to compensate for their loss of vision.},
  doi      = {10.1038/26228},
  file     = {:home/jaycee/PhD Papers/out (1).pdf:pdf},
}

@Article{bakker2006quasi,
  author   = {Bakker, Bram and Zhumatiy, Viktor and Gruener, Gabriel and Schmidhuber, J{\"{u}}rgen},
  title    = {{Quasi-online reinforcement learning for robots}},
  journal  = {Proceedings - IEEE International Conference on Robotics and Automation},
  year     = {2006},
  volume   = {2006},
  number   = {May},
  pages    = {2997--3002},
  issn     = {10504729},
  abstract = {This paper describes quasi-online reinforcement learning: while a robot is exploring its environment, in the background a probabilistic model of the environment is built on the fly as new experiences arrive; the policy is trained concurrently based on this model using an anytime algorithm. Prioritized sweeping, directed exploration, and transformed reward functions provide additional speed-ups. The robot quickly learns goal-directed policies from scratch, requiring few interactions with the environment and making efficient use of available computation time. From an outside perspective it learns the behavior online and in real time. We describe comparisons with standard methods and show the individual utility of each of the proposed techniques},
  doi      = {10.1109/ROBOT.2006.1642157},
  file     = {:home/jaycee/PhD Papers/01642157.pdf:pdf},
  isbn     = {0780395069},
}

@Misc{engineering1984st,
  author    = {Engineering, Biological},
  title     = {{St I Dt {\~{}} / + Mt}},
  year      = {1984},
  booktitle = {Medical {\&} Biological Engineering},
  file      = {:home/jaycee/PhD Papers/479857.pdf:pdf},
  number    = {March},
}

@Article{golledge2004stated,
  author   = {Golledge, Reginald G and Marston, James R and Loomis, Jack M and Klatzky, Roberta L},
  title    = {{Stated Preferences for Components of a Personal Guidance System for Nonvisual Navigation}},
  journal  = {Journal of Visual Impairment {\&} Blindness,},
  year     = {2004},
  volume   = {98},
  number   = {3},
  pages    = {135--147},
  abstract = {This article reports on a survey of the preferences of visually impaired persons for a personal navigation device. The results showed that the majority of participants preferred speech input and output interfaces, were willing to use such a product, thought that they would make more trips with such a device, and had some concerns about the cosmetic acceptability of a device and the use of a single or stereo headphone interface. (PsycINFO Database Record (c) 2012 APA, all rights reserved) (journal abstract).},
  file     = {:home/jaycee/PhD Papers/0145482x0409800304.pdf:pdf},
}

@Article{walker2005navigation,
  author   = {Walker, B N and Lindsay, J},
  title    = {{Navigation performance in a virtual environment with bonephones}},
  journal  = {Proceedings of the 11th International Conference on Auditory Display ({\{}ICAD{\}}2005)},
  year     = {2005},
  pages    = {260--263},
  abstract = {Audio navigation interfaces have traditionally been studied (and implemented)$\backslash$nusing headphones. However, many potential users (especially those$\backslash$nwith visual impairments) are hesitant to adopt these emerging wayfinding$\backslash$ntechnologies if doing so requires them to reduce their ability to$\backslash$nhear environmental sounds by wearing headphones. In this study we$\backslash$nexamined the performance of the SWAN audio navigation interface using$\backslash$nbone-conduction headphones (``bonephones''), which do not cover the$\backslash$near. Bonephones enabled all participants to complete the navigation$\backslash$ntasks with good efficiencies, though not immediately as effective$\backslash$nas regular headphones. Given the functional success here, and considering$\backslash$nthat the spatialization routines were not optimized for bonephones$\backslash$n(this essentially represents a worst-case scenario), the prospects$\backslash$nare excellent for more widespread usage of bone conduction for auditory$\backslash$nnavigation, and likely for many other auditory displays.},
  file     = {:home/jaycee/PhD Papers/WalkerLindsay2005.pdf:pdf},
  url      = {Proceedings/2005/WalkerLindsay2005.pdf},
}

@Article{demeester2008user,
  author   = {Demeester, Eric and H??ntemann, Alexander and Vanhooydonck, Dirk and Vanacker, Gerolf and {Van Brussel}, Hendrik and Nuttin, Marnix},
  title    = {{User-adapted plan recognition and user-adapted shared control: A Bayesian approach to semi-autonomous wheelchair driving}},
  journal  = {Autonomous Robots},
  year     = {2008},
  volume   = {24},
  number   = {2},
  pages    = {193--211},
  issn     = {09295593},
  abstract = {Many elderly and physically impaired people experience difficulties when maneuvering a powered wheelchair. In order to ease maneuvering, powered wheelchairs have been equipped with sensors, additional computing power and intelligence by various research groups. This paper presents a Bayesian approach to maneuvering assistance for wheelchair driving, which can be adapted to a specific user. The proposed framework is able to model and estimate even complex user intents, i.e. wheelchair maneuvers that the driver has in mind. Furthermore, it explicitly takes the uncertainty on the user's intent into account. Besides during intent estimation, user-specific properties and uncertainty on the user's intent are incorporated when taking assistive actions, such that assistance is tailored to the user's driving skills. This decision making is modeled as a greedy Partially Observable Markov Decision Process (POMDP). Benefits of this approach are shown using experimental results in simulation and on our wheelchair platform Sharioto.},
  doi      = {10.1007/s10514-007-9064-5},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Demeester et al. - 2008 - User-adapted plan recognition and user-adapted shared control A Bayesian approach to semi-autonomous wheelchai.pdf:pdf},
  isbn     = {978-1-4244-0911-2},
  keywords = {Intent estimation,Partially observable Markov decision process,Plan recognition,Shared control,User adaptation,User modeling},
}

@Article{dakopoulos2010wearable,
  author   = {Dakopoulos, Dimitrios and Bourbakis, Nikolaos G.},
  title    = {{Wearable obstacle avoidance electronic travel aids for blind: A survey}},
  journal  = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
  year     = {2010},
  volume   = {40},
  number   = {1},
  pages    = {25--35},
  issn     = {10946977},
  abstract = {The last decades a variety of portable or wearable navigation systems have been developed to assist visually impaired people during navigation in known or unknown, indoor or outdoor environments. There are three main categories of these systems: electronic travel aids (ETAs), electronic orientation aids (EOAs), and position locator devices (PLDs). This paper presents a comparative survey among portable/wearable obstacle detection/avoidance systems (a subcategory of ETAs) in an effort to inform the research community and users about the capabilities of these systems and about the progress in assistive technology for visually impaired people. The survey is based on various features and performance parameters of the systems that classify them in categories, giving qualitative-quantitative measures. Finally, it offers a ranking, which will serve only as a reference point and not as a critique on these systems.},
  doi      = {10.1109/TSMCC.2009.2021255},
  file     = {:home/jaycee/PhD Papers/dakopoulos2010.pdf:pdf},
  keywords = {Electronic travel aids,Navigation systems,Obstacle avoidance,Survey,Wearable systems},
}

@Article{calder2009assistive,
  author   = {Calder, David J.},
  title    = {{Assistive technology interfaces for the blind}},
  journal  = {2009 3rd IEEE International Conference on Digital Ecosystems and Technologies, DEST '09},
  year     = {2009},
  pages    = {318--323},
  abstract = {Assistive technology devices for the blind are portable electronic devices that are either hand-held or worn by the visually impaired user, to warn of obstacles ahead. Many assistive technology devices use ultrasonic pulse-echo techniques to gauge subject to object distance. Some use infrared light transceivers or laser technology to locate and warn of obstacles. These devices exhibit a number of problems, the most significant of which are related to the interface display that conveys navigation/obstacle warning information to the user. Other sensory channels should not be compromised by the device. This is exactly what can happen when, for example, audio signals are used in obstacle warning on/off displays or more significantly in orientation solutions, where continuous streams of synthetically generated stereo sound mask the natural ambient sound cues used by the blind. Despite the challenges, the commendable feature all these assistive device developers have in common is; they are striving to help a section of the population with a severe disability. Even if there is only partial success in this endevour to assist the blind, the small companies that produce these devices all have the right motive. That is a big step in the right direction. The author has attempted to address some of the problems mentioned in this paper by producing a first working prototype. Improvements to this original design form the basis for ongoing prototype development within the DEBI Institute at Curtin University.},
  doi      = {10.1109/DEST.2009.5276752},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Calder - 2009 - Assistive technology interfaces for the blind.pdf:pdf},
  isbn     = {9781424423460},
  keywords = {Ambient sound cues,Assistive technology,Disabled,Infrared,Laser,Long cane,Obstacle warning displays,Portable electronic device,Sensory channels,Sound interface displays,Ultrasonic pulse-echo,Visually impaired},
}

@Article{pounds2009design,
  author   = {Pounds, Paul E. and Mahony, Robert E. and Corke, Peter I.},
  title    = {{Design of a Static Thruster for Microair Vehicle Rotorcraft}},
  journal  = {Journal of Aerospace Engineering},
  year     = {2009},
  volume   = {22},
  number   = {1},
  pages    = {85--94},
  issn     = {0893-1321},
  abstract = {This paper presents a case study of a design for a complete microair$\backslash$nvehicle thruster. Fixed-pitch small-scale rotors, brushless motors,$\backslash$nlithium-polymer cells, and embedded control are combined to produce a$\backslash$nmechanically simple, high-performance thruster with potentially high$\backslash$nreliability. The custom rotor design requires a balance between$\backslash$nmanufacturing simplicity and rigidity of a blade versus its aerodynamic$\backslash$nperformance. An iterative steady-state aeroelastic simulator is used for$\backslash$nholistic blade design. The aerodynamic load disturbances of the$\backslash$nrotor-motor system in normal conditions are experimentally$\backslash$ncharacterized. The motors require fast dynamic response for$\backslash$nauthoritative vehicle flight control. We detail a dynamic compensator$\backslash$nthat achieves satisfactory closed-loop response time. The experimental$\backslash$nrotor-motor plant displayed satisfactory thrust performance and dynamic$\backslash$nresponse.},
  doi      = {10.1061/(ASCE)0893-1321(2009)22:1(85)},
  file     = {::},
  isbn     = {0893-1321},
  keywords = {Aerodynamics,Robotics,Dynamic response},
}

@InProceedings{borenstein1997guidecane,
  author    = {Borenstein, J and Ulrich, I},
  title     = {{The GuideCane - A Computerized Travel Aid for the Active Guidance of Blind Pedestrians}},
  booktitle = {Proc. of IEEE Int. Conf. on Robotics and Automation (ICRA)},
  year      = {1997},
  pages     = {1283--1288},
  annote    = {Pretty irrelevant},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Borenstein, Ulrich - 1997 - The GuideCane - A Computerized Travel Aid for the Active Guidance of Blind Pedestrians.pdf:pdf},
  keywords  = {blind,robot dog},
}

@InProceedings{lorigo1997visually,
  author    = {Lorigo, L.M. and Brooks, R.A. and Grimsou, W.E.L.},
  title     = {{Visually-guided obstacle avoidance in unstructured environments}},
  booktitle = {Proceedings of the International Conference on Intelligent Robot and Systems (IROS)},
  year      = {1997},
  volume    = {1},
  pages     = {373--379 vol.1},
  publisher = {IEEE/RSJ},
  abstract  = {This paper presents an autonomous vision-based obstacle avoidance system. The system consists of three independent vision modules for obstacle detection, each of which is computationally simple and uses a different criterion for detection purposes. These criteria are based on brightness gradients, RGB (red, green, blue) color, and HSV (hue, saturation, value) color, respectively. Selection of which modules are used to command the robot proceeds exclusively from the outputs of the modules themselves. The system is implemented on a small monocular mobile robot and uses very lour resolution images. It has been tested for over 200 hours in diverse environments},
  annote    = {Movement with vision robotics - not relevant},
  doi       = {10.1109/IROS.1997.649086},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lorigo, Brooks, Grimsou - 1997 - Visually-guided obstacle avoidance in unstructured environments.pdf:pdf},
  isbn      = {0780341198},
  keywords  = {modular design,reactive control,space explo-,tion,unstructured,vision-based navigation},
  url       = {http://ieeexplore.ieee.org/xpl/login.jsp?tp={\&}arnumber=649086{\&}url=http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=649086{\%}5Cnhttp://people.csail.mit.edu/brooks/papers/final-iros.pdf},
}

@Article{ansar2003linear,
  author   = {Ansar, Adnan and Daniilidis, Kostas},
  title    = {{Linear pose estimation from points or lines}},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {2003},
  volume   = {25},
  number   = {5},
  pages    = {578--589},
  issn     = {01628828},
  abstract = { Estimation of camera pose from an image of n points or lines with known correspondence is a thoroughly studied problem in computer vision. Most solutions are iterative and depend on nonlinear optimization of some geometric constraint, either on the world coordinates or on the projections to the image plane. For real-time applications, we are interested in linear or closed-form solutions free of initialization. We present a general framework which allows for a novel set of linear solutions to the pose estimation problem for both n points and n lines. We then analyze the sensitivity of our solutions to image noise and show that the sensitivity analysis can be used as a conservative predictor of error for our algorithms. We present a number of simulations which compare our results to two other recent linear algorithms, as well as to iterative approaches. We conclude with tests on real imagery in an augmented reality setup.},
  doi      = {10.1109/TPAMI.2003.1195992},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ansar, Daniilidis - 2003 - Linear pose estimation from points or lines.pdf:pdf},
  isbn     = {0162-8828 VO  - 25},
  keywords = {Absolute orientation,Camera localization,Exterior orientation,Pose estimation},
}

@Article{guerrero2012indoor,
  author   = {Guerrero, Luis a. and Vasquez, Francisco and Ochoa, Sergio F.},
  title    = {{An Indoor Navigation System for the Visually Impaired}},
  journal  = {Sensors},
  year     = {2012},
  volume   = {12},
  number   = {6},
  pages    = {8236--8258},
  issn     = {1424-8220},
  abstract = {Navigation in indoor environments is highly challenging for the severely visually impaired, particularly in spaces visited for the first time. Several solutions have been proposed to deal with this challenge. Although some of them have shown to be useful in real scenarios, they involve an important deployment effort or use artifacts that are not natural for blind users. This paper presents an indoor navigation system that was designed taking into consideration usability as the quality requirement to be maximized. This solution enables one to identify the position of a person and calculates the velocity and direction of his movements. Using this information, the system determines the user's trajectory, locates possible obstacles in that route, and offers navigation information to the user. The solution has been evaluated using two experimental scenarios. Although the results are still not enough to provide strong conclusions, they indicate that the system is suitable to guide visually impaired people through an unknown built environment.},
  doi      = {10.3390/s120608236},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guerrero, Vasquez, Ochoa - 2012 - An Indoor Navigation System for the Visually Impaired.pdf:pdf},
  isbn     = {5626895531},
  keywords = {augmented object,indoor positioning,movement detection,navigation system},
  pmid     = {22969398},
}

@Article{bellotto2013multimodal,
  author   = {Bellotto, Nicola},
  title    = {{A Multimodal Smartphone Interface for Active Perception by Visually Impaired}},
  journal  = {IEEE SMC Int. Workshop on Human-Machine Systems, Cyborgs and Enhancing Devices},
  year     = {2013},
  abstract = {The diffuse availability of mobile devices, such as smartphones and tablets, has the potential to bring substantial benefits to the people with sensory impairments. The solution proposed in this paper is part of an ongoing effort to create an accurate obstacle and hazard detector for the visually impaired, which is embedded in a hand-held device. In particular, it presents a proof of concept for a multimodal interface to control the orientation of a smartphone's camera, while being held by a person, using a combination of vocal messages, 3D sounds and vibrations. The solution, which is to be evaluated experimentally by users, will enable further research in the area of active vision with human-in-the-loop, with potential application to mobile assistive devices for indoor navigation of visually impaired people.},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellotto - 2013 - A Multimodal Smartphone Interface for Active Perception by Visually Impaired.pdf:pdf},
}

@Article{coughlan2006cell,
  author   = {Coughlan, James and Manduchi, Roberto and Shen, Huiying},
  title    = {{Cell phone-based wayfinding for the visually impaired}},
  journal  = {Proc. IMV 2006},
  year     = {2006},
  pages    = {1--15},
  abstract = {. A major challenge faced by the blind and visually impaired population is that of wayfinding – the ability of a person to find his or her way to a given destination. We propose a new wayfinding aid based on a camera cell phone, which is held by the user to find and read aloud specially designed machine-readable signs in the environment (labeling locations such as offices and restrooms). Our main technical innovation is that we have designed these machine-readable signs to be detected and located in fractions of a second on the cell phone CPU, even at a distance of several meters. A linear barcode printed on the sign is read using novel decoding algorithms that are robust to noisy images. The information read from the barcode is then read aloud using pre-recorded or synthetic speech. We have implemented a prototype system on the Nokia 7610 cell phone, and preliminary experiments with blind subjects demonstrate the feasibility of using the system as a real-time wayfinding aid. 2 1},
  annote   = {Using special markers for waypoint guidance},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Coughlan, Manduchi, Shen - 2006 - Cell phone-based wayfinding for the visually impaired(2).pdf:pdf},
  url      = {http://www.ski.org/Rehab/Coughlan{\_}lab/General/Publications/Wayfinding.pdf},
}

@Article{aghaei2016multi,
  author    = {Aghaei, Maedeh and Dimiccoli, Mariella and Radeva, Petia},
  title     = {{Multi-face tracking by extended bag-of-tracklets in egocentric photo-streams}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2016},
  volume    = {149},
  pages     = {146--156},
  issn      = {10773142},
  doi       = {10.1016/j.cviu.2016.02.013},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aghaei, Dimiccoli, Radeva - 2016 - Multi-face tracking by extended bag-of-tracklets in egocentric photo-streams.pdf:pdf},
  keywords  = {Egocentric vision,Face tracking,Low frame rate vid},
  publisher = {Elsevier Inc.},
  url       = {http://linkinghub.elsevier.com/retrieve/pii/S1077314216000679},
}

@Article{aghaei2016multia,
  author    = {Aghaei, Maedeh and Dimiccoli, Mariella and Radeva, Petia},
  title     = {{Multi-face tracking by extended bag-of-tracklets in egocentric photo-streams}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2016},
  volume    = {149},
  pages     = {146--156},
  issn      = {10773142},
  doi       = {10.1016/j.cviu.2016.02.013},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aghaei, Dimiccoli, Radeva - 2016 - Multi-face tracking by extended bag-of-tracklets in egocentric photo-streams.pdf:pdf},
  keywords  = {Egocentric vision,Face tracking,Low frame rate vid},
  publisher = {Elsevier Inc.},
  url       = {http://linkinghub.elsevier.com/retrieve/pii/S1077314216000679},
}

@Article{jiang2016enhanced,
  author    = {Jiang, Hairong and Zhang, Ting and Wachs, Juan P. and Duerstock, Bradley S.},
  title     = {{Enhanced Control of a Wheelchair-Mounted Robotic Manipulator Using 3-D Vision and Multimodal Interaction}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2016},
  volume    = {0},
  pages     = {1--11},
  issn      = {10773142},
  doi       = {10.1016/j.cviu.2016.03.015},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2016 - Enhanced Control of a Wheelchair-Mounted Robotic Manipulator Using 3-D Vision and Multimodal Interaction.pdf:pdf},
  keywords  = {3D vision,Multi-modal interface,Wheelchair mounted},
  publisher = {Elsevier Inc.},
  url       = {http://linkinghub.elsevier.com/retrieve/pii/S1077314216300066},
}

@Article{horne2015semantic,
  author    = {Horne, Lachlan and Alvarez, Jose and McCarthy, Chris and Salzmann, Mathieu and Barnes, Nick},
  title     = {{Semantic labeling for prosthetic vision}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2015},
  volume    = {149},
  pages     = {113--125},
  issn      = {1090235X},
  abstract  = {Current and near-term implantable prosthetic vision systems offer the potential to restore some visual function, but suffer from limited resolution and dynamic range of induced visual percepts. This can make navigating complex environments difficult for users. We introduce semantic labeling as a technique to improve navigation outcomes for prosthetic vision users. We produce a novel egocentric vision dataset to demonstrate how semantic labeling can be applied to this problem. We also improve the speed of semantic labeling with sparse computation of unary potentials, enabling its use in real-time wearable assistive devices. We use simulated prosthetic vision to demonstrate the results of our technique. Our approach allows a prosthetic vision system to selectively highlight specific classes of objects in the user's field of view, improving the user's situational awareness.},
  doi       = {10.1016/j.cviu.2016.02.015},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Horne et al. - 2015 - Semantic labeling for prosthetic vision.pdf:pdf},
  keywords  = {Egocentic vision,Prosthetic vision,Semantic labeling},
  publisher = {Elsevier Inc.},
}

@InProceedings{bouktir2008trajectory,
  author    = {Bouktir, Y. and Haddad, M. and Chettibi, T.},
  title     = {{Trajectory planning for a quadrotor helicopter}},
  booktitle = {2008 Mediterranean Conference on Control and Automation - Conference Proceedings, MED'08},
  year      = {2008},
  pages     = {1258--1263},
  abstract  = {A simple direct method able to generate time-optimal trajectories for a micro quadrotor helicopter is presented. It is based on modeling the quadrotor trajectory as a composition of a parametric function P(lambda) defining the quadrotor path, and a monotonically increasing function lambda(t), specifying the motion on this path. The optimal evolutions of P(lambda) and lambda(t), which are approximated using B-spline functions, are found using a nonlinear optimization technique. The proposed method accounts for the most important constraints inherent to the system behavior, such as underactuation, obstacles avoidance and limits on actuator torques and speeds.},
  annote    = {- Simulation},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bouktir, Haddad, Chettibi - 2008 - Trajectory planning for a quadrotor helicopter.pdf:pdf},
  keywords  = {Dynamics,Nonlinear optimization,Quadrotor,Trajectory},
}

@Article{etallrogeriodossantosalves;alexsoaresdesouza2014multimodal,
  author        = {{et all Rog{\'{e}}rio dos Santos Alves; Alex Soares de Souza} and et all {Rog{\'{e}}rio dos Santos Alves; Alex Soares de Souza}},
  title         = {{Multimodal and Alternative Perception for the Visually Impaired: A Survey Structured}},
  journal       = {Igarss 2014},
  year          = {2014},
  number        = {1},
  pages         = {1--5},
  issn          = {13514180},
  abstract      = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  doi           = {10.1007/s13398-014-0173-7.2},
  eprint        = {arXiv:1011.1669v3},
  file          = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Canal, Escalera, Angulo - 2015 - A real-time Human-Robot Interaction system based on gestures for assistive scenarios(2).pdf:pdf},
  isbn          = {9780874216561},
  keywords      = {high resolution images,research,risks management,sustainable reconstruction},
  pmid          = {15991970},
}

@Article{bologna2009use,
  author   = {Bologna, G. and Deville, B. and Pun, T.},
  title    = {{On the use of the auditory pathway to represent image scenes in real-time}},
  journal  = {Neurocomputing},
  year     = {2009},
  volume   = {72},
  number   = {4-6},
  pages    = {839--849},
  issn     = {09252312},
  abstract = {The See Color interface transforms a small portion of a colored video image into sound sources represented by spatialized musical instruments. Basically, the conversion of colors into sounds is achieved by quantization of the HSL color system. Our purpose is to provide visually impaired individuals with a capability of perception of the environment in real time. In this work we present the system principles of design and several experiments that have been carried out by several blindfolded persons with See ColOr prototypes related to static pictures on a tablet and simple video images. The goal of the first experiment was to identify the colors of static pictures' main features and then to interpret the image scenes. Although learning all instrument sounds in only a training session was too difficult, participants found that colors were helpful to limit the possible image interpretations. The experiments on the analysis of static pictures suggested that the order of magnitude of the slow down factor related to the use of the auditory channel, instead of the visual channel could correspond to the order of magnitude related to the ratio of visual channel capacity to auditory channel capacity. Afterwards, two experiments based on a head mounted camera have been performed. The first experiment pertaining to object manipulation is based on the pairing of colored socks, while the second experiment is related to outdoor navigation with the goal of following a colored serpentine painted on the ground. The "socks" experiment demonstrated that blindfolded individuals were able to accurately match pairs of colored socks. The same participants with the addition of a blind individual successfully followed a red serpentine painted on the ground for more than 80 m. According to task time durations, the order of magnitude of the slow down factor related to the "socks" and "serpentine" experiments could be equal to one. From a cognitive perspective this would be consistent with the fact that these two tasks are simpler than the interpretation of image scenes. ?? 2008 Elsevier B.V. All rights reserved.},
  doi      = {10.1016/j.neucom.2008.06.020},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bologna, Deville, Pun - 2009 - On the use of the auditory pathway to represent image scenes in real-time.pdf:pdf},
  isbn     = {0925-2312},
  keywords = {Blind navigation,Color sonification,Sensory substitution,Sound spatialization},
}

@Article{davison2003real,
  author   = {Davison, Andrew J and Mayol, Walterio W and Murray, David W},
  title    = {{Real-Time Localisation and Mapping with Wearable Active Vision {\pounds}}},
  journal  = {Mixed and Augmented Reality, 2003. Proceedings. The Second IEEE and ACM International Symposium on},
  year     = {2003},
  pages    = {315--316},
  issn     = {0-7695-2006-5},
  abstract = {We present a general method for real-time, vision-only single-camera simultaneous localisation and mapping (SLAM) - an algorithm which is applicable to the localisation of any camera moving through a scene - and study its application to the localisation of a wearable robot with active vision. Starting from very sparse initial scene knowledge, a map of natural point features spanning a section of a room is generated on-the-fly as the motion of the camera is simultaneously estimated in full 3D. Naturally this permits the annotation of the scene with rigidly-registered graphics, but further it permits automatic control of the robot's active camera: for instance, fixation on a particular object can be maintained during extended periods of arbitrary user motion, then shifted at will to another object which has potentially been out of the field of view. This kind of functionality is the key to the understanding or "management" of a workspace which the robot needs to have in order to assist its wearer usefully in tasks. We believe that the techniques and technology developed are of particular immediate value in scenarios of remote collaboration, where a remote expert is able to annotate, through the robot, the environment the wearer is working in.},
  doi      = {10.1109/ISMAR.2003.1240684},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davison, Mayol, Murray - 2003 - Real-Time Localisation and Mapping with Wearable Active Vision {\pounds}.pdf:pdf},
  isbn     = {0769520065},
  keywords = {SLAM,active camera,active vision,algorithm,augmentation,augmented reality,automatic,cameras,image sensors,natural point map,object detection,position control,real-time localization,registered graphics,remote collaboration,remote expert,scene annotation,scene knowledge,section spanning,simultaneous localisation,single-camera localisation,user motion,vision-only localisation,visual sensor,wearable active vision,wearable robot,wearable visual robot},
  url      = {http://ieeexplore.ieee.org/search/srchabstract.jsp?tp={\&}arnumber=1240684{\&}openedRefinements=*{\&}filter=AND(NOT(4283010803)){\&}searchField=Search+All{\&}queryText=?Real-time+localisation+and+mapping+with+wearable+active+vision,},
}

@Misc{rnibp2009future,
  author       = {{Royal National Institute of Blind People}},
  title        = {{Future sight loss UK (1): The economic impact of partial sight and blindness in the UK adult population}},
  howpublished = {http://www.rnib.org.uk/aboutus/Research/statistics},
  month        = {jul},
  year         = {2009},
  annote       = {Backgtound on blindness in UK - Gives motivation for research},
  file         = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Royal National Institute of Blind People - 2009 - Future sight loss UK (1) The economic impact of partial sight and blindness in the UK.pdf:pdf},
}

@Article{grzonka2012fully,
  author  = {Grzonka, Slawomir and Grisetti, Giorgio and Burgard, Wolfram},
  title   = {{A Fully Autonomous Indoor Quadrotor}},
  journal = {IEEE Transactions on Robotics},
  year    = {2012},
  volume  = {28},
  number  = {1},
  pages   = {90--100},
  month   = {feb},
  issn    = {1552-3098},
  doi     = {10.1109/TRO.2011.2162999},
  file    = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Grzonka, Grisetti, Burgard - 2012 - A Fully Autonomous Indoor Quadrotor.pdf:pdf},
  url     = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6004839},
}

@Article{bujacz2011sonification,
  author  = {Bujacz, Michal and Pec, Michal},
  title   = {{Sonification of 3d scenes in an electronic travel aid for the blind}},
  journal = {Advances in Sound Localization},
  year    = {2011},
  pages   = {251--268},
  doi     = {10.5772/15311},
  file    = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bujacz, Pec - 2011 - Sonification of 3d scenes in an electronic travel aid for the blind.pdf:pdf},
  isbn    = {978-953-307-224-1},
  url     = {http://www.intechopen.com/source/pdfs/15119/InTech-Sonification{\_}of{\_}3d{\_}scenes{\_}in{\_}an{\_}electronic{\_}travel{\_}aid{\_}for{\_}the{\_}blind.pdf},
}

@InProceedings{frauenberger20033d,
  author    = {Frauenberger, C and Noisternig, M},
  title     = {{3D Audio Interfaces for the Blind}},
  booktitle = {Proc. of the Int. Conf. on Auditory Display (ICAD)},
  year      = {2003},
  pages     = {280--283},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frauenberger, Noisternig - 2003 - 3D Audio Interfaces for the Blind.pdf:pdf},
}

@InProceedings{heng2011autonomous,
  author    = {Heng, Lionel and Meier, Lorenz and Tanskanen, Petri and Fraundorfer, Friedrich and Pollefeys, Marc},
  title     = {{Autonomous obstacle avoidance and maneuvering on a vision-guided MAV using on-board processing}},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  year      = {2011},
  pages     = {2472--2477},
  abstract  = {We present a novel stereo-based obstacle avoidance system on a vision-guided micro air vehicle (MAV) that is capable of fully autonomous maneuvers in unknown and dynamic environments. All algorithms run exclusively on the vehicle's on-board computer, and at high frequencies that allow the MAV to react quickly to obstacles appearing in its flight trajectory. Our MAV platform is a quadrotor aircraft equipped with an inertial measurement unit and two stereo rigs. An obstacle mapping algorithm processes stereo images, producing a 3D map representation of the environment; at the same time, a dynamic anytime path planner plans a collision-free path to a goal point.},
  annote    = {- Collision avoidance},
  doi       = {10.1109/ICRA.2011.5980095},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Heng et al. - 2011 - Autonomous obstacle avoidance and maneuvering on a vision-guided MAV using on-board processing.pdf:pdf},
  isbn      = {9781612843865},
  issn      = {10504729},
}

@Article{lepetit2008epnp,
  author  = {Lepetit, Vincent and Moreno-Noguer, Francesc and Fua, Pascal},
  title   = {{EPnP: An Accurate O(n) Solution to the PnP Problem}},
  journal = {International Journal of Computer Vision},
  year    = {2008},
  volume  = {81},
  number  = {2},
  pages   = {155--166},
  month   = {jul},
  issn    = {0920-5691},
  doi     = {10.1007/s11263-008-0152-6},
  file    = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lepetit, Moreno-Noguer, Fua - 2008 - EPnP An Accurate O(n) Solution to the PnP Problem.pdf:pdf},
  url     = {http://link.springer.com/10.1007/s11263-008-0152-6},
}

@InProceedings{willis2005rfida,
  author    = {Willis, S and Helal, S},
  title     = {{RFID information grid for blind navigation and wayfinding}},
  booktitle = {Proc. of the 9th IEEE Int. Symposium on Wearable Computers},
  year      = {2005},
  pages     = {34--37},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Willis, Helal - 2005 - RFID information grid for blind navigation and wayfinding.pdf:pdf},
  keywords  = {RFID information grid; blind navigation; centraliz},
}

@Article{yu2006novel,
  author   = {Yu, Wai and Kuber, Ravi and Murphy, Emma and Strain, Philip and McAllister, Graham},
  title    = {{A novel multimodal interface for improving visually impaired people's web accessibility}},
  journal  = {Virtual Reality},
  year     = {2006},
  volume   = {9},
  number   = {2-3},
  pages    = {133--148},
  issn     = {13594338},
  abstract = {This paper introduces a novel interface designed to help blind and visually impaired people to explore and navigate on the Web. In contrast to tradi- tionally used assistive tools, such as screen readers and magnifiers, the new interface employs a combination of both audio and haptic features to provide spatial and navigational information to users. The haptic features are presented via a low-cost force feedback mouse allowing blind people to interact with the Web, in a similar fashion to their sighted counterparts. The audio provides navigational and textual information through the use of non-speech sounds and synthesised speech. Interacting with the multimodal interface offers a novel experience to target users, especially to those with total blindness. A series of experiments have been conducted to ascertain the usability of the interface and compare its performance to that of a traditional screen reader. Results have shown the advantages that the new multi- modal interface offers blind and visually impaired peo- ple. This includes the enhanced perception of the spatial layout of Web pages, and navigation towards elements on a page. Certain issues regarding the design of the haptic and audio features raised in the evaluation are discussed and presented in terms of recommendations for future work.},
  doi      = {10.1007/s10055-005-0009-z},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu et al. - 2006 - A novel multimodal interface for improving visually impaired people's web accessibility.pdf:pdf},
  isbn     = {1359-4338},
  keywords = {Assistive technology,Audio,Haptics,Multimodal interface,Web accessibility,Web navigation},
}

@Article{cristina2016model,
  author    = {Cristina, Stefania and Camilleri, Kenneth P.},
  title     = {{Model-based head pose-free gaze estimation for assistive communication}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2016},
  volume    = {149},
  pages     = {157--170},
  issn      = {10773142},
  doi       = {10.1016/j.cviu.2016.02.012},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cristina, Camilleri - 2016 - Model-based head pose-free gaze estimation for assistive communication.pdf:pdf},
  keywords  = {Eye-gaze tracking,Head pose-free,Videooculography},
  publisher = {Elsevier Inc.},
  url       = {http://linkinghub.elsevier.com/retrieve/pii/S1077314216000667},
}

@TechReport{slade2012update,
  author      = {Slade, J},
  title       = {{Update on inclusive society}},
  institution = {Royal National Institute of Blind People (RNIB)},
  year        = {2012},
  month       = {jul},
  annote      = {- 53{\%} need help to get out of house (older gen)},
  file        = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Slade - 2012 - Update on inclusive society.pdf:pdf},
  keywords    = {blind; mobile phone,smart phone; visually impaired; elderly},
}

@Article{fischler1981random,
  author   = {Fischler, Martin a and Bolles, Robert C},
  title    = {{Random Sample Consensus: A Paradigm for Model Fitting with}},
  journal  = {Communications of the ACM},
  year     = {1981},
  volume   = {24},
  pages    = {381--395},
  issn     = {00010782},
  abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
  doi      = {10.1145/358669.358692},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Fischler, Bolles - 1981 - Random Sample Consensus A Paradigm for Model Fitting with.pdf:pdf},
  isbn     = {0934613338},
  keywords = {0,1,2,3,5,60,61,71,8,analysis,automated cartography,camera calibration,cr categories,determination,image matching,location,model fitting,phrases,scene},
}

@Article{le2015pedestrian,
  author    = {Le, Manh Cuong and Phung, Son Lam and Bouzerdoum, Abdesselam},
  title     = {{Pedestrian lane detection in unstructured environments for assistive navigation}},
  journal   = {2014 International Conference on Digital Image Computing: Techniques and Applications, DICTA 2014},
  year      = {2015},
  volume    = {149},
  pages     = {186--196},
  issn      = {10773142},
  doi       = {10.1109/DICTA.2014.7008122},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Phung, Bouzerdoum - 2015 - Pedestrian lane detection in unstructured environments for assistive navigation.pdf:pdf},
  isbn      = {9781479954094},
  keywords  = {Pedestrian lane detection,Vanishing point estimati,assistive and autonomous navigation,pedestrian lane detection,vanishing point estimation},
  publisher = {Elsevier Inc.},
}

@Article{lu2000fast,
  author   = {Lu, Chien Ping and Hager, Gregory D. and Mjolsness, Eric},
  title    = {{Fast and globally convergent pose estimation from video images}},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {2000},
  volume   = {22},
  number   = {6},
  pages    = {610--622},
  issn     = {01628828},
  abstract = {Determining the rigid transformation relating 2D images to known
3D geometry is a classical problem in photogrammetry and computer
vision. Heretofore, the best methods for solving the problem have relied
on iterative optimization methods which cannot be proven to converge
and/or which do not effectively account for the orthonormal structure of
rotation matrices. We show that the pose estimation problem can be
formulated as that of minimizing an error metric based on collinearity
in object (as opposed to image) space. Using object space collinearity
error, we derive an iterative algorithm which directly computes
orthogonal rotation matrices and which is globally convergent.
Experimentally, we show that the method is computationally efficient,
that it is no less accurate than the best currently employed
optimization methods, and that it outperforms all tested methods in
robustness to outliers},
  doi      = {10.1109/34.862199},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu, Hager, Mjolsness - 2000 - Fast and globally convergent pose estimation from video images.pdf:pdf},
  isbn     = {0162-8828 VO - 22},
}

@Article{flores2015vibrotactile,
  author   = {Flores, G and Kurniawan, S and Manduchi, R and Martinson, E and Morales, L and Sisbot, E},
  title    = {{Vibrotactile Guidance for Wayfinding of Blind Walkers}},
  journal  = {IEEE Transactions on Haptics},
  year     = {2015},
  issn     = {1939-1412},
  doi      = {10.1109/TOH.2015.2409980},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Flores et al. - 2015 - Vibrotactile Guidance for Wayfinding of Blind Walkers.pdf:pdf},
  keywords = {Acoustics;Belts;DC motors;Global Positioning Syste},
}

@Article{davison2002simultaneous,
  author   = {a.J. Davison and Murray, D W},
  title    = {{Simultaneous Localization and Map-Building Using Active Vision}},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {2002},
  volume   = {24},
  number   = {7},
  pages    = {865--880},
  issn     = {0162-8828},
  abstract = {An active approach to sensing can provide the focused measurement capability over a wide field of view which allows correctly formulated simultaneous localization and map-building (SLAM) to be implemented with vision, permitting repeatable longterm localization using only naturally occurring, automatically-detected features. In this paper, we present the first example of a general system for autonomous localization using active vision, enabled here by a high-performance stereo head, addressing such issues as uncertainty-based measurement selection, automatic map-maintenance, and goal-directed steering. We present varied real-time experiments in a complex environment},
  doi      = {10.1109/TPAMI.2002.1017615},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davison, Murray - 2002 - Simultaneous localization and map-building using active vision.pdf:pdf},
  isbn     = {0162-8828},
}

@Article{sanchez2016localization,
  author  = {S{\'{a}}nchez, Carlos and Taddei, Pierluigi and Ceriani, Simone and Wolfart, Erik and Sequeira, V{\'{i}}tor},
  title   = {{Localization and tracking in known large environments using portable real-time 3D sensors}},
  journal = {Computer Vision and Image Understanding},
  year    = {2016},
  volume  = {149},
  pages   = {197--208},
  issn    = {10773142},
  doi     = {10.1016/j.cviu.2015.11.012},
  file    = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\'{a}}nchez et al. - 2016 - Localization and tracking in known large environments using portable real-time 3D sensors.pdf:pdf},
  url     = {http://linkinghub.elsevier.com/retrieve/pii/S1077314215002568},
}

@Article{alcantarilla2012combining,
  author   = {Alcantarilla, Pablo F. and Yebes, Jos{\'{e}} J. and Almaz{\'{a}}n, Javier and Bergasa, Luis M.},
  title    = {{On combining visual SLAM and dense scene flow to increase the robustness of localization and mapping in dynamic environments}},
  journal  = {Proceedings - IEEE International Conference on Robotics and Automation},
  year     = {2012},
  pages    = {1290--1297},
  issn     = {10504729},
  abstract = {In this paper, we introduce the concept of dense scene flow for visual SLAM applications. Traditional visual SLAM methods assume static features in the environment and that a dominant part of the scene changes only due to camera egomotion. These assumptions make traditional visual SLAM methods prone to failure in crowded real-world dynamic environments with many independently moving objects, such as the typical environments for the visually impaired. By means of a dense scene flow representation, moving objects can be detected. In this way, the visual SLAM process can be improved considerably, by not adding erroneous measurements into the estimation, yielding more consistent and improved localization and mapping results. We show large-scale visual SLAM results in challenging indoor and outdoor crowded environments with real visually impaired users. In particular, we performed experiments inside the Atocha railway station and in the city-center of Alcalá de Henares, both in Madrid, Spain. Our results show that the combination of visual SLAM and dense scene flow allows to obtain an accurate localization, improving considerably the results of traditional visual SLAM methods and GPS-based approaches. View full abstract},
  annote   = {Detect movement to remove them from the scene classification -{\textgreater} make SLAM more robust},
  doi      = {10.1109/ICRA.2012.6224690},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alcantarilla et al. - 2012 - On combining visual SLAM and dense scene flow to increase the robustness of localization and mapping in dyn.pdf:pdf},
  isbn     = {9781467314039},
  pmid     = {6224690},
}

@Article{bourbakis2013system,
  author   = {Bourbakis, Nikolaos and Makrogiannis, Sokratis K. and Dakopoulos, Dimitrios},
  title    = {{A system-prototype representing 3D space via alternative-sensing for visually impaired navigation}},
  journal  = {IEEE Sensors Journal},
  year     = {2013},
  volume   = {13},
  number   = {7},
  pages    = {2535--2547},
  issn     = {1530437X},
  abstract = {Offering an alternative mode of interaction with the surrounding 3-D space to the visually impaired for collision free navigation is a goal of great significance that includes several key challenges. In this paper, we study the alternative 3-D space sensation that is interpreted by our computer vision prototype system and transferred to the user via a vibration array. There are two main tasks for conducting such a study. The first task is to detect obstacles in close proximity, and motion patterns in image sequences, both important issues for a safe navigation in a 3-D dynamic space. To achieve this task, the images from the left and right cameras are acquired to produce new stereo images, followed by video stabilization as a preprocessing stage, a nonlinear spatio-temporal diffusion and kernel based density estimation method to assess the motion activity, and finally watershed-based detection of moving regions (or obstacles) of interest. The second task is to efficiently represent the information of the captured static and dynamic visual scenes as 3-D detectable patterns of vibrations applied on the human body to create a 3-D sensation of the space during navigation. To accomplish this task, considering the current limitations imposed by the technology, we create a high-to-low (H-L) image resolution representation scheme to facilitate the mapping onto a low-resolution 2-D array of vibrators. The H-L scheme uses pyramidal modeling to obtain low-resolution images of interest-preserving motion and obstacles-that are mapped onto a vibration array. These patterns are utilized to train and test the performance of the users in free space navigation. Thus, in this paper we study the synergy of these two important schemes to offer an alternative sensation of the 3-D space to the visually impaired via an array of vibrators. Particularly, the motion component is employed as an element for the identification of visual information of interest to be retained during the H-L tran- formation. The role of the array vibrators is to create a small-scale front representation of the space via various levels of vibrations. Thus, 3-D vibrations applied on the user's body (chest, abdomen) offer a 3-D sensation of the surrounding space and the motion in it. In addition, we present experimental results that indicate the efficiency of this navigation scheme in creating low-resolution 3-D views of the free navigation space and detecting obstacles and moving areas.},
  annote   = {Feedback via worn vibrator module},
  doi      = {10.1109/JSEN.2013.2253092},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bourbakis, Makrogiannis, Dakopoulos - 2013 - A system-prototype representing 3D space via alternative-sensing for visually impaired navi.pdf:pdf},
  keywords = {2-D vibration arrays,alternative sensing,assistive technologies,blind's navigation,high-to-low image representation,human computer interface,motion detection},
}

@Article{meier2011pixhawk,
  author = {Meier, Lorenz and Tanskanen, Petri and Fraundorfer, Friedrich and Pollefeys, Marc},
  title  = {{PIXHAWK : A System for Autonomous Flight using Onboard Computer Vision}},
  year   = {2011},
  pages  = {2992--2997},
  annote = {- On-board image processing (PIXHAWK){\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Uses powerful pxCOMex board for processing},
  file   = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Meier et al. - 2011 - PIXHAWK A System for Autonomous Flight using Onboard Computer Vision.pdf:pdf},
  isbn   = {9781612843858},
}

@Article{mcgookin2008investigating,
  author   = {Mcgookin, David and Brewster, Stephen and Jiang, Weiwei},
  title    = {{Investigating Touchscreen Accessibility for People with Visual Impairments}},
  journal  = {Assistive Technology},
  year     = {2008},
  pages    = {298--307},
  abstract = {Touchscreen computing devices such as the iPhone are becoming more common. However this technology is largely inaccessible to people with visual impairments. We present the results of a requirements capture study that illustrates the problems with touchscreen accessibility, and the choices visually impaired people make when choosing assistive technology. We investigate ways of overcoming touchscreen accessibility problems by comparing a raised paper overlay touchscreen based MP3 player, with a touchscreen gesture based player. Twelve blindfolded participants, and one visually impaired person, were able to operate both players, though there were problems with short impact related operations in the gesture player. From our results we provide guidelines for future designers, to help them exploit the potential of touchscreen technology for visually impaired people.},
  annote   = {Not helpful},
  doi      = {10.1145/1463160.1463193},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcgookin, Brewster, Jiang - 2008 - Investigating Touchscreen Accessibility for People with Visual Impairments.pdf:pdf},
  isbn     = {9781595937049},
  keywords = {accessibility,blind,mobile devices,non-speech sound,speech,touchscreen,universal design,visual impairment},
}

@InProceedings{brabyn1993talking,
  author    = {Brabyn, J and Crandall, W and Gerrey, W},
  title     = {{Talking signs: a remote signage, solution for the blind, visually impaired and reading disabled}},
  booktitle = {Proc. of Annual Int. Conf. of the IEEE Engineering in Medicine and Biology Society},
  year      = {1993},
  pages     = {1309--1310},
  doi       = {10.1109/IEMBS.1993.979150},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brabyn, Crandall, Gerrey - 1993 - Talking signs a remote signage, solution for the blind, visually impaired and reading disabled.pdf:pdf},
}

@Article{maguire1999review,
  author   = {Maguire, M C},
  title    = {{A review of user-interface design guidelines for public information kiosk systems}},
  journal  = {International Journal of Human Computer Studies},
  year     = {1999},
  volume   = {50},
  number   = {3},
  pages    = {263--280},
  issn     = {1071-5819},
  abstract = {This paper reviews general guidelines on user interface design for self service and public information kiosk systems, based on the author's research and existing literature. The guidelines are divided into: defining user requirements, location and encouraging use, physical access, introduction and instruction, language selection, privacy, help, input, output, structure and navigation, and customisation. The paper also emphasises the need to design for stakeholders other than the end users, and offers some guidelines on user-based evaluation of kiosk systems.},
  annote   = {Rather unhelpful},
  doi      = {10.1006/ijhc.1998.0243},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maguire - 1999 - A review of user-interface design guidelines for public information kiosk systems.pdf:pdf},
  url      = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+Review+of+User-Interface+Design+Guidelines+for+Public+Information+Kiosk+Systems{\#}0},
}

@Article{ran2004drishtia,
  author   = {Ran, Lisa and Helal, Sumi and Moore, Steve},
  title    = {{Drishti: An integrated indoor/outdoor blind navigation system and service}},
  journal  = {Proceedings - Second IEEE Annual Conference on Pervasive Computing and Communications, PerCom},
  year     = {2004},
  pages    = {23--30},
  issn     = {1530-0811},
  abstract = { There are many navigation systems for visually impaired people but few can provide dynamic interactions and adaptability to changes. None of these systems work seamlessly both indoors and outdoors. Drishti uses a precise position measurement system, a wireless connection, a wearable computer, and a vocal communication interface to guide blind users and help them travel in familiar and unfamiliar environments independently and safely. Outdoors, it uses DGPS as its location system to keep the user as close as possible to the central line of sidewalks of campus and downtown areas; it provides the user with an optimal route by means of its dynamic routing and rerouting ability. The user can switch the system from an outdoor to an indoor environment with a simple vocal command. An OEM ultrasound positioning system is used to provide precise indoor location measurements. Experiments show an in-door accuracy of 22 cm. The user can get vocal prompts to avoid possible obstacles and step-by-step walking guidance to move about in an indoor environment. This paper describes the Drishti system and focuses on the indoor navigation design and lessons learned in integrating the indoor with the outdoor system.},
  annote   = {Drishti system Indoor ultrasound Only usable in special 'smart' house},
  doi      = {10.1109/PERCOM.2004.1276842},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ran, Helal, Moore - 2004 - Drishti An integrated indooroutdoor blind navigation system and service.pdf:pdf},
  isbn     = {0769520901},
}

@Article{coates2008learning,
  author    = {Coates, Adam and Abbeel, Pieter and Ng, Andrew Y.},
  title     = {{Learning for control from multiple demonstrations}},
  journal   = {Proceedings of the 25th international conference on Machine learning - ICML '08},
  year      = {2008},
  pages     = {144--151},
  address   = {New York, New York, USA},
  doi       = {10.1145/1390156.1390175},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Coates, Abbeel, Ng - 2008 - Learning for control from multiple demonstrations.pdf:pdf},
  isbn      = {9781605582054},
  publisher = {ACM Press},
  url       = {http://portal.acm.org/citation.cfm?doid=1390156.1390175},
}

@Article{damen2016you,
  author    = {Damen, Dima and Leelasawassuk, Teesid and Mayol-Cuevas, Walterio},
  title     = {{You-Do, I-Learn: Egocentric unsupervised discovery of objects and their modes of interaction towards video-based guidance}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2016},
  volume    = {149},
  pages     = {98--112},
  issn      = {10773142},
  annote    = {Automatic object detection and usage suggestion},
  doi       = {10.1016/j.cviu.2016.02.016},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Damen, Leelasawassuk, Mayol-Cuevas - 2016 - You-Do, I-Learn Egocentric unsupervised discovery of objects and their modes of interaction.pdf:pdf},
  keywords  = {Assistive,Real-time computer vision,Video guidance},
  publisher = {Elsevier Inc.},
  url       = {http://linkinghub.elsevier.com/retrieve/pii/S1077314216000709},
}

@Article{hesch2010design,
  author   = {Hesch, J A and Roumeliotis, S I},
  title    = {{Design and Analysis of a Portable Indoor Localization Aid for the Visually Impaired}},
  journal  = {The International Journal of Robotics Research},
  year     = {2010},
  volume   = {29},
  number   = {11},
  pages    = {1400--1415},
  issn     = {0278-3649},
  abstract = {In this paper, we present the design and analysis of a portable position and orientation (pose) estimation aid for the visually impaired. Our prototype navigation aid consists of a foot-mounted pedometer and a white cane-mounted sensing package, which comprises a three-axis gyroscope and a two-dimensional (2D) laser scanner. We introduce a two-layered estimator that tracks the 3D orientation of the white cane in the first layer, and the 2D position of the person holding the cane in the second layer. Our algorithm employs a known building map to update the person$\backslash${\^{}}a s position, and exploits perpendicularity in the building layout as a 3D structural compass. We analytically study the observability properties of the linearized dynamical system, and we provide sufficient observability conditions. We evaluate the real-world performance of our localization aid, and demonstrate its reliability for accurate, real-time human localization.},
  doi      = {10.1177/0278364910373160},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hesch, Roumeliotis - 2010 - Design and Analysis of a Portable Indoor Localization Aid for the Visually Impaired.pdf:pdf},
  isbn     = {0278-3649},
  keywords = {doutorado},
  url      = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364910373160{\%}5Cnpapers3://publication/doi/10.1177/0278364910373160},
}

@InCollection{giudice2008blind,
  author    = {Giudice, N A and Legge, G E},
  title     = {{Blind Navigation and the Role of Technology, in The Engineering Handbook of Smart Technology for Aging, Disability, and Independence}},
  publisher = {John Wiley {\&} Sons},
  year      = {2008},
  editor    = {Helal, A and Mokhtari, M and Abdulrazak, B},
  chapter   = {25},
  pages     = {479--500},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Giudice, Legge - 2008 - Blind Navigation and the Role of Technology, in The Engineering Handbook of Smart Technology for Aging, Disabili.pdf:pdf},
}

@TechReport{suttie2011ageing,
  author      = {Suttie, A and Daly, A and Good, G and Mambetakunov, K and Orr, A and Scott, J and Techavachara, P and Verstraten, P},
  title       = {{Ageing and Visual Impairment}},
  institution = {World Blind Union},
  year        = {2011},
  month       = {may},
  annote      = {Good motivation for assistive tech},
  file        = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suttie et al. - 2011 - Ageing and Visual Impairment.pdf:pdf},
}

@Article{kane2011usable,
  author   = {Kane, Sk and Wobbrock, Jo and Ladner, Re},
  title    = {{Usable gestures for blind people: understanding preference and performance}},
  journal  = {Proceedings of the 2011 annual conference on Human factors in computing systems - CHI '11},
  year     = {2011},
  pages    = {413--422},
  abstract = {Despite growing awareness of the accessibility issues surrounding touch screen use by blind people, designers still face challenges when creating accessible touch screen interfaces. One major stumbling block is a lack of understanding about how blind people actually use touch screens. We conducted two user studies that compared how blind people and sighted people use touch screen gestures. First, we conducted a gesture elicitation study in which 10 blind and 10 sighted people invented gestures to perform common computing tasks on a tablet PC. We found that blind people have different gesture preferences than sighted people, including preferences for edge-based gestures and gestures that involve tapping virtual keys on a keyboard. Second, we conducted a performance study in which the same participants performed a set of reference gestures. We found significant differences in the speed, size, and shape of gestures performed by blind people versus those performed by sighted people. Our results suggest new design guidelines for accessible touch screen interfaces.},
  doi      = {10.1145/1978942.1979001},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kane, Wobbrock, Ladner - 2011 - Usable gestures for blind people understanding preference and performance.pdf:pdf},
  isbn     = {9781450302678},
  keywords = {Accessibility,blind,gesture recognition.,gestures,touch screens},
  url      = {http://dl.acm.org/citation.cfm?id=1979001},
}

@Article{schweighofer2006robust,
  author = {Schweighofer, Gerald and Pinz, Axel},
  title  = {{Robust Pose Estimation from a Planar Target}},
  year   = {2006},
  volume = {28},
  number = {12},
  pages  = {2024--2030},
  file   = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schweighofer, Pinz - 2006 - Robust Pose Estimation from a Planar Target.pdf:pdf},
}

@Article{villamizar2016interactive,
  author    = {Villamizar, Michael and Garrell, Ana{\'{i}}s and Sanfeliu, Alberto and Moreno-Noguer, Francesc},
  title     = {{Interactive multiple object learning with scanty human supervision}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2016},
  volume    = {149},
  pages     = {51--64},
  issn      = {10773142},
  doi       = {10.1016/j.cviu.2016.03.010},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Villamizar et al. - 2016 - Interactive multiple object learning with scanty human supervision.pdf:pdf},
  keywords  = {Interactive learning,Object recognition,Online cla},
  publisher = {Elsevier Inc.},
  url       = {http://linkinghub.elsevier.com/retrieve/pii/S1077314216300042},
}

@Article{porzi2012visual,
  author   = {Porzi, Lorenzo and Ricci, Elisa and Ciarfuglia, Thomas A. and Zanin, Michele},
  title    = {{Visual-inertial tracking on Android for Augmented Reality applications}},
  journal  = {2012 IEEE Workshop on Environmental, Energy, and Structural Monitoring Systems, EESMS 2012 - Proceedings},
  year     = {2012},
  pages    = {35--41},
  abstract = {Augmented Reality (AR) aims to enhance a person's vision of the real world with useful information about the surrounding environment. Amongst all the possible applications, AR systems can be very useful as visualization tools for structural and environmental monitoring. While the large majority of AR systems run on a laptop or on a head-mounted device, the advent of smartphones have created new opportunities. One of the most important functionality of an AR system is the ability of the device to self localize. This can be achieved through visual odometry, a very challenging task for smartphone. Indeed, on most of the available smartphone AR applications, self localization is achieved through GPS and/or inertial sensors. Hence, developing an AR system on a mobile phone also poses new challenges due to the limited amount of computational resources. In this paper we describe the development of a egomotion estimation algorithm for an Android smartphone. We also present an approach based on an Extended Kalman Filter for improving localization accuracy integrating the information from inertial sensors. The implemented solution achieves a localization accuracy comparable to the PC implementation while running on an Android device.},
  doi      = {10.1109/EESMS.2012.6348402},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Porzi et al. - 2012 - Visual-inertial tracking on Android for Augmented Reality applications.pdf:pdf},
  isbn     = {9781467327374},
}

@InProceedings{saez2008stereo,
  author    = {S{\'{a}}ez, J M and Escolano, F},
  title     = {{Stereo-based Aerial Obstacle Detection for the Visually Impaired}},
  booktitle = {Workshop on Computer Vision Applications for the Visually Impaired},
  year      = {2008},
  abstract  = {In this paper, we present a novel approach for aerial obstacle detection using a stereo vision wearable device in the context of the visually impaired assistance. This kind of obstacles are specially dangerous because they could not be detected by the walking stick. The algorithm maintains a local 3D map of the vicinity of the user, which is estimated through a 6DOF egomotion algorithm. The trajectory is used to predict the next movement of the blind and the 3D information of the local map is used to evaluate a possible aerial obstacles in the next pose. A key stabilization algorithm is introduced in order to maintain the floor of the map continuously aligned with the horizontal plane. This is a very important task, because in a wearable 3D device, the relative transformation of the camera reference system with respect to the user and the environment is continuously changed. In the experimental section, we show the results of the algorithm in several situations using real data.},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\'{a}}ez, Escolano - 2008 - Stereo-based Aerial Obstacle Detection for the Visually Impaired.pdf:pdf},
}

@Article{ramon2012accurate,
  author  = {Ram{\'{o}}n, Antonio and Ruiz, Jim{\'{e}}nez and Granja, Fernando Seco and Carlos, Jos{\'{e}} and Honorato, Prieto and Rosas, Jorge I Guevara and {Jimenez Ruiz}, Antonio Ram{\'{o}}n and {Seco Granja}, Fernando and {Prieto Honorato}, Jos{\'{e}} Carlos and {Guevara Rosas}, Jorge I.},
  title   = {{Accurate Pedestrian Indoor Navigation by Tightly Coupling Foot-Mounted IMU and RFID Measurements}},
  journal = {IEEE Transactions on Instrumentation and Measurement},
  year    = {2012},
  volume  = {61},
  number  = {1},
  pages   = {178--189},
  issn    = {0018-9456},
  doi     = {10.1109/TIM.2011.2159317},
  file    = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ram{\'{o}}n et al. - 2012 - Accurate Pedestrian Indoor Navigation by Tightly Coupling Foot-Mounted IMU and RFID Measurements.pdf:pdf},
  url     = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5953513},
}

@Article{lowry2015visual,
  author   = {Lowry, Stephanie and Sunderhauf, Niko and Newman, Paul and Leonard, John J. and Cox, David and Corke, Peter and Milford, Michael J.},
  title    = {{Visual Place Recognition: A Survey}},
  journal  = {IEEE Transactions on Robotics},
  year     = {2015},
  volume   = {32},
  number   = {1},
  pages    = {1--19},
  issn     = {15523098},
  abstract = {Visual place recognition is a challenging problem due to the vast range of ways in which the appearance of real-world places can vary. In recent years, improvements in visual sensing capabilities, an ever-increasing focus on long-term mobile robot autonomy, and the ability to draw on state-of-the-art research in other disciplines—particularly recognition in computer vision and animal navigation in neuroscience—have all contributed to significant advances in visual place recognition systems. This paper presents a survey of the visual place recognition research landscape. We start by introducing the concepts behind place recognition—the role of place recognition in the animal kingdom, how a “place” is defined in a robotics context, and the major components of a place recognition system. Long-term robot operations have revealed that changing appearance can be a significant factor in visual place recognition failure; therefore, we discuss how place recognition solutions can implicitly or explicitly account for appearance change within the environment. Finally, we close with a discussion on the future of visual place recognition, in particular with respect to the rapid advances being made in the related fields of deep learning, semantic scene understanding, and video description.},
  doi      = {10.1109/TRO.2015.2496823},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowry et al. - 2015 - Visual Place Recognition A Survey.pdf:pdf},
}

@InProceedings{mellinger2011minimum,
  author    = {Mellinger, Daniel and Kumar, Vijay},
  title     = {{Minimum snap trajectory generation and control for quadrotors}},
  booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
  year      = {2011},
  pages     = {2520--2525},
  abstract  = {We address the controller design and the trajectory generation for a quadrotor maneuvering in three dimensions in a tightly constrained setting typical of indoor environments. In such settings, it is necessary to allow for significant excursions of the attitude from the hover state and small angle approximations cannot be justified for the roll and pitch. We develop an algorithm that enables the real-time generation of optimal trajectories through a sequence of 3-D positions and yaw angles, while ensuring safe passage through specified corridors and satisfying constraints on velocities, accelerations and inputs. A nonlinear controller ensures the faithful tracking of these trajectories. Experimental results illustrate the application of the method to fast motion (5{\&}{\#}x2013;10 body lengths/second) in three-dimensional slalom courses.},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mellinger, Kumar - 2011 - Minimum snap trajectory generation and control for quadrotors.pdf:pdf},
}

@Article{skala2012radial,
  author   = {Skala, Vaclav},
  title    = {{Radial Basis Functions for High-Dimensional Visualization}},
  journal  = {VisGra-ICONS},
  year     = {2012},
  number   = {c},
  pages    = {193--198},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Skala - 2012 - Radial Basis Functions for High-Dimensional Visualization.pdf:pdf},
  isbn     = {9781612081847},
  keywords = {- visualization,a distance computing of,computer graphics,interpolation,it is based on,point of view,radial basis functions,rbf,rbf interpolation is quite,simple from a mathematical,two},
}

@Article{cortes-rico2015human,
  author   = {Cort{\'{e}}s-Rico, Laura and Piedrahita-Sol{\'{o}}rzano, Giovanny},
  title    = {{Human-Computer Interaction – INTERACT 2015}},
  journal  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year     = {2015},
  volume   = {9298},
  pages    = {518--525},
  issn     = {16113349},
  abstract = {This paper presents a project for the social development of ICTs, which used a participatory design approach and sought to have a high social impact on a community of craftswomen (embroiderers from Cartago, Colombia). Participating in this project implied active dialogue with the community to recognize the knowledge of each participant and achieve culturally relevant representations materialized in technological artifacts. We posit dialogue, representation and recognition as key elements for developing successful participatory design. In practice, this was achieved through an iterative, incremental and open-ended methodology, whose main feature was engagement by doing. This process of design allowed engineers to recognize the craftswomen's traditional knowledge and allowed craftswomen to be less afraid of technology. The main resultant artifact was a tangible user interface that facilitates dialogue between fashion designers and embroiderers in the process of designing new embroidery patterns. This and other artifacts that emerged from the activities and dialogues, the level of engagement of the participants, and the convergence points discovered between embroidery and technology, lead us to conclude that the process presented here can be replicated with other craft communities, to reinforce these communities and assist them in generating innovation in their processes and products.},
  doi      = {10.1007/978-3-319-22698-9},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cort{\'{e}}s-Rico, Piedrahita-Sol{\'{o}}rzano - 2015 - Human-Computer Interaction – INTERACT 2015.pdf:pdf},
  isbn     = {978-3-319-22697-2},
  keywords = {Crafts,Dialogue,Embroidery,Participatory design,Recognition,Representation,Tangible user interfaces and social technologies},
  url      = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84946040011{\&}partnerID=tZOtx3y1},
}

@InProceedings{manduchi2010blind,
  author    = {Manduchi, R and Kurniawan, S and Bagherinia, H},
  title     = {{Blind guidance using mobile computer vision: a usability study}},
  booktitle = {Proc. of the 12th Int. ACM SIGACCESS Conf. on Computers and Accessibility},
  year      = {2010},
  pages     = {241--242},
  address   = {New York, NY, USA},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manduchi, Kurniawan, Bagherinia - 2010 - Blind guidance using mobile computer vision a usability study.pdf:pdf},
  isbn      = {978-1-60558-881-0},
  keywords  = {blind,guidance,mobility,orientation,recognition,visually impaired,wayfinding},
}

@Article{apostolopoulos2012integrated,
  author        = {Apostolopoulos, Ilias and Fallah, Navid and Folmer, Eelke and Bekris, Kostas E.},
  title         = {{Integrated online localization and navigation for people with visual impairments using smart phones}},
  journal       = {Proceedings - IEEE International Conference on Robotics and Automation},
  year          = {2012},
  volume        = {3},
  number        = {4},
  pages         = {1322--1329},
  issn          = {10504729},
  abstract      = {Indoor localization and navigation systems for individuals with visual impairments (VI) typically rely upon extensive augmentation of the physical space or heavy, expensive sensors; thus, few systems have been adopted. This work describes a system able to guide people with VI through buildings using inexpensive sensors, such as accelerometers, which are available in portable devices like smart phones. The method takes advantage of feedback from the human user, who confirms the presence of landmarks. The system calculates the user's location in real time and uses it to provide audio instructions on how to reach the desired destination. Previous work suggested that the accuracy of the approach depended on the type of directions and the availability of an appropriate transition model for the user. A critical parameter for the transition model is the user's step length. The current work investigates different schemes for automatically computing the user's step length and reducing the dependency of the approach to the definition of an accurate transition model. Furthermore, the direction provision method is able to use the localization estimate and adapt to failed executions of paths by the users. Experiments are presented that evaluate the accuracy of the overall integrated system, which is executed online on a smart phone. Both people with visual impairments, as well as blindfolded sighted people, participated in the experiments. The experiments included paths along multiple floors, that required the use of stairs and elevators. $\backslash${\&}copy; 2012 IEEE.},
  archiveprefix = {arXiv},
  arxivid       = {1005.3014},
  doi           = {10.1109/ICRA.2012.6225093},
  eprint        = {1005.3014},
  file          = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Apostolopoulos et al. - 2012 - Integrated online localization and navigation for people with visual impairments using smart phones.pdf:pdf},
  isbn          = {9781467314039},
}

@Article{farinella2016special,
  author    = {Farinella, Giovanni Maria and Kanade, Takeo and Leo, Marco and Medioni, Gerard G. and Trivedi, Mohan},
  title     = {{Special Issue on Assistive Computer Vision and Robotics - ``Assistive Solutions for Mobility, Communication and HMI''}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2016},
  volume    = {149},
  pages     = {1--2},
  issn      = {10773142},
  doi       = {10.1016/j.cviu.2016.05.014},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farinella et al. - 2016 - Special Issue on Assistive Computer Vision and Robotics - ``Assistive Solutions for Mobility, Communication an.pdf:pdf},
  publisher = {Elsevier Inc.},
  url       = {http://linkinghub.elsevier.com/retrieve/pii/S1077314216300674},
}

@Article{zhang2009autonomous,
  author = {Zhang, Tianguang and Kang, Ye and Achtelik, Markus and Kiihnlenz, Kolja and Buss, Martin},
  title  = {{Autonomous Hovering of a Vision/IMU Guided Quadrotor}},
  year   = {2009},
  pages  = {2870--2875},
  annote = {- Fuse camera data wth IMU for pose and localisation{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Uses ground stattion for control calculations{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Ground markers are used by camera to estimate pose and height above ground{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Results look prerry good},
  file   = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2009 - Autonomous Hovering of a VisionIMU Guided Quadrotor.pdf:pdf},
  isbn   = {9781424426935},
}

@Article{wilson2007swan,
  author   = {Wilson, Jeff and Walker, Bruce N. and Lindsay, Jeffrey and Cambias, Craig and Dellaert, Frank},
  title    = {{SWAN: System for wearable audio navigation}},
  journal  = {Int. Symposium on Wearable Computers},
  year     = {2007},
  pages    = {91--98},
  issn     = {15504816},
  abstract = {Wearable computers can certainly support audio-only presentation of information; a visual interface need not be present for effective user interaction. A system for wearable audio navigation (SWAN) is being developed to serve as a navigation and orientation aid for persons temporarily or permanently visually impaired. SWAN is a wearable computer consisting of audio-only output and tactile input via a handheld interface. SWAN aids a user in safe pedestrian navigation and includes the ability for the user to author new GIS data relevant to their needs of wayfinding, obstacle avoidance, and situational awareness support. Emphasis is placed on representing pertinent data with non-speech sounds through a process of sonification. SWAN relies on a geographic information system (GIS) infrastructure for supporting geocoding and spatialization of data. Furthermore, SWAN utilizes novel tracking technology.},
  annote   = {Very similar to ours},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson et al. - 2007 - SWAN System for wearable audio navigation.pdf:pdf},
  isbn     = {9781424414536},
}

@Article{katz2010navig,
  author   = {Katz, Brian F G and Truillet, Philippe and Thorpe, Simon J and Jouffrais, Christophe and Jouffrais},
  title    = {{NAVIG: Navigation Assisted by Artificial Vision and GNSS.}},
  journal  = {Workshop on Multimodal Location Based Techniques for Extreme Navigation},
  year     = {2010},
  number   = {1},
  pages    = {1--4},
  abstract = {Finding ones way to an unknown destination, navigating complex routes, finding desired inanimate objects; these are all tasks that can be challenging for the visually impaired. The project NAVIG (Navigation Assisted by artificial VIsion and GNSS) is directed towards increasing the autonomy of visually impaired users in known and unknown environments, exterior and interior, large scale and small scale, through a combination of a Global Navigation Satellite System (GNSS) and rapid visual recognition with which the precise position of the user can be determined. Relying on geographical databases and visually identified objects, the user is guided to their desired destination through spatialized audio rendering, always maintained in the head-centered reference frame. This paper presents the overall project design and architecture of the NAVIG system.},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Katz et al. - 2010 - NAVIG Navigation Assisted by Artificial Vision and GNSS.pdf:pdf},
  keywords = {Assisted navigation,guidance,spatial audio,visually impaired},
}

@Article{gallina2015progressive,
  author   = {Gallina, Paolo and Bellotto, Nicola and Di Luca, Massimiliano},
  title    = {{Progressive co-adaptation in human-machine interaction}},
  journal  = {Int. Conf. on Informatics in Control, Automation and Robotics},
  year     = {2015},
  volume   = {2},
  pages    = {362--368},
  abstract = {In this paper we discuss the concept of co-adaptation between a human operator and a machine interface and we summarize its application with emphasis on two different domains, teleoperation and assistive technology. The analysis of the literature reveals that only in a few cases the possibility of a temporal evolution of the co-adaptation parameters has been considered. In particular, it has been overlooked the role of time-related indexes that capture changes in motor and cognitive abilities of the human operator. We argue that for a more effective long-term co-adaptation process, the interface should be able to predict and adjust its parameters according to the evolution of human skills and performance. We thus propose a novel approach termed progressive co-adaptation, whereby human performance is continuously monitored and the system makes inferences about changes in the users' cognitive and motor skills. We illustrate the features of progressive co-adaptation in two possible applications, robotic telemanipulation and active vision for the visually impaired.},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gallina, Bellotto, Di Luca - 2015 - Progressive co-adaptation in human-machine interaction.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gallina, Bellotto, Luca - Unknown - Progressive Co-adaptation in Human-Machine Interaction.pdf:pdf},
  isbn     = {9789897581236},
  keywords = {Active vision,Assistive technology,Cyber-physical systems,Human-in-the-loop,Teleoperation,Usability},
}

@Article{pradeep2010robot,
  author   = {Pradeep, Vivek and Medioni, Gerard and Weiland, James},
  title    = {{Robot vision for the visually impaired}},
  journal  = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops},
  year     = {2010},
  pages    = {15--22},
  abstract = {We present a head-mounted, stereo-vision based navigational assistance device for the visually impaired. The head-mounted design enables our subjects to stand and scan the scene for integrating wide-field information, compared to shoulder or waist-mounted designs in literature which require body rotations. In order to extract and maintain orientation information for creating a sense of egocentricity in blind users, we incorporate visual odometry and feature based metric-topological SLAM into our system. Using camera pose estimates with dense 3D data obtained from stereo triangulation, we build a vicinity map of the user's environment. On this map, we perform 3D traversability analysis to steer subjects away from obstacles in the path. A tactile interface consisting of microvibration motors provides cues for taking evasive action, as determined by our vision processing algorithms. We report experimental results of our system (running at 10 Hz) and conduct mobility tests with blindfolded subjects to demonstrate the usefulness of our approach over conventional navigational aids like the white cane.},
  doi      = {10.1109/CVPRW.2010.5543579},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pradeep, Medioni, Weiland - 2010 - Robot vision for the visually impaired.pdf:pdf},
  isbn     = {978-1-4244-7029-7},
  url      = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5543579{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5543579},
}

@Article{k.narayanan2016vision,
  author    = {{K. Narayanan}, Vishnu and Pasteau, Fran{\c{c}}ois and Marchal, Maud and Krupa, Alexandre and Babel, Marie},
  title     = {{Vision-based adaptive assistance and haptic guidance for safe wheelchair corridor following}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2016},
  volume    = {149},
  pages     = {171--185},
  issn      = {10773142},
  doi       = {10.1016/j.cviu.2016.02.008},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/K. Narayanan et al. - 2016 - Vision-based adaptive assistance and haptic guidance for safe wheelchair corridor following.pdf:pdf},
  keywords  = {Assistive ro,Vision-based robotics,Visual servoing,vision-based robotics},
  publisher = {Elsevier Inc.},
  url       = {http://linkinghub.elsevier.com/retrieve/pii/S1077314216000539},
}

@Article{thaler2011neural,
  author    = {Thaler, L and Arnott, S R and Goodale, M A},
  title     = {{Neural Correlates of Natural Human Echolocation in Early and Late Blind Echolocation Experts}},
  journal   = {PLoS ONE},
  year      = {2011},
  volume    = {6},
  number    = {5},
  abstract  = { {\textless}sec{\textgreater} {\textless}title{\textgreater}Background{\textless}/title{\textgreater} {\textless}p{\textgreater}A small number of blind people are adept at echolocating silent objects simply by producing mouth clicks and listening to the returning echoes. Yet the neural architecture underlying this type of aid-free human echolocation has not been investigated. To tackle this question, we recruited echolocation experts, one early- and one late-blind, and measured functional brain activity in each of them while they listened to their own echolocation sounds.{\textless}/p{\textgreater} {\textless}/sec{\textgreater} {\textless}sec{\textgreater} {\textless}title{\textgreater}Results{\textless}/title{\textgreater} {\textless}p{\textgreater}When we compared brain activity for sounds that contained both clicks and the returning echoes with brain activity for control sounds that did not contain the echoes, but were otherwise acoustically matched, we found activity in calcarine cortex in both individuals. Importantly, for the same comparison, we did not observe a difference in activity in auditory cortex. In the early-blind, but not the late-blind participant, we also found that the calcarine activity was greater for echoes reflected from surfaces located in contralateral space. Finally, in both individuals, we found activation in middle temporal and nearby cortical regions when they listened to echoes reflected from moving targets.{\textless}/p{\textgreater} {\textless}/sec{\textgreater} {\textless}sec{\textgreater} {\textless}title{\textgreater}Conclusions{\textless}/title{\textgreater} {\textless}p{\textgreater}These findings suggest that processing of click-echoes recruits brain regions typically devoted to vision rather than audition in both early and late blind echolocation experts.{\textless}/p{\textgreater} {\textless}/sec{\textgreater} },
  annote    = {Blind using echolocation},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thaler, Arnott, Goodale - 2011 - Neural Correlates of Natural Human Echolocation in Early and Late Blind Echolocation Experts.pdf:pdf},
  publisher = {Public Library of Science},
}

@Article{bellotto2012cognitive,
  author    = {Bellotto, Nicola and Benfold, Ben and Harland, Hanno and Nagel, Hans Hellmut and Pirlo, Nicola and Reid, Ian and Sommerlade, Eric and Zhao, Chuan},
  title     = {{Cognitive visual tracking and camera control}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2012},
  volume    = {116},
  number    = {3},
  pages     = {457--471},
  issn      = {10773142},
  abstract  = {Cognitive visual tracking is the process of observing and understanding the behavior of a moving person. This paper presents an efficient solution to extract, in real-time, high-level information from an observed scene, and generate the most appropriate commands for a set of pan-tilt-zoom (PTZ) cameras in a surveillance scenario. Such a high-level feedback control loop, which is the main novelty of our work, will serve to reduce uncertainties in the observed scene and to maximize the amount of information extracted from it. It is implemented with a distributed camera system using SQL tables as virtual communication channels, and Situation Graph Trees for knowledge representation, inference and high-level camera control. A set of experiments in a surveillance scenario show the effectiveness of our approach and its potential for real applications of cognitive vision. ?? 2011 Elsevier Inc. All rights reserved.},
  doi       = {10.1016/j.cviu.2011.09.011},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellotto et al. - 2012 - Cognitive visual tracking and camera control.pdf:pdf},
  isbn      = {1077-3142},
  keywords  = {Active cameras,Cognitive vision,Human tracking},
  publisher = {Elsevier Inc.},
  url       = {http://dx.doi.org/10.1016/j.cviu.2011.09.011},
}

@InProceedings{kao1996object,
  author    = {Kao, G and Probert, P and Lee, D},
  title     = {{Object Recognition with FM Sonar; An Assistive Device for Blind and Visually-Impaired People}},
  booktitle = {Technical Report FS-96-05},
  year      = {1996},
  pages     = {38--45},
  address   = {Menlo Park, California},
  publisher = {AAAI Press},
  annote    = {FM Sonar - Only classifies rough, smooth surfaces},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kao, Probert, Lee - 1996 - Object Recognition with FM Sonar An Assistive Device for Blind and Visually-Impaired People.pdf:pdf},
}

@Article{li2012vision,
  author   = {Li, Mingyang and Mourikis, Anastasios I},
  title    = {{Vision-aided Inertial Navigation for Resource-constrained Systems}},
  year     = {2012},
  pages    = {1057--1063},
  abstract = {— In this paper we present a resource-adaptive framework for real-time vision-aided inertial navigation. Specif-ically, we focus on the problem of visual-inertial odometry (VIO), in which the objective is to track the motion of a mobile platform in an unknown environment. Our primary interest is navigation using miniature devices with limited computational resources, similar for example to a mobile phone. Our proposed estimation framework consists of two main components: (i) a hybrid EKF estimator that integrates two algorithms with complementary computational characteristics, namely a sliding-window EKF and EKF-based SLAM, and (ii) an adaptive image-processing module that adjusts the number of detected image features based on the availability of resources. By combining the hybrid EKF estimator, which optimally utilizes the feature measurements, with the adaptive image-processing algorithm, the proposed estimation architecture fully utilizes the system's computational resources. We present experimental results showing that the proposed estimation framework is capable of real-time processing of image and inertial data on the processor of a mobile phone.},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Mourikis - 2012 - Vision-aided Inertial Navigation for Resource-constrained Systems.pdf:pdf},
  isbn     = {9781467317351},
}

@InProceedings{x-flyer2002copyright,
  author    = {Ostrowski, James},
  title     = {X-flyer Stabilization},
  booktitle = {ProceIFAC 15th Triennial World Congress},
  year      = {2002},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/X-flyer, Ostrowski - 2002 - Copyright {\textcopyright} 2002 IFAC 15th Triennial World Congress, Barcelona, Spain.pdf:pdf},
  keywords  = {control lyapunov function,mobile robots,uav,unmanned aerial vehicle},
}

@Article{manduchi2012computer,
  author        = {Manduchi, Roberto and Coughlan, James},
  title         = {{(Computer) Vision without Sight.}},
  journal       = {Communications of the ACM},
  year          = {2012},
  volume        = {55},
  number        = {1},
  pages         = {96--104},
  issn          = {0001-0782},
  abstract      = {Computer vision holds great promise for helping persons with blindness or visual impairments (VI) to interpret and explore the visual world. To this end, it is worthwhile to assess the situation critically by understanding the actual needs of the VI population and which of these needs might be addressed by computer vision. This article reviews the types of assistive technology application areas that have already been developed for VI, and the possible roles that computer vision can play in facilitating these applications. We discuss how appropriate user interfaces are designed to translate the output of computer vision algorithms into information that the user can quickly and safely act upon, and how system-level characteristics affect the overall usability of an assistive technology. Finally, we conclude by highlighting a few novel and intriguing areas of application of computer vision to assistive technology.},
  annote        = {Survey of existing tools for vision and a few ideas},
  archiveprefix = {arXiv},
  arxivid       = {NIHMS150003},
  doi           = {10.1145/2063176.2063200},
  eprint        = {NIHMS150003},
  file          = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manduchi, Coughlan - 2012 - (Computer) Vision without Sight.pdf:pdf},
  isbn          = {2122633255},
  pmid          = {22815563},
  url           = {http://dl.acm.org/ft{\_}gateway.cfm?id=2063200{\&}type=html},
}

@Article{garciacarrillo2010stabilization,
  author  = {{Garc{\'{i}}a Carrillo}, L. R. and Rondon, E. and Sanchez, A. and Dzul, A. and Lozano, R.},
  title   = {{Stabilization and Trajectory Tracking of a Quad-Rotor Using Vision}},
  journal = {Journal of Intelligent {\&} Robotic Systems},
  year    = {2010},
  volume  = {61},
  number  = {1-4},
  pages   = {103--118},
  month   = {oct},
  issn    = {0921-0296},
  annote  = {- Use on-board camera for pose estimation - Uses PID},
  doi     = {10.1007/s10846-010-9472-1},
  file    = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Garc{\'{i}}a Carrillo et al. - 2010 - Stabilization and Trajectory Tracking of a Quad-Rotor Using Vision.pdf:pdf},
  url     = {http://link.springer.com/10.1007/s10846-010-9472-1},
}

@Article{hehn2011quadrocopter,
  author   = {Hehn, Markus and Andrea, Raffaello D},
  title    = {{Quadrocopter Trajectory Generation and Control}},
  year     = {2011},
  pages    = {1485--1491},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hehn, Andrea - 2011 - Quadrocopter Trajectory Generation and Control.pdf:pdf},
  keywords = {autonomous vehicle navigation,control,flying robots,guidance and,making,mission planning and decision,motion control,trajectory generation},
}

@Article{li2011dynamic,
  author    = {Li, Jun and Li, Yuntang},
  title     = {{Dynamic analysis and PID control for a quadrotor}},
  journal   = {2011 IEEE International Conference on Mechatronics and Automation},
  year      = {2011},
  pages     = {573--578},
  month     = {aug},
  doi       = {10.1109/ICMA.2011.5985724},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Li - 2011 - Dynamic analysis and PID control for a quadrotor.pdf:pdf},
  isbn      = {978-1-4244-8113-2},
  keywords  = {- quadrotor,dynamic model,pid controller},
  publisher = {Ieee},
  url       = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5985724},
}

@Article{raffo2010integral,
  author    = {Raffo, Guilherme V. and Ortega, Manuel G. and Rubio, Francisco R.},
  title     = {{An integral predictive/nonlinear control structure for a quadrotor helicopter}},
  journal   = {Automatica},
  year      = {2010},
  volume    = {46},
  number    = {1},
  pages     = {29--39},
  month     = {jan},
  issn      = {00051098},
  annote    = {- Hinf controller/model predictive for path follower{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Bietjie bo-oor my kop...moet weer hierdeur lees{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Results look good},
  doi       = {10.1016/j.automatica.2009.10.018},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raffo, Ortega, Rubio - 2010 - An integral predictivenonlinear control structure for a quadrotor helicopter.pdf:pdf},
  keywords  = {control,nonlinear h},
  publisher = {Elsevier Ltd},
  url       = {http://linkinghub.elsevier.com/retrieve/pii/S0005109809004798},
}

@Misc{johnsonoptical,
  author      = {Johnson, Richard and Hines, Braden},
  title       = {{Optical proxy for sensing and pointing of light sources.pdf}},
  institution = {Thermata, Inc.},
}

@Misc{hines2013compound,
  author      = {Hines, Braden},
  title       = {{Compound optical proxy for sensing and pointing of light sources.pdf}},
  year        = {2013},
  institution = {Solar Reserve},
}

@Misc{hinescompound,
  author      = {Hines, Braden and Bailey, Terry and Johnson, Richard},
  title       = {{Compound optical proxy for sensing and pointing of light sources}},
  institution = {SolarReserve},
}

@Misc{hines2012apparatus,
  author      = {Hines, Braden and Johnson, Richard},
  title       = {{Apparatus and method for pointing light sources}},
  year        = {2012},
  annote      = {International Patent application},
  institution = {Thermata Inc.},
}

@Misc{hines2014apparatus,
  author      = {Hines, Braden and Johnson, Richard},
  title       = {{Apparatus and method for pointing light sources}},
  year        = {2014},
  annote      = {United states patent},
  institution = {Thermata},
}

@Article{klatzky2006cognitive,
  author    = {Klatzky, Roberta L and Marston, James R and Giudice, Nicholas A and Golledge, Reginald G and Loomis, Jack M},
  title     = {Cognitive load of navigating without vision when guided by virtual sound versus spatial language.},
  journal   = {Journal of experimental psychology: Applied},
  year      = {2006},
  volume    = {12},
  number    = {4},
  pages     = {223--232},
  issn      = {1076898X},
  abstract  = {A vibrotactile N-back task was used to generate cognitive load while participants were guided along virtual paths without vision. As participants stepped in place, they moved along a virtual path of linear segments. Information was provided en route about the direction of the next turning point, by spatial language ("left," "right," or "straight") or virtual sound (i.e., the perceived azimuth of the sound indicated the target direction). The authors hypothesized that virtual sound, being processed at direct perceptual levels, would have lower load than even simple language commands, which require cognitive mediation. As predicted, whereas the guidance modes did not differ significantly in the no-load condition, participants showed shorter distance traveled and less time to complete a path when performing the N-back task while navigating with virtual sound as guidance. Virtual sound also produced better N-back performance than spatial language. By indicating the superiority of virtual sound for guidance when cognitive load is present, as is characteristic of everyday navigation, these results have implications for guidance systems for the visually impaired and others.},
  file      = {:home/jaycee/PhD Papers/klatzky2006.pdf:pdf},
  keywords  = {Blind,Cognitive load,Navigation,Perception,Virtual sound},
  publisher = {American Psychological Association},
}

@Book{aloimonos2013active,
  title     = {Active perception},
  publisher = {Psychology Press},
  year      = {2013},
  author    = {Aloimonos, Yiannis},
  month     = {may},
  isbn      = {9780203773178},
  doi       = {10.4324/9780203773178},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aloimonos - 2013 - Active Perception.pdf:pdf},
  url       = {https://www.taylorfrancis.com/books/9780203773178},
}

@Article{bourne2017magnitude,
  author    = {Bourne, Rupert R A and Flaxman, Seth R and Braithwaite, Tasanee and Cicinelli, Maria V and Das, Aditi and Jonas, Jost B and Keeffe, Jill and Kempen, John H and Leasher, Janet and Limburg, Hans and Naidoo, Kovin and Pesudovs, Konrad and Resnikoff, Serge and Silvester, Alex and Stevens, Gretchen A and Tahhan, Nina and Wong, Tien Y and Taylor, Hugh R and {Vision Loss Expert Group}},
  title     = {Magnitude, temporal trends, and projections of the global prevalence of blindness and distance and near vision impairment: a systematic review and meta-analysis},
  journal   = {The Lancet Global Health},
  year      = {2017},
  volume    = {5},
  number    = {9},
  pages     = {e888--e897},
  issn      = {2214-109X},
  abstract  = {Background Global and regional prevalence estimates for blindness and vision impairment are important for the development of public health policies. We aimed to provide global estimates, trends, and projections of global blindness and vision impairment. Methods We did a systematic review and meta-analysis of population-based datasets relevant to global vision impairment and blindness that were published between 1980 and 2015. We fitted hierarchical models to estimate the prevalence (by age, country, and sex), in 2015, of mild visual impairment (presenting visual acuity worse than 6/12 to 6/18 inclusive), moderate to severe visual impairment (presenting visual acuity worse than 6/18 to 3/60 inclusive), blindness (presenting visual acuity worse than 3/60), and functional presbyopia (defined as presenting near vision worse than N6 or N8 at 40 cm when best-corrected distance visual acuity was better than 6/12). Findings Globally, of the 7{\textperiodcentered}33 billion people alive in 2015, an estimated 36{\textperiodcentered}0 million (80{\%} uncertainty interval [UI] 12{\textperiodcentered}9–65{\textperiodcentered}4) were blind (crude prevalence 0{\textperiodcentered}48{\%}; 80{\%} UI 0{\textperiodcentered}17–0{\textperiodcentered}87; 56{\%} female), 216{\textperiodcentered}6 million (80{\%} UI 98{\textperiodcentered}5–359{\textperiodcentered}1) people had moderate to severe visual impairment (2{\textperiodcentered}95{\%}, 80{\%} UI 1{\textperiodcentered}34–4{\textperiodcentered}89; 55{\%} female), and 188{\textperiodcentered}5 million (80{\%} UI 64{\textperiodcentered}5–350{\textperiodcentered}2) had mild visual impairment (2{\textperiodcentered}57{\%}, 80{\%} UI 0{\textperiodcentered}88–4{\textperiodcentered}77; 54{\%} female). Functional presbyopia affected an estimated 1094{\textperiodcentered}7 million (80{\%} UI 581{\textperiodcentered}1–1686{\textperiodcentered}5) people aged 35 years and older, with 666{\textperiodcentered}7 million (80{\%} UI 364{\textperiodcentered}9–997{\textperiodcentered}6) being aged 50 years or older. The estimated number of blind people increased by 17{\textperiodcentered}6{\%}, from 30{\textperiodcentered}6 million (80{\%} UI 9{\textperiodcentered}9–57{\textperiodcentered}3) in 1990 to 36{\textperiodcentered}0 million (80{\%} UI 12{\textperiodcentered}9–65{\textperiodcentered}4) in 2015. This change was attributable to three factors, namely an increase because of population growth (38{\textperiodcentered}4{\%}), population ageing after accounting for population growth (34{\textperiodcentered}6{\%}), and reduction in age-specific prevalence (−36{\textperiodcentered}7{\%}). The number of people with moderate and severe visual impairment also increased, from 159{\textperiodcentered}9 million (80{\%} UI 68{\textperiodcentered}3–270{\textperiodcentered}0) in 1990 to 216{\textperiodcentered}6 million (80{\%} UI 98{\textperiodcentered}5–359{\textperiodcentered}1) in 2015. Interpretation There is an ongoing reduction in the age-standardised prevalence of blindness and visual impairment, yet the growth and ageing of the world's population is causing a substantial increase in number of people affected. These observations, plus a very large contribution from uncorrected presbyopia, highlight the need to scale up vision impairment alleviation efforts at all levels. Funding Brien Holden Vision Institute.},
  doi       = {10.1016/S2214-109X(17)30293-0},
  file      = {:home/jaycee/PhD Papers/1-s2.0-S2214109X17302930-main.pdf:pdf},
  pmid      = {28779882},
  publisher = {Elsevier},
  url       = {http://www.ncbi.nlm.nih.gov/pubmed/28779882},
}

@Article{wu2010fitts,
  author    = {Wu, Jinglong and Yang, Jiajia and Honda, Taichi},
  title     = {Fitts’ law holds for pointing movements under conditions of restricted visual feedback},
  journal   = {Human movement science},
  year      = {2010},
  volume    = {29},
  number    = {6},
  pages     = {882--892},
  issn      = {01679457},
  abstract  = {Fitts' law robustly predicts the time required to move rapidly to a target. However, it is unclear whether Fitts' law holds for visually guided actions under visually restricted conditions. We tested whether Fitts' law applies under various conditions of visual restriction and compared pointing movements in each condition. Ten healthy participants performed four pointing movement tasks under different visual feedback conditions, including full-vision (FV), no-hand-movement (NM), no-target-location (NT), and no-vision (NV) feedback conditions. The movement times (MTs) for each task exhibited highly linear relationships with the index of difficulty (r2{\textgreater} 96). These findings suggest that pointing movements follow Fitts' law even when visual feedback is restricted or absent. However, the MTs and accuracy of pointing movements decreased for difficult tasks involving visual restriction. {\textcopyright} 2010 Elsevier B.V.},
  doi       = {10.1016/j.humov.2010.03.009},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Yang, Honda - 2010 - Fitts' law holds for pointing movements under conditions of restricted visual feedback.pdf:pdf},
  isbn      = {0167-9457},
  keywords  = {Fitts' law,Motor control,Movement time,Pointing movement,Visual restriction},
  pmid      = {20659774},
  publisher = {Elsevier},
  url       = {http://dx.doi.org/10.1016/j.humov.2010.03.009},
}

@Article{guastavino2004perceptual,
  author    = {Guastavino, Catherine and Katz, Brian F G},
  title     = {Perceptual evaluation of multi-dimensional spatial audio reproduction},
  journal   = {The Journal of the Acoustical Society of America},
  year      = {2004},
  volume    = {116},
  number    = {2},
  pages     = {1105--1115},
  issn      = {00014966},
  abstract  = {Perceptual differences between sound reproduction systems with multiple spatial dimensions have been investigated. Two blind studies were performed using system configurations involving 1-D, 2-D, and 3-D loudspeaker arrays. Various types of source material were used, ranging from urban soundscapes to musical passages. Experiment I consisted in collecting subjects' perceptions in a free-response format to identify relevant criteria for multi-dimensional spatial sound reproduction of complex auditory scenes by means of linguistic analysis. Experiment II utilized both free response and scale judgments for seven parameters derived form Experiment I. Results indicated a strong correlation between the source material (sound scene) and the subjective evaluation of the parameters, making the notion of an "optimal" reproduction method difficult for arbitrary source material.},
  doi       = {10.1121/1.1763973},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guastavino, Katz - 2004 - Perceptual evaluation of multi-dimensional spatial audio reproduction.pdf:pdf},
  isbn      = {0001-4966 (Print) 0001-4966 (Linking)},
  pmid      = {15376676},
  publisher = {ASA},
}

@Article{kammoun2012navigation,
  author    = {Kammoun, S. and Parseihian, G. and Gutierrez, O. and Brilhault, A. and Serpa, A. and Raynal, M. and Oriola, B. and Mac{\'{E}}, M. J.M. and Auvray, M. and Denis, M. and Thorpe, S. J. and Truillet, P. and Katz, B. F.G. and Jouffrais, C.},
  title     = {Navigation and space perception assistance for the visually impaired: The NAVIG project},
  journal   = {Irbm},
  year      = {2012},
  volume    = {33},
  number    = {2},
  pages     = {182--189},
  issn      = {19590318},
  abstract  = {Navigation, especially in unknown areas, remains a major problem for the visually impaired (VI). Over the past 50 years, a number of electronic travel aids (ETAs) have been developed with the aim of improving the mobility of the VI. Despite the efforts, these systems are rarely used. Although the explanation is likely to be incomplete, it is possible to identify three important factors: (1) positioning accuracy provided by these devices is not sufficient to guide a VI pedestrian, (2) these systems are based on Geographical Information Systems not adapted to pedestrian mobility, and (3) the guidance methods should be adapted to the task of pedestrian navigation. The NAVIG project aims to answer all these limitations through a participatory design framework with the VI and orientation and mobility instructors. The NAVIG device aims to complement conventional mobility aids (i.e. white cane and guide dog), while also adding unique features to localize specific objects in the environment, restore some visuomotor abilities, and assist navigation. {\textcopyright} 2012 Elsevier Masson SAS. All rights reserved.},
  doi       = {10.1016/j.irbm.2012.01.009},
  file      = {:home/jaycee/PhD Papers/1-s2.0-S1959031812000103-main.pdf:pdf},
  publisher = {Elsevier},
  url       = {http://dx.doi.org/10.1016/j.irbm.2012.01.009},
}

@Article{loomis2001navigating,
  author    = {Loomis, JM and Klatzky, RL and Golledge, RG and others},
  title     = {Navigating without vision: basic and applied research},
  journal   = {Optometry \& Vision Science},
  year      = {2001},
  volume    = {78},
  number    = {5},
  pages     = {282--289},
  issn      = {10405488 (ISSN)},
  abstract  = {We describe some of the results of our program of basic and applied research on navigating without vision. One basic research topic that we have studied extensively is path integration, a form of navigation in which perceived self-motion is integrated over time to obtain an estimate of current position and orientation. In experiments on pathway completion, one test of path integration ability, we have found that subjects who are passively guided over the outbound path without vision exhibit significant errors when attempting to return to the origin but are nevertheless sensitive to turns and segment lengths in the stimulus path. We have also found no major differences in path integration ability among blind and sighted populations. A model we have developed that attributes errors in path integration to errors in encoding the stimulus path is a good beginning toward understanding path integration performance. In other research on path integration, in which optic flow information was manipulated in addition to the proprioceptive and vestibular information of nonvisual locomotion, we have found that optic flow is a weak input to the path integration process. In other basic research, our studies of auditory distance perception in outdoor environments show systematic underestimation of sound source distance. Our applied research has been concerned with developing and evaluating a navigation system for the visually impaired that uses three recent technologies: the Global Positioning System, Geographic Information Systems, and virtual acoustics. Our work shows that there is considerable promise of these three technologies in allowing visually impaired individuals to navigate and learn about unfamiliar environments without the assistance of human guides.},
  file      = {:home/jaycee/PhD Papers/Navigating{\_}without{\_}Vision{\_}{\_}Basic{\_}and{\_}Applied.11.pdf:pdf},
  keywords  = {Animals,Auditory distance perception,Blind,Blindness,Cognition,Distance Perception,Error analysis,GIS,GPS,Geographic Information System,Global Positioning System,Humans,Locomotion,Navigation,Navigation system for the blind,Ophthalmology,Optical properties,Path integration,Population statistics,Research,Self-motion,Sensory aids,Sound Localization,Space Perception,Spatial hearing,Virtual sound,Vision,Visually impaired,article,blindness,clinical article,controlled study,distance perception,environmental factor,hearing,human,locomotion,motion,priority journal,stimulus response,task performance,technology,vestibular system,virtual reality,vision},
  publisher = {LWW},
  url       = {https://insights.ovid.com/crossref?an=00006324-200105000-00011{\%}0Ahttp://www.scopus.com/inward/record.url?eid=2-s2.0-0034997761{\&}partnerID=40{\&}md5=43aa7d02fd50e6abd2cb4f15f8a6eb94},
}

@Article{lewald2013exceptional,
  author    = {Lewald, J{\"{o}}rg},
  title     = {Exceptional ability of blind humans to hear sound motion: implications for the emergence of auditory space},
  journal   = {Neuropsychologia},
  year      = {2013},
  volume    = {51},
  number    = {1},
  pages     = {181--186},
  issn      = {00283932},
  abstract  = {Blind people may compensate for their visual loss by the increased use of auditory spatial information, thus showing normal or even supra-normal ability to localize sources of sound. However, the problem of how blind persons develop and maintain an internal concept of the topography of the auditory space in the absence of calibration by visual information is still unsolved. The present study demonstrated a substantial superiority of blind subjects in perception of auditory motion: The minimum audible movement angle of blind subjects (mean 3°) was about half the value found in matched sighted controls, whereas no such advantage was demonstrable for localization of stationary sound. There were no significant differences between early or congenitally blind subjects and late blind subjects, suggesting that long-term visual deprivation per se, independently of the point in time of its onset, was relevant for the superiority in auditory motion perception. The results were compatible with the hypothesis that in the absence of visual input the calibration of the auditory space is performed by audiomotor feedback, that is, by the evaluation of systematic changes of auditory spatial cues resulting from head and body movements. It is reasonable to assume that with blindness the neuronal circuits specifically concerned with the analysis of auditory motion are more intensely trained than in sighted people. It seems possible that the higher demand of motion analysis associated with blindness is related to processes of reorganization in the brain, as have been previously reported to occur also in areas known to be involved in auditory and/or visual motion analysis in sighted persons. {\textcopyright} 2012 Elsevier Ltd.},
  doi       = {10.1016/j.neuropsychologia.2012.11.017},
  file      = {:home/jaycee/PhD Papers/1-s2.0-S0028393212004848-main.pdf:pdf},
  keywords  = {Auditory motion,Blindness,Multisensory plasticity,Sound localization,Spatial hearing},
  publisher = {Elsevier},
}

@Article{ward2010visual,
  author    = {Ward, Jamie and Meijer, Peter},
  title     = {{Visual Experiences in the Blind Induced by an Auditory Sensory Substitution Device}},
  journal   = {Conciousness and Cognition},
  year      = {2010},
  volume    = {19},
  number    = {1},
  pages     = {492--500},
  issn      = {10538100},
  abstract  = {In this report, the phenomenology of two blind users of a sensory substitution device - "The vOICe" - that converts visual images to auditory signals is described. The users both report detailed visual phenomenology that developed within months of immersive use and has continued to evolve over a period of years. This visual phenomenology, although triggered through use of The vOICe, is likely to depend not only on online visualization of the auditory signal but also on the users' previous (albeit distant) experience of veridical vision (e.g. knowledge of shapes and visual perspective). Once established, the sensory substitution mapping between the auditory and visual domains is not confined to when the device is worn and, thus, may constitute an example of acquired synaesthesia. ?? 2009 Elsevier Inc. All rights reserved.},
  annote    = {Study on the vOICe},
  doi       = {10.1016/j.concog.2009.10.006},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ward, Meijer - 2010 - Visual experiences in the blind induced by an auditory sensory substitution device.pdf:pdf},
  isbn      = {1053-8100},
  keywords  = {Blind,Mental imagery,Sensory substitution,Synaesthesia/synesthesia,Visual consciousness,vOICe},
  pmid      = {19955003},
  publisher = {Elsevier Inc.},
  url       = {http://dx.doi.org/10.1016/j.concog.2009.10.006},
}

@Article{arditi2013user,
  author   = {Arditi, Aries and Tian, YingLi},
  title    = {{User Interface Preferences in the Design if a Camera-Based Navigation and Wayfinding Aid}},
  journal  = {Journal of Visual Impairment {\&} Blindness},
  year     = {2013},
  volume   = {107},
  number   = {2},
  pages    = {118--129},
  issn     = {0145-482X},
  abstract = {Introduction: Development of a sensing device that can provide a sufficient perceptual substrate for persons with visual impairments to orient themselves and travel confidently has been a persistent rehabilitation technology goal, with the user interface posing a significant challenge. In the study presented here, we enlist the advice and ideas of individuals who are blind with respect to this challenge, for an envisioned camera-based aid to navigation and wayfinding. Methods: We administered a short questionnaire about user preferences and needs for such a device to a sample of 10 well-educated, employed (or retired) visually impaired participants with light perception or less, who were familiar and comfortable with assistive technology. Generally, the items were rankings of relative priority. Results: Participants preferred speech as a communications medium for navigating the environment; preferred controlling the auditory display by querying the system rather than interacting via a menu or receiving a stream of continuous speech; and preferred providing input to the system through a keypad rather than through a voice recognition system. Architectural features such as doors and stairs were ranked the most important environmental objects to be located with such a device (over furniture, persons, personal items, and even text signs). Discussion: Our sample reported a desire for devices that can guide them to architectural features of their environment. They appear to prefer device interfaces that give them control, and would rather query a system than interact with a menu. They prefer unobtrusive input on a device via keypad rather than through voice recognition. Implications for practitioners: Designers of camera-based navigation devices may wish to consider the preferences of our sample by incorporating a query-based interface with simple keypad input and speech output, and to include in their object recognition efforts the goal of identifying architectural features that are significant to users who are blind in navigation.},
  annote   = {Query for output found to be preferential Unobvious that they are visiually impaired very important},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arditi, Tian - 2013 - User Interface Preferences in the Design of a Camera-Based Navigation and Wayfinding Aid.pdf:pdf},
  keywords = {Adult,Aged,Assistive Technology Devices -- Utilization,Chi Square Test,Disabled,Equipment Design,Friedman Test,Human,Mann-Whitney U Test,Middle Age,Questionnaires,Vision Disorders},
}

@Article{tian2013computer,
  author   = {Tian, Yingli and Yang, Xiaodong and Yi, Chucai and Arditi, Aries},
  title    = {{Toward a Computer Vision-Based Wayfinding Aid for Blind Persons to Access Unfamiliar Indoor Environments}},
  journal  = {Machine Vision and Applications},
  year     = {2013},
  volume   = {24},
  number   = {3},
  pages    = {521--535},
  issn     = {09328092},
  abstract = {Independent travel is a well known challenge for blind and visually impaired persons. In this paper, we propose a proof-of-concept computer vision-based wayfinding aid for blind people to independently access unfamiliar indoor environments. In order to find different rooms (e.g. an office, a lab, or a bathroom) and other building amenities (e.g. an exit or an elevator), we incorporate object detection with text recognition. First we develop a robust and efficient algorithm to detect doors, elevators, and cabinets based on their general geometric shape, by combining edges and corners. The algorithm is general enough to handle large intra-class variations of objects with different appearances among different indoor environments, as well as small inter-class differences between different objects such as doors and door-like cabinets. Next, in order to distinguish intra-class objects (e.g. an office door from a bathroom door), we extract and recognize text information associated with the detected objects. For text recognition, we first extract text regions from signs with multiple colors and possibly complex backgrounds, and then apply character localization and topological analysis to filter out background interference. The extracted text is recognized using off-the-shelf optical character recognition (OCR) software products. The object type, orientation, location, and text information are presented to the blind traveler as speech.},
  doi      = {10.1007/s00138-012-0431-7},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tian et al. - 2013 - Toward a computer vision-based wayfinding aid for blind persons to access unfamiliar indoor environments.pdf:pdf},
  isbn     = {0013801204317},
  keywords = {Blind/visually impaired persons,Computer vision,Indoor wayfinding,Object detection,Optical character recognition (OCR),Text extraction},
  pmid     = {23630409},
}

@Article{lee2015rgb,
  author    = {Lee, YH and Medioni, G},
  title     = {{RGB-D Camera Based Wearable Navigation System for the Visually Impaired}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2015},
  volume    = {149},
  pages     = {3--20},
  issn      = {1090235X},
  abstract  = {In this paper, a novel wearable RGB-D camera based indoor navigation system for the visually impaired is presented. The system guides the visually impaired user from one location to another location without a prior map or GPS information. Accurate real-time egomotion estimation, mapping, and path planning in the presence of obstacles are essential for such a system. We perform real-time 6-DOF egomotion estimation using sparse visual features, dense point clouds, and the ground plane to reduce drift from a head-mounted RGB-D camera. The system also builds 2D probabilistic occupancy grid map for efficient traversability analysis which is a basis for dynamic path planning and obstacle avoidance. The system can store and reload maps generated by the system while traveling and continually expand the coverage area of navigation. Next, the shortest path between the start location to destination is generated. The system generates a safe and efficient way point based on the traversability analysis result and the shortest path and updates the way point while a user is constantly moving. Appropriate cues are generated and delivered to a tactile feedback system to guide the visually impaired user to the way point. The proposed wearable system prototype is composed of multiple modules including a head-mounted RGB-D camera, standard laptop that runs a navigation software, smart phone user interface, and haptic feedback vest. The proposed system achieves real-time navigation performance at 28.6Hz in average on a laptop, and helps the visually impaired extends the range of their activities and improve the orientation and mobility performance in a cluttered environment. We have evaluated the performance of the proposed system in mapping and localization with blind-folded and the visually impaired subjects. The mobility experiment results show that navigation in indoor environments with the proposed system avoids collisions successfully and improves mobility performance of the user compared to conventional and state-of-the-art mobility aid devices.},
  annote    = {INdoor navigation and path planning, SLAM with RGBD camera},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Medioni - 2015 - RGB-D camera based wearable navigation system for the visually impaired.pdf:pdf},
  keywords  = {Assistive technologies for the visually impaired,Visual SLAM,Wearable navigation system},
  publisher = {Elsevier Inc.},
}

@Article{chessa2016integrated,
  author    = {Chessa, Manuela and Noceti, Nicoletta and Odone, Francesca and Solari, Fabio and Sosa-Garc{\'{i}}a, Joan and Zini, Luca},
  title     = {{An Integrated Artificial Vision Framework for Assisting Visually Impaired Users}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2016},
  volume    = {149},
  pages     = {209--228},
  issn      = {10773142},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chessa et al. - 2016 - An integrated artificial vision framework for assisting visually impaired users.pdf:pdf},
  keywords  = {Applications for the visually impaired,Scene under,applications for the visually,impaired},
  publisher = {Elsevier Inc.},
}

@Article{rivlin2000control,
  author    = {Rivlin, Ehud and Rotstein, H{\'{e}}ctor},
  title     = {Control of a camera for active vision: Foveal vision, smooth tracking and saccade},
  journal   = {International Journal of Computer Vision},
  year      = {2000},
  volume    = {39},
  number    = {2},
  pages     = {81--96},
  issn      = {09205691},
  abstract  = {In an active vision system, the information available for feedback{\$}\backslash{\$}nhas to be computed from images acquired on real-time. This image{\$}\backslash{\$}nprocessing task can be seen as a degree of freedom that the designer has{\$}\backslash{\$}nat hand to maximize the tracking performance, for instance one can{\$}\backslash{\$}ndefine a region of the image where heavy processing is performed - the{\$}\backslash{\$}nfovea. Based on control considerations, this paper shows how to compute{\$}\backslash{\$}nthe optimal size of this fovea, and presents a two-tracking mechanism{\$}\backslash{\$}nwhich is necessary if foveated vision is to be implemented on a real{\$}\backslash{\$}nenvironment},
  annote    = {Control, not really relevent},
  doi       = {10.1023/A:1008166825510},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rivlin, Rotstein - 2000 - Control of a camera for active vision Foveal vision, smooth tracking and saccade.pdf:pdf},
  isbn      = {0-7803-2975-9},
  keywords  = {active vision,saccade,smooth pursuit},
  publisher = {Springer},
}

@Book{hersh2008assistive,
  title     = {Assistive Technology for Visually Impaired and Blind People},
  publisher = {Springer-Verlag London},
  year      = {2008},
  author    = {Hersh, Marion A. and Johnson, Michael A.},
  edition   = {1},
  isbn      = {1846288665,9781846288661},
  abstract  = {This book is designed to inform a wide range of current and future professionals about the assistive technology used by visually impaired and blind people to achieve independence and social inclusiveness in the home and the wider community. Basic engineering principles are explained and the ways these are used to develop and drive assistive technology applications for visually impaired and blind people described. The volume has some chapters that refer to a generic comprehensive assistive technologymodel to capture the essentials of the applied systemand this model should find applications in other assistive technology areas.},
  booktitle = {Chemistry {\&} {\ldots}},
  doi       = {10.1007/978-1-84628-867-8},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hersh, Johnson - 2008 - Assistive Technology for Visually Impareired and Blind People.pdf:pdf},
  pages     = {743},
  url       = {http://onlinelibrary.wiley.com/doi/10.1002/cbdv.200490137/abstract},
}

@Article{khoo2016multimodal,
  author    = {Khoo, Wai Lun and Zhu, Zhigang},
  title     = {Multimodal and alternative perception for the visually impaired: a survey},
  journal   = {Journal of Assistive Technologies},
  year      = {2016},
  volume    = {10},
  number    = {1},
  pages     = {11--26},
  issn      = {20428723},
  abstract  = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
  doi       = {10.1108/JAT-04-2015-0014},
  file      = {:home/jaycee/PhD Papers/JAT-04-2015-0014.pdf:pdf},
  keywords  = {Assistive technologies,Auditory,Games,Haptic,Prostheses,Virtual environment},
  publisher = {Emerald Group Publishing Limited},
}

@Article{holland2002audiogps,
  author    = {Holland, Simon and Morse, David R. and Gedenryd, Henrik},
  title     = {AudioGPS: Spatial audio navigation with a minimal attention interface},
  journal   = {Personal and Ubiquitous computing},
  year      = {2002},
  volume    = {6},
  number    = {4},
  pages     = {253--259},
  issn      = {16174909},
  abstract  = {(2002). AudioGPS: Spatial audio navigation with a minimal attention interface. Personal and Ubiquitous Computing, 6(4) pp. 253–259.: +44 (0) 1908 653148, +44 (0) 1908 858463, +44 (0) 1908 659542 Fax: +44 (0) 1908 652140 Abstract In this paper we describe a prototype audio user interface for a Global Positioning System (GPS) that is designed to allow mobile computer users to carry out a location task while their eyes, hands or attention are otherwise engaged. Audio user interfaces for GPS have typically been designed to meet the needs of visually impaired users, and generally (though not exclusively) employ speech-audio. In contrast, our prototype system uses a simple form of non-speech, spatial audio. This paper analyses various candidate audio mappings of location and distance information. A variety of tasks, design considerations, technological opportunities and design trade-offs are considered. The findings from pilot evaluation experiments are reported. Finally, opportunities for improvements to the system and for future empirical testing are explored.},
  annote    = {Very relevant. Not portable Uses 3d spatialisation Sparse test Report good findings},
  doi       = {10.1007/s007790200025},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holland, Morse, Gedenryd - 2002 - AudioGPS Spatial audio navigation with a minimal attention interface.pdf:pdf},
  isbn      = {1617-4909},
  keywords  = {Audio,Global Positioning System (GPS),Location,Minimal Attention User Interface,Navigation,Non-speech audio,Sound,Spatial audio},
  pmid      = {21301092},
  publisher = {Springer},
}

@Article{ashmead1998spatial,
  author    = {Ashmead, Daniel H. and Wall, Robert S. and Ebinger, Kiara A. and Eaton, Susan B. and Snook-Hill, Mary M. and Yang, Xuefeng},
  title     = {Spatial hearing in children with visual disabilities},
  journal   = {Perception},
  year      = {1998},
  volume    = {27},
  number    = {1},
  pages     = {105--122},
  issn      = {03010066},
  abstract  = {A study is reported of the effect of early visual experience on the development of auditory space perception. The spatial hearing of thirty-five children with visual disabilities (twenty-two with congenital total blindness) was compared with that of eighteen sighted children and seventeen sighted adults. The tests provided a comprehensive assessment of spatial-hearing ability, including psychophysical estimates of spatial resolution in the horizontal, vertical, and distance dimensions, as well as measures of reaching and walking to the locations of sound sources. The spatial hearing of the children with visual disabilities was comparable to or somewhat better than that of the sighted children and adults. This pattern held even when the group with visual disabilities was restricted to those children with congenital total blindness; in fact, some of those children had exceptionally good spatial hearing. These findings imply that the developmental calibration of human spatial hearing is not dependent on a history of visual experience. It seems likely that this calibration arises from the experience of changes in sound-localization cues arising from self-motion, such as turning the head or walking. As a practical matter, orientation and mobility instructors may reasonably assume that individuals with visual disabilities can use their hearing effectively in day-to-day travel situations.},
  doi       = {10.1068/p270105},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ashmead et al. - 1998 - Spatial hearing in children with visual disabilities.pdf:pdf},
  isbn      = {0301-0066},
  pmid      = {9692091},
  publisher = {SAGE Publications Sage UK: London, England},
}

@Article{algazi2001elevation,
  author    = {Algazi, V. Ralph and Avendano, Carlos and Duda, Richard O.},
  title     = {Elevation localization and head-related transfer function analysis at low frequencies},
  journal   = {The Journal of the Acoustical Society of America},
  year      = {2001},
  volume    = {109},
  number    = {3},
  pages     = {1110--1122},
  issn      = {00014966},
  abstract  = {Monaural spectral features due to pinna diffraction are the primary cues for elevation. Because these features appear above 3 kHz where the wavelength becomes comparable to pinna size, it is generally believed that accurate elevation estimation requires wideband sources. However, psychoacoustic tests show that subjects can estimate elevation for low-frequency sources. In the experiments reported, random noise bursts low-pass filtered to 3 kHz were processed with individualized head-related transfer functions (HRTFs), and six subjects were asked to report the elevation angle around four cones of confusion. The accuracy in estimating elevation was degraded when compared to a baseline test with wideband stimuli. The reduction in performance was a function of azimuth and was highest in the median plane. However, when the source was located away from the median plane, subjects were able to estimate elevation, often with surprisingly good accuracy. Analysis of the HRTFs reveals the existence of elevation-dependent features at low frequencies. The physical origin of the low-frequency features is attributed primarily to head diffraction and torso reflections. It is shown that simple geometrical approximations and models of the head and torso explain these low-frequency features and the corresponding elevations cues.},
  doi       = {10.1121/1.1349185},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Algazi, Avendano, Duda - 2001 - Elevation localization and head-related transfer function analysis at low frequencies.pdf:pdf},
  pmid      = {11303925},
  publisher = {ASA},
}

@Article{zotkin2004rendering,
  author    = {Zotkin, Dmitry N. and Duraiswami, Ramani and Davis, Larry S.},
  title     = {Rendering localized spatial audio in a virtual auditory space},
  journal   = {IEEE Transactions on Multimedia},
  year      = {2004},
  volume    = {6},
  number    = {4},
  pages     = {553--564},
  issn      = {15209210},
  abstract  = {High-quality virtual audio scene rendering is required for emerging virtual and augmented reality applications, perceptual user interfaces, and sonification of data. We describe algorithms for creation of virtual auditory spaces by rendering cues that arise from anatomical scattering, environmental scattering, and dynamical effects. We use a novel way of personalizing the head related transfer functions (HRTFs) from a database, based on anatomical measurements. Details of algorithms for HRTF interpolation, room impulse response creation, HRTF selection from a database, and audio scene presentation are presented. Our system runs in real time on an office PC without specialized DSP hardware.},
  doi       = {10.1109/TMM.2004.827516},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zotkin, Duraiswami, Davis - 2004 - Rendering localized spatial audio in a virtual auditory space.pdf:pdf},
  keywords  = {3-D audio processing,Audio user interfaces,Head-related transfer function,Spatial audio,User interfaces,Virtual auditory spaces,Virtual environments,Virtual reality},
  publisher = {IEEE},
}

@InProceedings{manduchi2014last,
  author       = {Manduchi, R and Coughlan, J},
  title        = {The last meter: blind visual guidance to a target},
  booktitle    = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
  year         = {2014},
  pages        = {3113--3122},
  organization = {ACM},
  file         = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manduchi, Coughlan - 2014 - The Last Meter Blind Visual Guidance to a Target.pdf:pdf},
}

@Article{geronazzo2016interactive,
  author    = {Geronazzo, Michele and Bedin, Alberto and Brayda, Luca and Campus, Claudio and Avanzini, Federico},
  title     = {{Interactive spatial sonification for non-visual exploration of virtual maps}},
  journal   = {International Journal of Human Computer Studies},
  year      = {2016},
  volume    = {85},
  pages     = {4--15},
  issn      = {10959300},
  abstract  = {This paper presents a multimodal interactive system for non-visual (auditory-haptic) exploration of virtual maps. The system is able to display haptically the height profile of a map, through a tactile mouse. Moreover, spatial auditory information is provided in the form of virtual anchor sounds located in specific points of the map, and delivered through headphones using customized Head-Related Transfer Functions (HRTFs). The validity of the proposed approach is investigated through two experiments on non-visual exploration of virtual maps. The first experiment has a preliminary nature and is aimed at assessing the effectiveness and the complementarity of auditory and haptic information in a goal reaching task. The second experiment investigates the potential of the system in providing subjects with spatial knowledge: specifically in helping with the construction of a cognitive map depicting simple geometrical objects. Results from both experiments show that the proposed concept, design, and implementation allow to effectively exploit the complementary natures of the "proximal" haptic modality and the "distal" auditory modality. Implications for orientation {\&} mobility (O{\&}M) protocols for visually impaired subjects are discussed.},
  file      = {:home/jaycee/PhD Papers/1-s2.0-S1071581915001287-main (1).pdf:pdf},
  keywords  = {3D audio,Binaural sound,Haptic mouse,Haptics,Multimodal interaction,Multisensory integration,Non-visual navigation,Virtual maps,Visual impairment},
  publisher = {Elsevier},
}

@Article{marentakis2006effects,
  author   = {Marentakis, Georgios N. and Brewster, Stephen a.},
  title    = {{Effects of feedback, mobility and index of difficulty on deictic spatial audio target acquisition in the horizontal plane}},
  journal  = {Proc. CHI '06, ACM Press},
  year     = {2006},
  pages    = {359},
  abstract = {We present the results of an empirical study investigating the effect of feedback, mobility and index of difficulty on a deictic spatial audio target acquisition task in the horizontal plane in front of a user. With audio feedback, spatial audio display elements are found to enable usable deictic interaction that can be described using Fitts law. Feedback does not affect perceived workload or preferred walking speed compared to interaction without feedback. Mobility is found to degrade interaction speed and accuracy by 20{\%}. Participants were able to perform deictic spatial audio target acquisition when mobile while walking at 73{\%} of their preferred walking speed. The proposed feedback design is examined in detail and the effects of variable target widths are quantified. Deictic interaction with a spatial audio display is found to be a feasible solution for future interface designs.},
  doi      = {10.1145/1124772.1124826},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marentakis, Brewster - 2006 - Effects of feedback, mobility and index of difficulty on deictic spatial audio target acquisition in th(2).pdf:pdf},
  isbn     = {1595933727},
  url      = {http://portal.acm.org/citation.cfm?doid=1124772.1124826},
}

@Article{rodriguez2012obstacle,
  author  = {Rodr{\'{i}}guez, Alberto and Bergasa, Luis M and Alcantarilla, Pablo F and Yebes, Javier and Cela, Andr{\'{e}}s},
  title   = {{Obstacle Avoidance System for Assisting Visually Impaired People}},
  journal = {Intelligent Vehicles Symposium Workshops},
  year    = {2012},
  pages   = {1--6},
  file    = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodr{\'{i}}guez et al. - 2012 - Obstacle Avoidance System for Assisting Visually Impaired People.pdf:pdf},
  isbn    = {9788469534724},
}

@Article{leo2017computer,
  author    = {Leo, M. and Medioni, G. and Trivedi, M. and Kanade, T. and Farinella, G. M.},
  title     = {{Computer vision for assistive technologies}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2017},
  volume    = {154},
  pages     = {1--15},
  issn      = {1090235X},
  abstract  = {In the last decades there has been a tremendous increase in demand for Assistive Technologies (AT) useful to overcome functional limitations of individuals and to improve their quality of life. As a consequence, different research papers addressing the development of assistive technologies have appeared into the literature pushing the need to organize and categorize them taking into account the application assistive aims. Several surveys address the categorization problem for works concerning a specific need, hence giving the overview on the state of the art technologies supporting the related function for the individual. Unfortunately, this “user-need oriented” way of categorization considers each technology as a whole and then a deep and critical explanation of the technical knowledge used to build the operative tasks as well as a discussion on their cross-contextual applicability is completely missing making thus existing surveys unlikely to be technically inspiring for functional improvements and to explore new technological frontiers. To overcome this critical drawback, in this paper an original “task oriented” way to categorize the state of the art of the AT works has been introduced: it relies on the split of the final assistive goals into tasks that are then used as pointers to the works in literature in which each of them has been used as a component. In particular this paper concentrates on a set of cross-application Computer Vision tasks that are set as the pivots to establish a categorization of the AT already used to assist some of the user's needs. For each task the paper analyzes the Computer Vision algorithms recently involved in the development of AT and, finally, it tries to catch a glimpse of the possible paths in the short and medium term that could allow a real improvement of the assistive outcomes. The potential impact on the assessment of AT considering users, medical, economical and social perspective is also addressed.},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leo et al. - 2017 - Computer vision for assistive technologies(2).pdf:pdf},
  isbn      = {1077-3142},
  keywords  = {Assistive technologies,Computer vision},
  publisher = {Elsevier Inc.},
}

@InProceedings{bouabdallah2004pid,
  author    = {Bouabdallah, Samir and Noth, Andk and Siegwan, Roland},
  title     = {{PID vs LQ Control Techniques Applied to an Indoor Micro Quadrotor}},
  booktitle = {Proceedings of the International Conference on Intelligent Robots and Systems (IROS)},
  year      = {2004},
  pages     = {2451--2456},
  publisher = {IEEE/RSJ},
  annote    = {- Early paper investigating control techniques{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Nice dynamic model derivation{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Compares LQR and PID{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- LQR slegter as PID (Moet propeller dynamics in ag neem)},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bouabdallah, Noth, Siegwan - 2004 - PID vs LQ Control Techniques Applied to an Indoor Micro Quadrotor.pdf:pdf},
}

@Article{mellinger2012trajectory,
  author   = {Mellinger, D and Michael, N and Kumar, V},
  title    = {{Trajectory generation and control for precise aggressive maneuvers with quadrotors}},
  journal  = {The International Journal of Robotics Research},
  year     = {2012},
  volume   = {31},
  number   = {5},
  pages    = {664--674},
  month    = {jan},
  issn     = {0278-3649},
  annote   = {- Trajectory design and control{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Interesting method of using 3 modes to perform and to recover from complex mauevers},
  doi      = {10.1177/0278364911434236},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mellinger, Michael, Kumar - 2012 - Trajectory generation and control for precise aggressive maneuvers with quadrotors.pdf:pdf},
  isbn     = {0278364911434},
  keywords = {control,quadrotors,trajectory generation},
  url      = {http://ijr.sagepub.com/cgi/doi/10.1177/0278364911434236},
}

@Article{hoffmann2009aerodynamics,
  author    = {Hoffmann, G.M. and Waslander, S.L. and Tomlin, C.J.},
  title     = {{Aerodynamics and control of autonomous quadrotor helicopters in aggressive maneuvering}},
  journal   = {2009 IEEE International Conference on Robotics and Automation},
  year      = {2009},
  pages     = {3277--3282},
  month     = {may},
  annote    = {- Investigates un-modelled aerodynamic effects. Has effect on high speed flight and manuevering},
  doi       = {10.1109/ROBOT.2009.5152561},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoffmann, Waslander, Tomlin - 2009 - Aerodynamics and control of autonomous quadrotor helicopters in aggressive maneuvering.pdf:pdf},
  isbn      = {978-1-4244-2788-8},
  publisher = {Ieee},
  url       = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5152561},
}

@Article{schoellig2012optimization,
  author   = {Schoellig, Angela P and Mueller, Fabian L and D'Andrea, Raffaello},
  title    = {{Optimization-based iterative learning for precise quadrocopter trajectory tracking}},
  journal  = {Autonomous Robots},
  year     = {2012},
  volume   = {33},
  number   = {1-2},
  pages    = {103--127},
  month    = {apr},
  issn     = {0929-5593},
  annote   = {- Interesting learning method. Aims to make the UAV learn its own model and adjusts the plant input accordingly.{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Method imed on 2D trajectories, but may be expanded to 3D{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Uses off-board computer for calculations. Also indoor test arena},
  doi      = {10.1007/s10514-012-9283-2},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Schoellig, Mueller, D'Andrea - 2012 - Optimization-based iterative learning for precise quadrocopter trajectory tracking.pdf:pdf},
  keywords = {aerial robotics,convex,input and state constraints,kalman filtering,learning,motion planning and control,quadrocopter control,trajectory tracking},
  url      = {http://link.springer.com/10.1007/s10514-012-9283-2},
}

@Article{raffo2010integrala,
  author    = {Raffo, Guilherme V. and Ortega, Manuel G. and Rubio, Francisco R.},
  title     = {{An integral predictive/nonlinear control structure for a quadrotor helicopter}},
  journal   = {Automatica},
  year      = {2010},
  volume    = {46},
  number    = {1},
  pages     = {29--39},
  month     = {jan},
  issn      = {00051098},
  annote    = {- Hinf controller/model predictive for path follower{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Bietjie bo-oor my kop...moet weer hierdeur lees{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Results look good},
  doi       = {10.1016/j.automatica.2009.10.018},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Raffo, Ortega, Rubio - 2010 - An integral predictivenonlinear control structure for a quadrotor helicopter.pdf:pdf},
  keywords  = {control,nonlinear h},
  publisher = {Elsevier Ltd},
  url       = {http://linkinghub.elsevier.com/retrieve/pii/S0005109809004798},
}

@Article{canal2015real,
  author        = {Canal, Gerard and Escalera, Sergio and Angulo, Cecilio},
  title         = {{A real-time Human-Robot Interaction system based on gestures for assistive scenarios}},
  journal       = {Computer Vision and Image Understanding},
  year          = {2015},
  volume        = {149},
  number        = {1},
  pages         = {65--77},
  month         = {jul},
  issn          = {1090235X},
  abstract      = {Natural and intuitive human interaction with robotic systems is a key point to develop robots assisting people in an easy and effective way. In this paper, a Human Robot Interaction (HRI) system able to recognize gestures usually employed in human non-verbal communication is introduced, and an in-depth study of its usability is performed. The system deals with dynamic gestures such as waving or nodding which are recognized using a Dynamic Time Warping approach based on gesture specific features computed from depth maps. A static gesture consisting in pointing at an object is also recognized. The pointed location is then estimated in order to detect candidate objects the user may refer to. When the pointed object is unclear for the robot, a disambiguation procedure by means of either a verbal or gestural dialogue is performed. This skill would lead to the robot picking an object in behalf of the user, which could present difficulties to do it by itself. The overall system - which is composed by a NAO and Wifibot robots, a KinectTM v2 sensor and two laptops - is firstly evaluated in a structured lab setup. Then, a broad set of user tests has been completed, which allows to assess correct performance in terms of recognition rates, easiness of use and response times.},
  address       = {Menlo Park, California},
  annote        = {From Duplicate 1 (A real-time Human-Robot Interaction system based on gestures for assistive scenarios - Canal, Gerard; Escalera, Sergio; Angulo, Cecilio) From Duplicate 16 (Apparatus and method for pointing light sources - Hines, Braden; Johnson, Richard) From Duplicate 1 (Apparatus and method for pointing light sources - Hines, Braden; Johnson, Richard) International Patent application From Duplicate 2 (Apparatus and method for pointing light sources - Hines, Braden; Johnson, Richard) United states patent From Duplicate 18 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) From Duplicate 1 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) From Duplicate 2 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) - Estimate state parameters of drone - Drone model estimation - Estimate state parameters of drone - Drone model estimation From Duplicate 2 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) From Duplicate 1 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) And Duplicate 2 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) And Duplicate 3 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) And Duplicate 5 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) - Estimate state parameters of drone - Drone model estimation From Duplicate 4 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) Parameter estimation for quadcopter From Duplicate 20 (AudioGPS: Spatial audio navigation with a minimal attention interface - Holland, Simon; Morse, David R.; Gedenryd, Henrik) Very relevant. Not portable Uses 3d spatialisation Sparse test Report good findings From Duplicate 42 (You-Do, I-Learn: Egocentric unsupervised discovery of objects and their modes of interaction towards video-based guidance - Damen, Dima; Leelasawassuk, Teesid; Mayol-Cuevas, Walterio) And Duplicate 67 (You-Do, I-Learn: Egocentric unsupervised discovery of objects and their modes of interaction towards video-based guidance - Damen, Dima; Leelasawassuk, Teesid; Mayol-Cuevas, Walterio) Automatic object detection and usage suggestion From Duplicate 48 (RGB-D camera based wearable navigation system for the visually impaired - Lee, Young Hoon; Medioni, G??rard) And Duplicate 78 (RGB-D camera based wearable navigation system for the visually impaired - Lee, Young Hoon; Medioni, G??rard) INdoor navigation and path planning, SLAM with RGBD camera From Duplicate 58 (Control of a camera for active vision: Foveal vision, smooth tracking and saccade - Rivlin, Ehud; Rotstein, H{\'{e}}ctor) And Duplicate 111 (Control of a camera for active vision: Foveal vision, smooth tracking and saccade - Rivlin, Ehud; Rotstein, H{\'{e}}ctor) Control, not really relevent From Duplicate 87 (The GuideCane - A Computerized Travel Aid for the Active Guidance of Blind Pedestrians - Borenstein, J; Ulrich, I) Pretty irrelevant From Duplicate 91 (Visually-guided obstacle avoidance in unstructured environments - Lorigo, L.M.; Brooks, R.A.; Grimsou, W.E.L.) Movement with vision robotics - not relevant From Duplicate 92 (Mobile vision as assistive technology for the blind: an experimental study - Manduchi, R) Test with Manduchi's coloured markers From Duplicate 93 (A system-prototype representing 3D space via alternative-sensing for visually impaired navigation - Bourbakis, Nikolaos; Makrogiannis, Sokratis K.; Dakopoulos, Dimitrios) Feedback via worn vibrator module From Duplicate 95 (Cell phone-based wayfinding for the visually impaired - Coughlan, James; Manduchi, Roberto; Shen, Huiying) Using special markers for waypoint guidance From Duplicate 96 (On combining visual SLAM and dense scene flow to increase the robustness of localization and mapping in dynamic environments - Alcantarilla, Pablo F.; Yebes, Jos{\'{e}} J.; Almaz{\'{a}}n, Javier; Bergasa, Luis M.) Detect movement to remove them from the scene classification -{\textgreater} make SLAM more robust From Duplicate 98 (Spatialized audio environmental awareness for blind users with a smartphone - Blum, Jeffrey R.; Bouchard, Mathieu; Cooperstock, Jeremy R.) Spacial audio interface for VIPs. Pretty good and noteworthy From Duplicate 99 (Future sight loss UK (1): The economic impact of partial sight and blindness in the UK adult population - Royal National Institute of Blind People) Backgtound on blindness in UK - Gives motivation for research From Duplicate 101 (Ageing and Visual Impairment - Suttie, A; Daly, A; Good, G; Mambetakunov, K; Orr, A; Scott, J; Techavachara, P; Verstraten, P) Good motivation for assistive tech From Duplicate 102 (Design and Analysis of a Portable Indoor Localization Aid for the Visually Impaired - Hesch, J A; Roumeliotis, S I) Cane-mounted detection. Pretty good From Duplicate 104 (Update on inclusive society - Slade, J) - 53{\%} need help to get out of house (older gen) From Duplicate 105 (Neural Correlates of Natural Human Echolocation in Early and Late Blind Echolocation Experts - Thaler, L; Arnott, S R; Goodale, M A) Blind using echolocation From Duplicate 107 (Object Recognition with FM Sonar; An Assistive Device for Blind and Visually-Impaired People - Kao, G; Probert, P; Lee, D) FM Sonar - Only classifies rough, smooth surfaces From Duplicate 108 (SWAN: System for wearable audio navigation - Wilson, Jeff; Walker, Bruce N.; Lindsay, Jeffrey; Cambias, Craig; Dellaert, Frank) Very similar to ours From Duplicate 112 ((Computer) Vision without Sight. - Manduchi, Roberto; Coughlan, James) Survey of existing tools for vision and a few ideas From Duplicate 118 (Visual experiences in the blind induced by an auditory sensory substitution device - Ward, Jamie; Meijer, Peter) Study on the vOICe From Duplicate 119 (Drishti: An integrated indoor/outdoor blind navigation system and service - Ran, Lisa; Helal, Sumi; Moore, Steve) Drishti system Indoor ultrasound Only usable in special 'smart' house From Duplicate 125 (A review of user-interface design guidelines for public information kiosk systems - Maguire, M C) Rather unhelpful From Duplicate 127 (Drishti: an integrated navigation system for visually impaired and$\backslash$ndisabled - Helal, a.; Moore, S.E.; Ramachandran, B.) Moeilik om te lees From Duplicate 128 (Investigating Touchscreen Accessibility for People with Visual Impairments - Mcgookin, David; Brewster, Stephen; Jiang, Weiwei) Not helpful},
  archiveprefix = {arXiv},
  arxivid       = {arXiv:1011.1669v3},
  chapter       = {25},
  doi           = {10.1016/j.cviu.2016.03.004},
  editor        = {Helal, A and Mokhtari, M and Abdulrazak, B},
  eprint        = {arXiv:1011.1669v3},
  file          = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davison, Mayol, Murray - 2003 - Real-Time Localisation and Mapping with Wearable Active Vision {\pounds}.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanchez et al. - 2009 - Exploiting co-adaptation for the design of symbiotic neuroprosthetic assistants.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bullock - 1995 - Co-evolutionary design Implications for evolutionary robotics.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ran, Helal, Moore - 2004 - Drishti An integrated indooroutdoor blind navigation system and service.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mcgookin, Brewster, Jiang - 2008 - Investigating Touchscreen Accessibility for People with Visual Impairments.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Holland, Morse, Gedenryd - 2002 - AudioGPS Spatial audio navigation with a minimal attention interface.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Coughlan, Manduchi, Shen - 2006 - Cell phone-based wayfinding for the visually impaired(2).pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\'{a}}ez, Escolano - 2008 - Stereo-based Aerial Obstacle Detection for the Visually Impaired.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu et al. - 2006 - A novel multimodal interface for improving visually impaired people's web accessibility.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahmaniemi, Lantz - 2009 - Augmented Reality Target Finding Based on Tactile Cues.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frauenberger, Noisternig - 2003 - 3D Audio Interfaces for the Blind.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ward, Meijer - 2010 - Visual experiences in the blind induced by an auditory sensory substitution device.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Borenstein, Ulrich - 1997 - The GuideCane - A Computerized Travel Aid for the Active Guidance of Blind Pedestrians.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bologna, Deville, Pun - 2009 - On the use of the auditory pathway to represent image scenes in real-time.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davison, Murray - 2002 - Simultaneous localization and map-building using active vision.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zwiers et al. - 2001 - A spatial hearing deficit in early-blind humans.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Giudice, Legge - 2008 - Blind Navigation and the Role of Technology, in The Engineering Handbook of Smart Technology for Aging, Disabili.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Faria et al. - 2010 - Electronic white cane for blind people navigation assistance.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Katz et al. - 2010 - NAVIG Navigation Assisted by Artificial Vision and GNSS(2).pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Royal National Institute of Blind People - 2009 - Future sight loss UK (1) The economic impact of partial sight and blindness in the UK.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kane, Wobbrock, Ladner - 2011 - Usable gestures for blind people understanding preference and performance.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Thaler, Arnott, Goodale - 2011 - Neural Correlates of Natural Human Echolocation in Early and Late Blind Echolocation Experts.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Calder - 2009 - Assistive technology interfaces for the blind.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/DiGiovanna et al. - 2009 - Coadaptive Brain - Machine Interface via Reinforcement Learning.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Friedlander, Schlueter, Mantei - 1998 - Bullseye! when Fitts' law doesn't fit.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson et al. - 2007 - SWAN System for wearable audio navigation.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ulrich, Borenstein - 2001 - Correspondence {\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_} The GuideCane — Ap.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marentakis, Brewster - 2005 - A comparison of feedback cues for enhancing pointing efficiency in interaction with spatial audio displays.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mitsunaga et al. - 2005 - Robot behavior adaptation for human-robot interaction based on policy gradient reinforcement learning.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burgess - 1992 - Techniques for low cost spatial audio.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Katz, Picinali - 2011 - Spatial audio applied to research with the Blind.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manduchi, Coughlan - 2012 - (Computer) Vision without Sight.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellotto et al. - 2012 - Cognitive visual tracking and camera control.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zotkin, Duraiswami, Davis - 2004 - Rendering localized spatial audio in a virtual auditory space.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pradeep, Medioni, Weiland - 2010 - Robot vision for the visually impaired.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abas, Legowo, Akmeliawati - 2011 - Quadrotor.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ram{\'{o}}n et al. - 2012 - Accurate Pedestrian Indoor Navigation by Tightly Coupling Foot-Mounted IMU and RFID Measurements.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Helal, Moore, Ramachandran - 2001 - Drishti an integrated navigation system for visually impaired andndisabled.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mohammad, Nishida - 2008 - Human adaptation to a miniature robot Precursors of mutual adaptation.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Brabyn, Crandall, Gerrey - 1993 - Talking signs a remote signage, solution for the blind, visually impaired and reading disabled.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lorigo, Brooks, Grimsou - 1997 - Visually-guided obstacle avoidance in unstructured environments.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Willis, Helal - 2005 - RFID information grid for blind navigation and wayfinding.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alcantarilla et al. - 2012 - On combining visual SLAM and dense scene flow to increase the robustness of localization and mapping in dyn.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manduchi, Kurniawan, Bagherinia - 2010 - Blind guidance using mobile computer vision.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bujacz, Pec - 2011 - Sonification of 3d scenes in an electronic travel aid for the blind.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hersh, Johnson - 2008 - Assistive Technology for Visually Impareired and Blind People.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Mourikis - 2012 - Vision-aided Inertial Navigation for Resource-constrained Systems.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Day et al. - 2005 - Using Spatial Audio in Minimal Attention Interfaces Towards An Effective Audio GPS Navigation System.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guerrero, Vasquez, Ochoa - 2012 - An Indoor Navigation System for the Visually Impaired.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Porzi et al. - 2012 - Visual-inertial tracking on Android for Augmented Reality applications.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tian et al. - 2013 - Toward a computer vision-based wayfinding aid for blind persons to access unfamiliar indoor environments.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodr{\'{i}}guez et al. - 2012 - Obstacle Avoidance System for Assisting Visually Impaired People.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Marentakis, Brewster - 2006 - Effects of feedback, mobility and index of difficulty on deictic spatial audio target acquisition in the h.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manduchi - 2012 - Mobile vision as assistive technology for the blind an experimental study.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Blum, Bouchard, Cooperstock - 2013 - Spatialized audio environmental awareness for blind users with a smartphone.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2020 - 2013 - UK Vision Strategy 2013-2018.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ashmead et al. - 1998 - Spatial hearing in children with visual disabilities.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arditi, Tian - 2013 - User Interface Preferences in the Design of a Camera-Based Navigation and Wayfinding Aid.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellotto - 2013 - A Multimodal Smartphone Interface for Active Perception by Visually Impaired.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pohlmeyer et al. - 2014 - Using reinforcement learning to provide stable brain-machine interface control despite neural input reorganiza.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Apostolopoulos et al. - 2012 - Integrated online localization and navigation for people with visual impairments using smart phones.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rivlin, Rotstein - 2000 - Control of a camera for active vision Foveal vision, smooth tracking and saccade.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bourbakis, Makrogiannis, Dakopoulos - 2013 - A system-prototype representing 3D space via alternative-sensing for visually impaired navi.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Maguire - 1999 - A review of user-interface design guidelines for public information kiosk systems.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Greibach - 1975 - Lecture Notes in Computer Science.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Saez Martinez, Escolano Ruiz - 2008 - Stereo-based Aerial Obstacle Detection for the Visually Impaired.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Yang, Honda - 2010 - Fitts' law holds for pointing movements under conditions of restricted visual feedback.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gallina, Bellotto, Di Luca - 2015 - Progressive co-adaptation in human-machine interaction.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowry et al. - 2015 - Visual Place Recognition A Survey.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gallina, Bellotto, Luca - Unknown - Progressive Co-adaptation in Human-Machine Interaction.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cort{\'{e}}s-Rico, Piedrahita-Sol{\'{o}}rzano - 2015 - Human-Computer Interaction – INTERACT 2015.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Canal, Escalera, Angulo - 2015 - A real-time Human-Robot Interaction system based on gestures for assistive scenarios(2).pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Katz et al. - 2010 - NAVIG Navigation Assisted by Artificial Vision and GNSS.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manduchi, Coughlan - 2014 - The Last Meter Blind Visual Guidance to a Target.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Slade - 2012 - Update on inclusive society.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Suttie et al. - 2011 - Ageing and Visual Impairment.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Flores et al. - 2015 - Vibrotactile Guidance for Wayfinding of Blind Walkers.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manduchi, Kurniawan, Bagherinia - 2010 - Blind guidance using mobile computer vision a usability study.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kao, Probert, Lee - 1996 - Object Recognition with FM Sonar An Assistive Device for Blind and Visually-Impaired People.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hesch, Roumeliotis - 2010 - Design and Analysis of a Portable Indoor Localization Aid for the Visually Impaired.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rivera-Rubio et al. - 2015 - An assistive haptic interface for appearance-based indoor navigation (under review).pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Damen, Leelasawassuk, Mayol-Cuevas - 2016 - You-Do, I-Learn Egocentric unsupervised discovery of objects and their modes of interaction.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Aghaei, Dimiccoli, Radeva - 2016 - Multi-face tracking by extended bag-of-tracklets in egocentric photo-streams.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mollaret et al. - 2015 - A multi-modal perception based assistive robotic system for the elderly.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chessa et al. - 2016 - An integrated artificial vision framework for assisting visually impaired users.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Horne et al. - 2015 - Semantic labeling for prosthetic vision.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Villamizar et al. - 2016 - Interactive multiple object learning with scanty human supervision.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiang et al. - 2016 - Enhanced Control of a Wheelchair-Mounted Robotic Manipulator Using 3-D Vision and Multimodal Interaction.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\'{a}}nchez et al. - 2016 - Localization and tracking in known large environments using portable real-time 3D sensors.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Phung, Bouzerdoum - 2015 - Pedestrian lane detection in unstructured environments for assistive navigation.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abebe, Cavallaro, Parra - 2016 - Robust multi-dimensional motion features for first-person vision activity recognition.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Canal, Escalera, Angulo - 2015 - A real-time Human-Robot Interaction system based on gestures for assistive scenarios.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/K. Narayanan et al. - 2016 - Vision-based adaptive assistance and haptic guidance for safe wheelchair corridor following.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Medioni - 2015 - RGB-D camera based wearable navigation system for the visually impaired.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Paragios et al. - 2003 - Editorial Board.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2015 - Gender and gaze gesture recognition for human-computer interaction.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Farinella et al. - 2016 - Special Issue on Assistive Computer Vision and Robotics - ``Assistive Solutions for Mobility, Communication an.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cristina, Camilleri - 2016 - Model-based head pose-free gaze estimation for assistive communication.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Algazi, Avendano, Duda - 2001 - Elevation localization and head-related transfer function analysis at low frequencies.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Leo et al. - 2017 - Computer vision for assistive technologies.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Radmard, Croft - 2017 - Active target search for high dimensional robotic systems.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kurniawan et al. - 2004 - Design and user evaluation of a spatial audio rendering system for blind users.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Guastavino, Katz - 2004 - Perceptual evaluation of multi-dimensional spatial audio reproduction.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hartmann - 1999 - How We Localize Sound.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Shepard - 2015 - in Judgments of Relative Pitch construction of.pdf:pdf;:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bujacz, Strumio - 2016 - Sonification Review of Auditory Display Solutions in Electronic Travel Aids for the Blind.pdf:pdf},
  howpublished  = {http://www.rnib.org.uk/aboutus/Research/statistics},
  institution   = {Solar Reserve},
  isbn          = {978-953-307-224-1},
  keywords      = {Dynamic Time Warping,Egocentric vision,Face tracking,Gesture recognition,Human Robot Interaction,Low frame rate vid,Pointing location estimation},
  pmid          = {15991970},
  publisher     = {Elsevier Inc.},
  url           = {http://dx.doi.org/10.1016/j.cviu.2016.09.001 http://doi.acm.org/10.1145/1647314.1647383 http://portal.acm.org/citation.cfm?doid=1085777.1085787 http://portal.acm.org/citation.cfm?doid=1124772.1124826 http://dl.acm.org/citation.cfm?id=274644.274681 http:/},
}

@Article{zhang2015gender,
  author    = {Zhang, Wenhao and Smith, Melvyn L. and Smith, Lyndon N. and Farooq, Abdul},
  title     = {{Gender and gaze gesture recognition for human-computer interaction}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2015},
  volume    = {149},
  pages     = {32--50},
  issn      = {1090235X},
  abstract  = {The identification of visual cues in facial images has been widely explored in the broad area of computer vision. However theoretical analyses are often not transformed into widespread assistive Human-Computer Interaction (HCI) systems, due to factors such as inconsistent robustness, low efficiency, large computational expense or strong dependence on complex hardware. We present a novel gender recognition algorithm, a modular eye centre localisation approach and a gaze gesture recognition method, aiming to escalate the intelligence, adaptability and interactivity of HCI systems by combining demographic data (gender) and behavioural data (gaze) to enable development of a range of real-world assistive-technology applications.The gender recognition algorithm utilises Fisher Vectors as facial features which are encoded from low-level local features in facial images. We experimented with four types of low-level features: greyscale values, Local Binary Patterns (LBP), LBP histograms and Scale Invariant Feature Transform (SIFT). The corresponding Fisher Vectors were classified using a linear Support Vector Machine. The algorithm has been tested on the FERET database, the LFW database and the FRGCv2 database, yielding 97.7{\{}{\%}{\}}, 92.5{\{}{\%}{\}} and 96.7{\{}{\%}{\}} accuracy respectively.The eye centre localisation algorithm has a modular approach, following a coarse-to-fine, global-to-regional scheme and utilising isophote and gradient features. A Selective Oriented Gradient filter has been specifically designed to detect and remove strong gradients from eyebrows, eye corners and self-shadows (which sabotage most eye centre localisation methods). The trajectories of the eye centres are then defined as gaze gestures for active HCI. The eye centre localisation algorithm has been compared with 10 other state-of-the-art algorithms with similar functionality and has outperformed them in terms of accuracy while maintaining excellent real-time performance.The above methods have been employed for development of a data recovery system that can be employed for implementation of advanced assistive technology tools. The high accuracy, reliability and real-time performance achieved for attention monitoring, gaze gesture control and recovery of demographic data, can enable the advanced human-robot interaction that is needed for developing systems that can provide assistance with everyday actions, thereby improving the quality of life for the elderly and/or disabled.},
  doi       = {10.1016/j.cviu.2016.03.014},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2015 - Gender and gaze gesture recognition for human-computer interaction.pdf:pdf},
  keywords  = {Assistive HCI,Directed advertising,Eye centre localisation,Gaze analysis,Gender recognition},
  publisher = {Elsevier Inc.},
}

@Article{rivera-rubio2015assistive,
  author    = {Rivera-Rubio, Jose and Arulkumaran, Kai and Rishi, Hemang and Alexiou, Ioannis and Bharath, Anil A},
  title     = {{An assistive haptic interface for appearance-based indoor navigation}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2015},
  volume    = {149},
  number    = {Assistive Computer Vision and Robotics},
  pages     = {126--145},
  issn      = {10773142},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rivera-Rubio et al. - 2015 - An assistive haptic interface for appearance-based indoor navigation (under review).pdf:pdf},
  keywords  = {Assistive technology,Human navigation,Localisation},
  publisher = {Elsevier},
}

@Article{abebe2016robust,
  author    = {Abebe, Girmaw and Cavallaro, Andrea and Parra, Xavier},
  title     = {{Robust multi-dimensional motion features for first-person vision activity recognition}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2016},
  volume    = {149},
  pages     = {229--248},
  issn      = {10773142},
  doi       = {10.1016/j.cviu.2015.10.015},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abebe, Cavallaro, Parra - 2016 - Robust multi-dimensional motion features for first-person vision activity recognition.pdf:pdf},
  keywords  = {Human activity recognition,First-person vision,Gri,human activity recognition},
  publisher = {Elsevier Inc.},
  url       = {http://linkinghub.elsevier.com/retrieve/pii/S1077314215002350},
}

@Article{abas2011quadrotor,
  author   = {Abas, Norafizah and Legowo, Ari and Akmeliawati, Rini},
  title    = {{Quadrotor}},
  year     = {2011},
  number   = {May},
  pages    = {17--19},
  annote   = {From Duplicate 1 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) From Duplicate 2 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) - Estimate state parameters of drone - Drone model estimation - Estimate state parameters of drone - Drone model estimation From Duplicate 2 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) From Duplicate 1 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) And Duplicate 2 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) And Duplicate 3 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) And Duplicate 5 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) - Estimate state parameters of drone - Drone model estimation From Duplicate 4 (Quadrotor - Abas, Norafizah; Legowo, Ari; Akmeliawati, Rini) Parameter estimation for quadcopter},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abas, Legowo, Akmeliawati - 2011 - Quadrotor.pdf:pdf},
  isbn     = {9781612844374},
  keywords = {-component,and parameter estimation,autonomous quadrotor,figure 1,quadrotor flying-robot,state,ukf,unscented kalman filter},
}

@Article{bouabdallah2007full,
  author    = {Bouabdallah, Samir and Siegwart, Roland},
  title     = {{Full control of a quadrotor}},
  journal   = {2007 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  year      = {2007},
  number    = {1},
  pages     = {153--158},
  month     = {oct},
  annote    = {- Nice model derivation{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- Uses integral backstepping, gives good response and disturbance rejection.{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}- OS4 project. Reffered to by a lot of other papers},
  doi       = {10.1109/IROS.2007.4399042},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bouabdallah, Siegwart - 2007 - Full control of a quadrotor.pdf:pdf},
  isbn      = {978-1-4244-0911-2},
  publisher = {Ieee},
  url       = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4399042},
}

@Article{pounds2010modelling,
  author   = {Pounds, P. and Mahony, R. and Corke, P.},
  title    = {{Modelling and control of a large quadrotor robot}},
  journal  = {Control Engineering Practice},
  year     = {2010},
  volume   = {18},
  number   = {7},
  pages    = {691--699},
  abstract = {Typical quadrotor aerial robots used in research weigh {\textless}3kg and carry payloads measured in hundreds of grams. Several obstacles in design and control must be overcome to cater for expected industry demands that push the boundaries of existing quadrotor performance. The X-4 Flyer, a 4. kg quadrotor with a 1. kg payload, is intended to be prototypical of useful commercial quadrotors. The custom-built craft uses tuned plant dynamics with an onboard embedded attitude controller to stabilise flight. Independent linear SISO controllers were designed to regulate flyer attitude. The performance of the system is demonstrated in indoor and outdoor flight. {\textcopyright} 2010 Elsevier Ltd.},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pounds, Mahony, Corke - 2010 - Modelling and control of a large quadrotor robot.pdf:pdf},
  keywords = {Aerospace,Control,Dynamics,Robotics,Unmanned vehicles},
}

@Article{mollaret2015multi,
  author    = {Mollaret, C and Mekonnen, A A and Lerasle, F and Ferran{\'{e}}, I and Pinquier, J and Boudet, B and Rumeau, P},
  title     = {{A multi-modal perception based assistive robotic system for the elderly}},
  journal   = {Computer Vision and Image Understanding},
  year      = {2015},
  volume    = {149},
  pages     = {78--97},
  issn      = {1090235X},
  abstract  = {In this paper, we present a multi-modal perception based framework to realize a non-intrusive domestic assistive robotic system. It is non-intrusive in that it only starts interaction with a user when it detects the user's intention to do so. All the robot's actions are based on multi-modal perceptions which include user detection based on RGB-D data, user's intention-for-interaction detection with RGB-D and audio data, and communication via user distance mediated speech recognition. The utilization of multi-modal cues in different parts of the robotic activity paves the way to successful robotic runs (94{\{}{\%}{\}} success rate). Each presented perceptual component is systematically evaluated using appropriate dataset and evaluation metrics. Finally the complete system is fully integrated on the PR2 robotic platform and validated through system sanity check runs and user studies with the help of 17 volunteer elderly participants.},
  doi       = {10.1016/j.cviu.2016.03.003},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mollaret et al. - 2015 - A multi-modal perception based assistive robotic system for the elderly.pdf:pdf},
  keywords  = {Assistive technology,Elderly care,Human-robot interaction,Intention detection,Multi-modal data fusion,Robotic perception},
  publisher = {Elsevier Inc.},
}

@Article{orsag2009hybrid,
  author    = {Orsag, Matko and Bogdan, Stjepan},
  title     = {{Hybrid control of quadrotor}},
  journal   = {2009 17th Mediterranean Conference on Control and Automation},
  year      = {2009},
  volume    = {1},
  pages     = {1239--1244},
  month     = {jun},
  annote    = {- Nogal {\&}{\#}039;n meh verslag. Die {\&}{\#}039;hybrid{\&}{\#}039;is basies {\&}{\#}039;n samestelling van timing sequences en {\&}{\#}039;n PID beheerder},
  doi       = {10.1109/MED.2009.5164716},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Orsag, Bogdan - 2009 - Hybrid control of quadrotor.pdf:pdf},
  isbn      = {978-1-4244-4684-1},
  keywords  = {Aerospace control,Discrete event systems,Hybrid systems},
  publisher = {Ieee},
  url       = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5164716},
}

@Book{goerzen2009survey,
  title     = {{A Survey of Motion Planning Algorithms from the Perspective of Autonomous UAV Guidance}},
  year      = {2009},
  author    = {Goerzen, C and Kong, Z and Mettler, B},
  volume    = {57},
  number    = {1-4},
  month     = {nov},
  isbn      = {1084600993831},
  annote    = {Review!!},
  booktitle = {Journal of Intelligent and Robotic Systems},
  doi       = {10.1007/s10846-009-9383-1},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goerzen, Kong, Mettler - 2009 - A Survey of Motion Planning Algorithms from the Perspective of Autonomous UAV Guidance.pdf:pdf},
  issn      = {0921-0296},
  keywords  = {algorithm,and nnx07an31a,as part of the,autonomous,complexity,foundation,guidance,heuristics,motion planning,optimization,research,san jose state university,this research was completed,trajectory,uav,under grants nnx08a134a,university of minnesota,us army},
  pages     = {65--100},
  url       = {http://link.springer.com/10.1007/s10846-009-9383-1},
}

@Article{gadewadikar2008h,
  author   = {Gadewadikar, Jyotirmay and Lewis, Frank L. and Subbarao, Kamesh and Peng, Kemao and Chen, Ben M.},
  title    = {{H-Infinity Static Output-feedback Control for Rotorcraft}},
  journal  = {Journal of Intelligent and Robotic Systems},
  year     = {2008},
  volume   = {54},
  number   = {4},
  pages    = {629--646},
  month    = {aug},
  issn     = {0921-0296},
  doi      = {10.1007/s10846-008-9279-5},
  file     = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gadewadikar et al. - 2008 - H-Infinity Static Output-feedback Control for Rotorcraft.pdf:pdf},
  keywords = {ek laboratories,national university of singapore,republic of singapore,singapore 117508},
  url      = {http://link.springer.com/10.1007/s10846-008-9279-5},
}

@Book{goerzen2009surveya,
  title     = {{A Survey of Motion Planning Algorithms from the Perspective of Autonomous UAV Guidance}},
  year      = {2009},
  author    = {Goerzen, C. and Kong, Z. and Mettler, B.},
  volume    = {57},
  number    = {1-4},
  month     = {nov},
  isbn      = {1084600993831},
  booktitle = {Journal of Intelligent and Robotic Systems},
  doi       = {10.1007/s10846-009-9383-1},
  file      = {:home/jaycee/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Goerzen, Kong, Mettler - 2009 - A Survey of Motion Planning Algorithms from the Perspective of Autonomous UAV Guidance.pdf:pdf},
  issn      = {0921-0296},
  keywords  = {algorithm,and nnx07an31a,as part of the,autonomous,complexity,foundation,guidance,heuristics,motion planning,optimization,research,san jose state university,this research was completed,trajectory,uav,under grants nnx08a134a,university of minnesota,us army},
  pages     = {65--100},
  url       = {http://link.springer.com/10.1007/s10846-009-9383-1},
}

@Article{lichtenstein2012headphone,
  author  = {Lichenstein, Richard and Smith, Daniel Clarence and Ambrose, Jordan Lynne and Moody, Laurel Anne},
  title   = {Headphone use and pedestrian injury and death in the United States: 2004--2011},
  journal = {Injury prevention},
  year    = {2012},
  volume  = {18},
  number  = {5},
  pages   = {287 -- 290},
}

@InProceedings{Lock2019a,
  Title                    = {{ActiVis: Mobile Object Detection and Active Guidance for People with Visual Impairments}},
  Author                   = {J.C. Lock and A. G. Tramontano and S. Ghidoni and N. Bellotto},
  Booktitle                = {Proc. of the Int. Conf. on Image Analysis and Processing (ICIAP)},
  Year                     = {2019}
}

@Comment{jabref-meta: databaseType:bibtex;}
