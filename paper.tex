\documentclass[acmsmall]{acmart}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[caption=false]{subfig}
\usepackage{siunitx}
\usepackage[capitalise]{cleveref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{natbib}
\usepackage{xcolor}
\usepackage[most]{tcolorbox}
%\usepackage{soul}
%\usepackage{color}
\usepackage{todonotes}

%\newcommand\hl[1]{\colorbox{yellow}{\textcolor{black}{#1}}}
\newcommand\hl[1]{\textcolor{red}{#1}}
%\newcommand\hl[1]{{\sethlcolor{yellow}\hl{#1}}}
\newtcolorbox{highlighted}{colback=yellow,coltext=black,breakable}
%\newcommand\hl[1]{%
  %\bgroup
  %\hskip0pt\color{yellow!80!black}%
  %#1%
  %\egroup
%}
%\sethlcolor{yellow}
\citestyle{acmauthoryear}

\begin{document}

\title{Experimental Analysis of a Spatialised Audio Interface for People with Visual Impairments}

\author{Jacobus C. Lock}
\authornote{Contact Author}
\email{jaycee.lock@gmail.com}
\affiliation{%
  \institution{University of Lincoln}
  \streetaddress{Brayford Pool}
  \city{Lincoln}
  \country{UK}
  \postcode{LN6 7TS}
}

\author{Iain D. Gilchrist}
\affiliation{%
  \institution{University of Bristol}
  \city{Bristol}
  \country{UK}
}

\author{Grzegorz Cielniak}
\affiliation{%
  \institution{University of Lincoln}
  \city{Lincoln}
  \country{UK}
}

\author{Nicola Bellotto}
\affiliation{%
  \institution{University of Lincoln}
  \city{Lincoln}
  \country{UK}
}

\begin{abstract}
  Sound perception is a fundamental skill for many people with severe sight impairments.
  The research presented in this paper is part of an ongoing project with the aim to create a mobile guidance aid to help people with vision impairments find objects within an unknown indoor environment.
  This system requires an effective non-visual interface and uses bone-conduction headphones to transmit audio instructions to the user.
  It has been implemented and tested with spatialised audio cues, which convey the direction of a predefined target in 3D space.
  We present an in-depth evaluation of the audio interface with several experiments that involve a large number of participants, both blindfolded and with actual visual impairments, and analyse the pros and cons of our design choices.
  In addition to producing results comparable to the state-of-the-art, we found that Fitts's Law (a predictive model for human movement) can be used to generate a metric to improve and refine the quality of the audio interface in future mobile navigation aids. 
\end{abstract}

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003120.10003121.10003126</concept_id>
<concept_desc>Human-centered computing~HCI theory, concepts and models</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003120.10003121.10003122.10011749</concept_id>
<concept_desc>Human-centered computing~Laboratory experiments</concept_desc>
<concept_significance>300</concept_significance>
</concept>
<concept>
<concept_id>10003120.10003121.10003125.10010597</concept_id>
<concept_desc>Human-centered computing~Sound-based input / output</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003120.10003121.10003125.10010873</concept_id>
<concept_desc>Human-centered computing~Pointing devices</concept_desc>
<concept_significance>100</concept_significance>
</concept>
<concept>
<concept_id>10003120.10011738.10011773</concept_id>
<concept_desc>Human-centered computing~Empirical studies in accessibility</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[100]{Human-centered computing~HCI theory, concepts and models}
\ccsdesc[300]{Human-centered computing~Laboratory experiments}
\ccsdesc[500]{Human-centered computing~Sound-based input / output}
\ccsdesc[100]{Human-centered computing~Pointing devices}
\ccsdesc[500]{Human-centered computing~Empirical studies in accessibility}

\keywords{%
  Visual impairment; active vision; guidance system; audio interface; Fitts Law
}

\maketitle

\section{Introduction}

\hl{The ActiVis project is an effort to create a fully mobile phone-based navigation aid for people with visual impairments to guide them towards a desired object or location within an unknown indoor environment.
Improvements to computer vision and machine learning techniques, as well as mobile computing hardware performance, are exploited to make this system possible.
In particular, techniques from the active vision field are used to enable a mobile device to gather information on the surrounding environment and use it to generate guidance instructions for a user with limited or no vision.
However, in the literature, these techniques are typically limited to directing electro-mechanical servos~\citep{bajcsy2018revisiting} and in this project, we attempt to find out if the same techniques can also be used to direct a user's attention to a target location. 
The proposed system was introduced by~\citet{lock2017portable}, who describe the aforementioned autonomous guidance system paired with a co-adaptive human-machine interface that changes its own parameters over time to better match the user's strengths and limitations.
In previous work we developed a prototype guidance system that uses active vision and machine learning models to gather information and help a person find objects within an unknown indoor environment, showing that these techniques can indeed successfully be applied to direct humans' attention~\citep{lock2019active}.
However, in this work we examine the effectiveness of the proposed interface for a searching task and investigate a metric that can be used in the next phases of the project to enable the aforementioned co-adaptive paradigm that could benefit the user experience and boost long-term navigation performance.}

The hardware used for the guidance system prototype are a Google Project Tango device\footnote{https://en.wikipedia.org/wiki/Tango\_(platform)}, along with a set of bone-conducting headphones (both are shown in \cref{fig:participant}).
This Tango device uses a set of embedded RGB-D cameras and other hardware and software components to provide accurate real-time localisation estimates and has powerful image-processing capabilities.
It also gives access to Android's full range of interface and I/O options.
Unlike normal headphones, bone-conducting headphones are placed on and conduct audio signals through a user's cheekbones, instead of their ear canal.
These headphones have the benefit of not interfering with a user's normal hearing function.

\begin{figure}
  \centering
  %\subfloat[]{\label{fig:tango-headphone}\includegraphics[width=0.5\columnwidth]{figures/tango_tablet_headphone_edit.pdf}}
%~
  %\subfloat[]{\label{fig:participant}\includegraphics[width=0.8\columnwidth]{figures/vi_participant_e.png}}
  \includegraphics[width=0.8\columnwidth]{figures/vi_participant_e.png}
  \caption{The Tango tablet and bone-conduction headphones in use during an experiment. The angle reference system that the guidance instructions are based on is also included , showing the camera vector, $C$, and the pan and elevation angles to the target. Adapted from~\citet{lock2019bone}.}\label{fig:participant}
  \Description{The Tango tablet and the bone-conduction headphones, the primary hardware components used in this work, and a participant with visual impairments using the system during an experiment.}
\end{figure}

The system generates guidance instructions that can be interpreted in real-time with minimal additional cognitive load to the user, given humans' natural ability to determine a sound source's 3D position.
By adjusting a tone's spectral make-up (elevation angle), time delay and level difference (pan angle), and intensity (distance), a sound source can be spatialised to come from any arbitrary location.
In this case, only the pan and elevation angles are given to the user to instruct them to point the camera towards a target object or visual feature.
However, the spectral signature generated by bone-conduction headphones cannot properly be interpreted by a human, since they are placed on the user's cheekbones and bypass the ear's outer structure.
We address this limitation by having the system convey the target's elevation angle by adjusting the tone's pitch instead of spatialising it in the elevation dimension.
In previous work, we showed that this audio signal transmission scheme can direct users to a target position with a level of accuracy comparable to fully spatialised signals used with expensive closed-cup headphones.
These results were presented at the 9th International Workshop on Assistive Engineering in 2019~\citep{lock2019bone}.
We expand upon this initial investigation with a larger dataset and look at if, and how, changing the behaviour of the pitch affects target acquisition performance in terms of time and angular error.
The participants' hearing characteristics are also measured to determine if there are limitations to how well they can determine audio pitch or direction. 

The main contributions of this paper are two-fold: 
\begin{itemize}
  \item we provide comprehensive experimental results, with two groups of participants with healthy and limited eyesight, on how well a tone with varying pitch can convey a target's elevation angle when using a mobile device with a bone-conducting headset; 
  \item we show that this sound-based human-machine interface exhibits a response that is well-modelled by Fitts's Law which can provide a useful metric of performance for similar mobile user interfaces.
\end{itemize}

The rest of the paper starts by discussing relevant works and previous research in \cref{sec:prev-work}.
This is followed by a discussion on the design and implementation of our interface in \cref{sec:system-description}.
The experiments that were conducted are discussed in \cref{sec:experiments}, while their results are presented and discussed in \cref{sec:results}.
The paper concludes with a summary of the work and discussions on future research prospects in \cref{sec:conclusion}.

\section{Previous Work}\label{sec:prev-work}

\hl{Over the years, commercial and academic groups have devised new and innovative mobile navigation and travel aids for people with visual impairments to address macro- and micro-navigation issues.
  The latter refer to the scale that navigation takes place on, with macro-navigation addressing directional guidance on a topographical map using turn-by-turn instructions, for example, while micro-navigation focusses on conveying information of the user's immediate surroundings~\citep{petrie1997mobic}. 
Many of the proposed systems make use of one or a combination of vocal~\citep{mocanu2016when,chessa2016integrated,kanwal2015navigation,sato2019navcog}, audio~\citep{schwarze2015intuitive,rodriguez2012obstacle,kammoun2012navigation} and haptic~\citep{rivera-rubio2015assistive,lee2015rgb,xiao2015assistive} feedback media to present a user with macro- and/or micro-guidance instructions, each of which has its own set of features and limitations.
The interface for this work is used to provide instructions to a stationary user to guide them to point a camera at a target location and we therefore are mainly concerned with micro-navigation tools. 
In this context, respondents with visual impairments typically prefer haptic and vocal feedback over an audio tone~\citep{arditi2013user,golledge2004stated}.
However, from an ergonomic perspective, haptic feedback has significant shortcomings and require additional external hardware to transmit guidance instructions with sufficient resolution.
Furthermore, in a micro-navigation task, high-resolution guidance and many adjustments are required to reach the target location.
This this regard, both haptic and vocal feedback can present a cognitive burden and overwhelm a user's input bandwidth, which could have a detrimental effect on performance and the user experience~\citep{klatzky2006cognitive}.}
%Indeed, \citet{arditi2013user} reports that users prefer controlling the rate at which guidance signals are presented to them, instead of being given constant guidance. 
As an alternative, simple audio tones are less prone to these bandwidth and hardware limitations.
However, such tones can potentially fatigue a user if there are too unpleasant.
Researchers have started using cameras and object detectors to determine what a user is looking at and then use simple audio tones and vocal feedback signals to indicate to the user where to find an object.
They report favourable results with these systems~\citep{schauerte2012assistive,tian2013computer,fiannaca2014headlock,vazquez2012helping}.

Work has been done in an attempt to spatialise these tones with a head-related transfer function (HRTF) that simulates a sound source being placed at some arbitrary 3D position.
The authors generally report favourable results when using normal over-ear headphones or speakers~\citep{geronazzo2016interactive,wilson2007swan,katz2010navig,blum2013spatialized,presti2019watchout,crispien1994guib}.
However, the audio transmission device used has a significant effect on performance.
Indeed, research has shown that cheaper headphones and bone-conducting headphones have diminished performance in location perception with HRTFs when compared to over-ear or other expensive headphones~\citep{stanley2006lateralization,voong2019auditory,mascetti2016sonification}.
However, this performance degradation seems limited to the elevation dimension, which is drastically improved when the HRTFs are adjusted for the bone-conduction pathway~\citep{stanley2006lateralization,pec2008individual}.
Another way around the performance issue in the elevation dimension is to transmit the elevation angle by adjusting the audio tone's pitch, such as in the work by~\citet{durette2008visuo}.
We expanded upon the latter's work and investigated using spatialised audio and a varying pitch for guidance in an initial study~\cite{lock2019bone}, and found that participants are able to adequately determine a target's elevation with this approach with performance in both pan and elevation dimensions.
The measured performance is comparable to those of more expensive and over-ear headphones.

Fitts's Law~\citep{fitts1954information} is a predictive model of human movement and is particularly useful to evaluate human-computer interactions.
Indeed, researchers have previously used Fitts's Law, and more recently MacKenzie's modified version of it~\citep{mackenzie1992fitts}, as a metric to evaluate the performance of a spatial audio HMI\@.
Fitts's Law was originally proposed for visual target search tasks, but has since been applied in non-visual target search tasks as well.
For example, experiments with a haptic feedback pointing device have been performed to evaluate how effective it was at directing a user towards a target~\citep{ahmaniemi2009augmented} and the authors showed that the search time adheres to Fitts's Law.
However, they also note that it is not a perfect fit, citing the fact that Fitts's Law does not take into account a user's search strategy.
Another group of researchers conducted experiments using a spatial audio interface to describe the position of a target on the horizontal plane~\citep{marentakis2006effects}.
Here, participants pointed to where they thought the targets were, on their left or right, as they traversed a path.
Their results show a good relation between target difficulty and search time, providing a strong argument that Fitts's Law can be used to describe the performance of a spatial audio interface.
These results have since been supported by other authors, who found that Fitts's Law provided a good explanation for the results from an experiment using visual, limited visual and non-visual feedback cues~\citep{wu2010fitts}.
However, Fitts's Law has not yet been shown to apply to a spatial tone that uses varying pitch to convey the target's elevation angle, as demonstrated in this paper.

\section{System Description}\label{sec:system-description}

\hl{Existing electronic navigation aids have typically struggled to gain market traction and replace the traditional walking cane as the standard assistive tool for people with visual impairments.
Current technological limitations include prohibitive costs, bulky hardware requirements and non-user-friendly interfaces~\citep{golledge2004stated,yusif2016older,arditi2013user}.
To address these issues, we implemented a handheld mobile system that is based on a concept proposed by~\citet{lock2019active} and tested by~\citet{lock2019bone} using a Google Tango device that is able to localise itself in real-time.
This system has the benefit of minimal hardware requirements and a compact, familiar form-factor, which will help to overcome the hurdle of user-acceptance and usability.}

\hl{People with visual impairments rely heavily on their hearing~\citep{golledge2004stated} and we wish to avoid blocking this information pathway, since doing to can have severe undesirable effects~\citep{lichtenstein2012headphone}.
We therefore use a set of bone-conduction headphones that are placed on a user's cheekbones and conduct audio signals through the skull to the inner ear, completely avoiding the outer ear and therefore does not impede the user's ambient sound perception abilities. 
Alternative, open-back headphones were also considered, but it was found that they still interfere with hearing and were therefore disregarded.
The AfterShockz headphones (\cref{fig:participant}) were ultimately selected.
These headphones have the added benefit of a more discreet form-factor when compared to other over-headphones, thereby addressing the issue of user acceptance.}

%\subsection{Audio Interface}

\hl{Humans are able to localise sounds in three dimensions by extracting different cues from the audio signal~\citep{blauert1969sound,blauert1997spatial}.
These include binaural, where the user compares the signals received at both ears (e.g.\ inter-aural time and level differences , ITD and ILD respectively) and monaural cues, where a cue is extracted directly from each ear's audio signal (e.g.\ the spectral profile, audio intensity).
The ITD and ILD extracted from binaural cues allow a user to localise a sound on the horizontal plane (e.g.\ a sound from the right will hit the right ear before left one), while monaural cues indicate a sound's distance (e.g. louder sounds appear closer than softer ones) and its position on the vertical plane (e.g.\ the spectral profile generated from the signal hitting the user's body).
Prior to transmitting an audio signal to the user, it can be adjusted by an HRTF to mimic a natural sound source and make a user believe it is located at some arbitrary 3D location. 
An HRTF is a mathematical function that simulates a human head and ear's response to an external sound and is derived by capturing key characteristics that affect the monaural and binaural responses at the user's ear, such as the hearing level and anatomy. 
An HRTF will produce the most accurate results when it is customised to match a specific user, since each user's hearing response is unique, but this is a complex process and HRTFs generated with average values (head sizes, heights, ear shapes, etc.) produce acceptable results~\citep{gardner1995hrtf}.}

\hl{The interface presents the user with guidance instructions in the form of angular adjustments that are required in the pan and elevation dimensions to point a camera at the desired target (see \cref{fig:participant}).
Spatialised audio is well-suited to this micro-navigation task, displaying accuracy levels comparable to vocal feedback, but with less cognitive load at the high resolution of this task~\citep{klatzky2006cognitive}. 
However, we propose a linear adjustment to the signal's pitch as a function of the elevation angle to overcome the limitations of bone-conduction headphones and spatialised audio stated previously.
The pan angle can still be conveyed with a spatialised audio signal generated by an HRTF.
Indeed, this dimension is unaffected by the use of bone-conduction headphones~\citep{schonstein2008comparison,macdonald2006spatial,stanley2006lateralization,lock2019bone}. 
This interface was implemented and evaluated in~\citet{lock2019bone}.}

%Humans localise a sound source in three dimensions by considering cues recorded in one ear (monaural cues) and comparing cues received at both ears (binaural cues)~\citep{blauert1997spatial,blauert1969sound}.
%The binaural cues include inter-aural time and level differences (ITD and ILD respectively) that help to determine a source's location on the horizontal plane.
%Monaural cues are taken from the interaction of the sound with the human anatomy, e.g.\ head, shoulders, outer ear, before it enters the ear canal.
%When the modified audio signal enters the inner ear canal, the human brain is able to analyse the frequency response and accurately determine the position of the sound source on the median plane. 
%The distance to the source is simply derived as the intensity, or volume, of the source, i.e.\ a louder sound would appear closer to the user than a softer one. 

%When an audio signal is transmitted via a set of speakers or headphones, it can be transformed with an HRTF to mimic the characteristics of a natural sound source before it is transmitted, making the brain believe a sound is located at some arbitrary position.
%An HRTF is a mathematical function that simulates the response signal of a human head and is derived by capturing key characteristics that affect the monaural and binaural responses, such as the user's hearing levels and head size.
%Since hearing responses are unique amongst different users, the best results would be observed if each user had their own customised HRTF.\
%However, given the complex process involved to capture the required user characteristics, making unique HRTFs is often an untenable solution and using average values (e.g.\ head measurements, height, etc.) have shown to produce acceptable results~\citep{gardner1995hrtf}.

%The guidance information is presented to the user in terms of pan and elevation angles, indicating the angular adjustments required to point the device camera at the target location, as shown in \cref{fig:cam-coords}.
%Spatialised audio signals are well-suited to the task, displaying similar levels of performance to vocal feedback, but with less cognitive load and higher resolution~\citep{klatzky2006cognitive}.
%However, given the previously discussed limitations of bone-conduction and spatialised audio, we propose a simple linear adjustment to the signal's pitch as a function of the elevation angle. 
%The pan angle, instead, can be conveyed by transforming the audio signal with an HRTF, and indeed it has been found that this dimension is unaffected by using bone-conduction headphones~\citep{schonstein2008comparison,macdonald2006spatial,stanley2006lateralization,lock2019bone}. 
%This interface was implemented and evaluated by~\citet{lock2019bone}.

\begin{figure}
  \centering
  %\subfloat[]{\label{fig:cam-coords}\includegraphics[width=0.3\columnwidth]{figures/camera_coordinate.pdf}}
%~
  %\subfloat[]{\label{fig:pitch-gain}\includegraphics[clip, trim=0 0 0 40, width=0.7\columnwidth]{figures/pitch_gain_function.png}}
  \includegraphics[clip, trim=0 0 0 40, width=0.7\columnwidth]{figures/pitch_gain_function.png}
  \caption{A plot depicting the pitch gain function used to convey the target's elevation angle. Note the logarithmic scale of the frequency axis. Adapted from~\citet{lock2019bone}. }\label{fig:pitch-gain}
  \Description{The reference system used by the guidance interface showing the camera vector and pan and elevation angles and the pitch gain function used to convey the target's elevation angle. Note the logarithmic scale of the frequency axis. }
\end{figure}

\subsection{Pan Dimension}

\hl{To localise a sound on the horizontal plane, the human audition system compares characteristics from the signals received at both ears (binaural cues), such as their volume difference and the time delay between the same sound reaching both ears~\citep{blauert1969sound}.
For example, a sound placed at a person's right will hit their right ear first with a slightly higher volume compared to the left ear.
Since the binaural cues are largely independent if the signal's spectral profile, it would be convenient to use a simple audio wave to transmit guidance instructions to the user.
Therefore, in this work a pure sine wave was used, but can easily be replaced with a richer tone once the system's characteristics are better understood. 
The since was was spatialised using OpenAL's default HRTF , based on the MIT's KEMAR dataset~\citep{hiebert2005openal}.} %which uses the person and targets' positions as input, and outputs a transformed audio signal.}

\subsection{Elevation Dimension}

\hl{To compensate for the loss of elevation localisation performance when conveying spatialised audio via bone-conduction headphones~\citep{macdonald2006spatial,schonstein2008comparison}, we adjust the sine wave's frequency (i.e.\ audible pitch) as a linear function of the target's elevation angle, as shown in \cref{fig:pitch-gain}.
When the required elevation adjustment is above or below where the camera is currently pointed, the pitch is increased or lowered respectively. 
This high/low scheme was selected based on humans' natural association of high-pitched sounds with elevated objects and vice-versa for low-lying objects~\citep{pratt1930spatial,blauert1997spatial}.
The pitch is constantly adjusted and updated at \SI{10}{\hertz} in octave- and semitone-based intervals to ensure perceptible changes, while keeping the tone's timbre constant~\citep{shepard1964circularity}. }

\hl{The pitch is changed as a linear function of the elevation angle, the gradient of which is determined by setting the angle and pitch limits.
These limits are set at some number of octaves from the neutral pitch that is emitted when the camera is on-target. 
This tone is heuristically set to \SI{512}{\hertz} following practical tests, which is comfortably audible for a large number of octaves changes.
For this work, we only consider a \SI{180}{\degree} field of view in front of the user and limit the elevation angle to a range of \SI{\pm90}{\degree}, or $[-\frac{\pi}{2}, \frac{\pi}{2}]$.
In previous tests with this specific interface, its performance was comparable to normal over-ear headphones transmitting a spatialised signal that transmits the elevation angle~\citep{lock2019bone}.}

\section{Experiments}\label{sec:experiments}

We performed a set of experiments with the audio interface to determine how effective it is at directing a user to adjust the pan and elevation angles of a camera for point it to a target.
Furthermore, we also carried out a set of pre-screening experiments to determine each participant's hearing characteristics in order to determine their perception limits in the respective audio dimensions.
The participants were given time before the experiments commenced to familiarise themselves with the device and the tones it emits, as well as what the `on-target' tone sounds like.
We also tested the system with a group of participants with severe sight impairments and compared their data to the blindfolded participant dataset.
The results from the experiments we performed allow us to better understand how the users respond to different settings for the spatial audio feedback stimulus and use them to improve and optimise the behaviour of the feedback modes in our portable navigation aid.

\subsection{System Setup}

A diagram of the experimental system pipeline is shown in \cref{fig:pipeline}, where the arrows indicate the direction of information flow.
The system implementation used here is similar to the setup used by~\citet{lock2019bone}.
When the user taps the Tango's screen, a new virtual target is generated and its coordinates are sent to the audio generation module, along with the device's current position and orientation.
The audio generator then produces a tone based on the difference between the device and the target's positions.
The tone is sent to the audio output channel, which plays it back to the user.
A WiFi recording module is constantly monitoring the different values of the device's parameters and of the target's position, as well as the system's output, recording everything in a remotely stored datafile. 

Even though the Tango is able to detect distances to object, we opted to use virtual targets given the added ability to place targets at any random location without physical manipulation while without losing the ability to determine its exact position relative to the device.
When this interface is used with a real navigation system, it will be extended to work with real targets.

\begin{figure}
  \centering
  \includegraphics[clip=true, trim=0 120 80 50, width=0.8\columnwidth]{figures/pipeline.pdf}
  \caption{A diagram of the individual system components and their communication pipelines. $F$ indicates a feedback signal and $P$ a pose signal. Adapted from~\citet{lock2019bone}. }\label{fig:pipeline}
  \Description{A diagram of the individual system components and their communication pipelines. $F$ indicates a feedback signal and $P$ a pose signal. }
\end{figure}

\subsection{Participant Characterisation}\label{sec:participant_characterisation}

A preliminary set of experiments were conducted to characterise the participants' hearing characteristics.
The measured characteristics were each participant's audio localisation ability on the lateral plane, as well as the participants' ability to discriminate between tones with different frequencies. 
These results will be used to provide context to the following target search experiment and provide additional insight on any possible biases or limitations. 

\subsubsection{Sound Localisation}\label{sec:sound_localisation}

In this experiment, we evaluated a participant's ability to determine the lateral direction a sound is coming from.
To do this, we played a continuous \SI{512}{\hertz} sinusoidal tone to the participant through the headphones and applied an HRTF to spatialise and place its source to the participant's left or right.
The participant then had to select the direction the sound came from.
The longer the experiment lasts and the more correct guesses the participant makes, the closer the source moves to the centre-front of the participant, making it increasingly harder to localise. 

For this progressive increase in difficulty, a ``2-up, 1-down'' step process is used~\citep{wetherill1965sequential,levitt1971transformed}, meaning that for every two correct answers, the distance to the centre halves.
Conversely, the task becomes easier for each incorrect answer by doubling the sound source's distance from the centre.
We also use two different step sequences, one starting at a large angular distance (\SI{45}{\degree}) from the user and the other at the minimum distance (approximately \SI{1}{\degree}), giving an `easy' and a `hard' progression respectively.
The terminating condition for the experiment is when the two sequences converge to within two intervals of one another for three consecutive guesses.
For example, the experiment will terminate when one sequence is set to \SI{11.25}{\degree} and the other is between \SI{2.8}{\degree} or \SI{45}{\degree} for three consecutive guesses.
This gives a distance band where the participant is capable of localizing the sound source.
Each participant performed this experiment three times. 

\subsubsection{Pitch Discrimination}\label{sec:pitch_discrimination}

Here we determined a participant's ability to differentiate the pitches of two different tones, i.e.\ how well they can tell if a tone is high or low pitched.
We played two tones to the participants in succession, with the second tone being higher or lower-pitched than the first.
The participants were then asked to select which tone was higher or lower.

The first tone is randomly generated, while the second tone is generated by adding or subtracting some value from the first one.
The tone difference depends on how well the participant can tell the tones apart.
Like the sound localisation experiment, a ``2-up, 1-down'' step process is used: for every two consecutive correct answers, the pitch difference between the tones is halved, while it is doubled for every incorrect answer.
Two-step sequences are again used here, one starting with a large pitch difference ($f_h=2^9=$ \SI{512}{\hertz}) between the tones and the other with a small difference ($f_l=2^1=$ \SI{2}{\hertz}).
The termination condition is when the two-step sequences are within one octave of each other (i.e.\ $\log_2\frac{f_h}{f_l}=2$) for three consecutive answers.
For example, the experiment will terminate when one sequence is set to \SI{64}{\hertz} and the other is between \SI{32}{\hertz} or \SI{128}{\hertz} for three consecutive guesses.
Pitch differences are measured in semitones, which can be obtained with

\begin{equation}
\label{eq:semitone-difference}
  \Delta f = 12\log_2\frac{f_0}{f_1}, 
\end{equation}

\noindent where $f_0$ and $f_1$ are the frequencies of the first and second tone respectively.
Each participant performed this experiment twice. 

\subsection{Target Search}\label{sec:target_search}

A set of experiments were conducted to determine the interface's guidance effectiveness for a pointing task.
These experiments captured the difference between the targets' actual directions and the directions the participants perceived them to be located. 
The participants were given a Tango tablet running an app implementing the experimental setup in \cref{fig:pipeline}.
This app generates a set of virtual targets, one at a time, and presents their directions to the participants with audio guidance signals.
These targets are generated at a constant distance from the participants with pan and elevation angles that are uniformly distributed across the four quadrants to avoid clustering.
Each target's relative angular position is communicated to the participants in real-time as the device is moved around via the bone-conducting headphones.
Every time a participant was confident that they were pointing at a target, i.e.\ hearing a signal that is placed at their front at a frequency of \SI{512}{\hertz}, they tapped the device's screen, marking the location and generating a new target.
The targets are all positioned relative to the device's camera coordinate system, which is tracked by the Tango hardware and localisation API.\
Each participant was tasked with finding 28 targets each.

A similar experimental setup was used in~\cite{lock2019bone}.
However, in these experiments we also want to see how changing the gradient of the pitch function, visualised in \cref{fig:pitch-gain}, affects target acquisition performance, e.g.\ does a steeper pitch gain as a function of the elevation angle improve accuracy or decrease the search time?
Pitch limits of one, two and three octaves above and below the neutral tone were then set for the so-called \textit{lo}, \textit{med} and \textit{hi} pitch gradient settings respectively, giving pitch intervals of

\begin{gather*}
  f_{lo}\in[\SI{256}{\hertz}, \SI{1024}{\hertz}]\\
  f_{med}\in[\SI{128}{\hertz}, \SI{2048}{\hertz}]\\
  f_{hi}\in[\SI{64}{\hertz}, \SI{4096}{\hertz}].
\end{gather*}

We use two different metrics to compare the three different pitch gradient settings: acquisition accuracy and search time.
The accuracy is given as the difference between the Tango's orientation at the time the participant confirmed they were on target, and the target's actual orientation.
We separate the results of the elevation and pan dimensions in order to see how the different pitch gradients affect a participant's pointing accuracy. 

We also compare the performance of the three pitch gradient settings in terms of the time it takes each participant to find a target.
However, since each participant was presented with a different, randomly generated set of targets, a direct time comparison is not possible.
\hl{Instead, we use Fitts's Law~\citep{fitts1954information}, modified by MacKenzie for uncertain target sizes and noisy data~\citep{mackenzie1992fitts}, which states that there is a relation between the time it takes to find a target and its index of difficulty ($ID$, the ratio between the distance to the target and its width).
It also provides a so-called `index of performance' ($IP$), which can be used to compare the results between the three different configurations used in the experiments.
Furthermore, if a Fitts-like relationship is indeed found, it can be used as an optimisation metric in the co-adaptive interface described by~\cite{lock2017portable}, where interface parameters (e.g.\ pitch gradient, volume setting, etc.) are automatically tweaked in order to maximise target finding performance.}

Here we briefly summarise the equations and quantities involved in this metric.
Fitts's Law is given by  

\begin{equation}
  \label{eq:fitts-base}
  t = a + b~ID,
\end{equation}

\noindent
where $t$ is the time it takes to find a target, $a$ and $b$ are constants determined through regression and $ID$ is a description of the difficulty of the target, given as logarithmic function of the ratio between the distance to the target and the target's width.
In our case, the targets have no width, since they are points in space, and we therefore use MacKenzie's modified form for $ID$, given by

\begin{equation}
  \label{eq:fitts-id}
  ID = \log_2\left(\frac{\theta}{w_e} + 1\right).
\end{equation}

\noindent
Here $\theta$ is the angular distance between subsequent target centres and $w_e$ is the targets' effective angular width~\citep{welford1968fundamentals}, given by

\begin{equation}
  \label{eq:fitts-we} w_e = \sqrt{2\pi e}~\sigma = 4.133~\sigma,
\end{equation}

\noindent \hl{where $\sigma$ is the standard deviation of the combined pan and elevation error ($x, y$), calculated with}

\begin{equation}
  \sigma = \sqrt{\frac{\sum_{i=1}^{N}\left(\sqrt{(x_i-\bar{x})^2 + (y_i - \bar{y})^2}\right)^2}{N-1}}
\end{equation}

\noindent as proposed by~\citet{wobbrock2011effects}, taken as the angle between the participant's target selection and target's actual angular position.
\hl{The virtual targets have a programmed radius of approximately} \SI{10}{\centi\metre} (equivalent to approximately \SI{0.1}{\radian}) and when the participant pointed the camera within this radius, \SI{512}{\hertz} on-target tone was emitted.
\hl{This is similar to the work by}~\citet{kabbash1995prince}\hl{, where they had participants search for a point-target with an oversized cursor and found that this was similar to a Fitts-like response.
However, the participants were not explicitly informed when they were within this radius and had to use their own subjective judgement of the audio tone.
Since the judgement accuracy will vary across different participants, effectively giving different target sizes, it was necessary to use the effective width as an approximation for the actual target size.
Previous authors have found this to capture the true performance metrics fairly well}~\citep{zhai2004speed}.
Fitts's index of performance, $IP$, can then be calculated using 

\begin{equation}
  \label{eq:fitts-performance}
  IP = \frac{ID}{t}.
\end{equation}

\subsection{Procedure}

Two groups of participants were recruited for the experiments on a volunteer basis. 
\hl{Group~\textit{G1} consisted of 42 undergraduate students (10 male, 32, female) with normal eyesight who were blindfolded for the experiments (mean age: $20\pm2$).
Group \textit{G2} contained 10 people (7 male, 3 female) with severe visual impairments (mean age: $61\pm17$).
Of the latter group, 3 are congenitally blind, while the rest were classified as severely sight impaired later in life.
Of these, 3 participants still have limited light perception with no ability to reliably discern shapes and objects. 
Nevertheless, they were asked to close their eyes during the experiment to. 
None of the participants reported any significant prior experience with electronic navigation aids and none had any hearing or other disabilities that could have influenced their performance in the experiments.
These demographics are summarised in \cref{tab:demographics}.}
Each participant performed three sets of experiments each, with the two characterisation experiments in \cref{sec:participant_characterisation} preceding the final target-search experiment in \cref{sec:target_search}. 
Both groups were given some time before the target-search experiment to familiarise themselves with the system, the audio signal's behaviour and the \SI{512}{\hertz} on-level tone. 
Furthermore, to minimise any potential speed/accuracy biases, we asked the participants to focus on finding the targets without worrying about the time it took to complete the task. 

\begin{table}
  \centering
  \caption{}\label{tab:demographics}
  \begin{tabular}{p{2.7cm}p{1.8cm}p{6cm}}
    \toprule
                                 & Group \textit{G1} & Group \textit{G2} \\\midrule
    Gender [M/F]                 & 10/32    & 7/3 \\
    Age [years]                  & $20\pm2$ & $61\pm17$ \\
    Degrees of Vision Impairment & N/A      & 3 totally blind, 7 severely sight impaired (3 with very limited light perception) \\
    Experience with ETAs         & None     & None \\
    \bottomrule
  \end{tabular}
\end{table}

\section{Results}\label{sec:results}

\subsection{Characterisation of Sound Localisation}

\cref{fig:sound-localisation} shows the results captured from the sound localisation experiment where the participants had to select the direction (left or right) that the tone was being played from. 
It can be seen that the vast majority of guesses for both groups were correct.
For Group~\textit{G1}, most of the errors were made at the minimum distance from the centre, i.e.\ the most difficult to guess correctly, which is the expected behaviour.
This indicates that the participants in \textit{G1} consistently progressed through the distance intervals and we can therefore conclude they had little difficulty determining sound direction.

\begin{figure}
  \centering
  \includegraphics[width=0.8\columnwidth]{figures/sound_localisation.png}
  \caption{Histograms of the participants' guesses of the tone locations that show the correct and incorrect guesses for each bin. }\label{fig:sound-localisation}
  \Description{Histograms of the participants' guesses of the tone locations that show the correct and incorrect guesses for each bin. }
\end{figure}

Group~\textit{G2} also displays a concentration of erroneous guesses in the central interval.
However, it also shows more errors in other distance intervals and a more even progression towards the centre.
This could indicate that, instead of terminating the experiment as described in \cref{sec:sound_localisation}, there was more switching back and forth between the three central intervals. 
These results show that both participant groups are capable of determining a sound source's location with a reasonable level of consistency and accuracy and are in line previous literature, confirming that humans are very adept at localising a sound source, particularly in the pan dimension~\cite{wersenyi2003localization}. 

\subsection{Characterisation of Pitch Discrimination}

The results of the pitch discrimination experiment are shown in \cref{fig:pitch-discrimination}, where bar plots are used to show the proportion of correct to incorrect guesses of which tone was higher pitched for different tone difference intervals. 
For Group~\textit{G1}, we see that their guesses are normally spread around the 0 semitone-difference interval and the highest proportion of incorrect guesses occurs in the $[-0.25, 0.25]$ semitone-difference interval. 
The guesses from Group~\textit{G2} are more concentrated around the centre and the majority of incorrect guesses also occurs in the $[-0.25, 0.25]$ semitone-difference interval.
\hl{The concentration of incorrect guesses around the centre is expected, given the experiment process's progressive increase in difficulty.}

Assuming these differences are normally distributed, we fit a cumulative distribution function (CDF) over each participant's set of results for their correct guesses.
We then used each CDF's parameters to determine a frequency cut-off threshold, where the participant could not longer reliably tell tones apart, which is set to contain 75\% of each participant's correct guesses starting from the easiest to distinguish tones with large frequency differences, to the hardest to distinguish with the smallest differences. 
The median of these threshold values can then be used to estimate the frequency difference at which the entire participant population can no longer tell the difference between two tones. 
It can also be used to improve the interface's frequency profile and performance. 
\cref{fig:pitch-thresholds} shows the threshold distribution, along with the median value, which was found to be 0.29 for Group \textit{G1} and 0.35 for Group \textit{G2}. 

\hl{For the target search experiment, the pitch differences between the \SI{512}{\hertz} on-target tone and the participants' selected tones were collected and their cut-off threshold were determined in a similar way.
Each participant's median tone error for each setting was then plotted alongside the groups' median thresholds and plotted in \cref{fig:pitch-thresholds-hist}.
These data are plotted on a linear Hertz-based scale instead of a semitone scale to highlight the grouping between the three settings.}{}

\begin{figure}
  \centering
  \includegraphics[width=0.8\columnwidth]{figures/pitch_discrimination.png}
  \caption{Histograms of the participants' guesses of which tone was higher pitched that show the correct and incorrect guesses for each bin. }\label{fig:pitch-discrimination}
  \Description{Histograms of the participants' guesses of which tone was higher pitched that show the correct and incorrect guesses for each bin. }
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.8\columnwidth]{figures/pitch_thresholds.png}
  \caption{Distributions of the median cut-off frequency thresholds along with the median 75\% cut-off thresholds. }\label{fig:pitch-thresholds}
  \Description{Distributions of the median cut-off frequency thresholds along with the median 75\% cut-off thresholds. }
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.8\columnwidth]{figures/pitch_thresholds_limits.png}
  \caption{Histogram distributions of the participants' 75\% cut-off thresholds. }\label{fig:pitch-thresholds-hist}
  \Description{Histogram distributions of the participants' 75\% cut-off thresholds. }
\end{figure}

\subsection{Target Search}

The results from the target search experiment shown on the 2D histograms in \cref{fig:target-errors}, where the angular errors in the pan and elevation dimensions are plotted against each other. 
A set of box-plots of the angle errors are also given in \cref{fig:target-boxplot-error} for each audio setting.
The results are summarised in \cref{tab:target-results}.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/target_errors.png}
  \caption{Distributions of the angular errors in the pan and elevation dimensions for the 3 different pitch gradient settings. }\label{fig:target-errors}
  \Description{Distributions of the angular errors in the pan and elevation dimensions for the 3 different pitch gradient settings. }
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/boxplot_target_search_median_error.png}
  \caption{Box-plots of the median pan and elevation errors for each audio setting. }\label{fig:target-boxplot-error}
  \Description{Box-plots of the median pan and elevation errors for each audio setting. }
\end{figure}

\begin{table}
  \centering
  \caption{The average target acquisition error in the pan and elevation dimensions for each participant group. }\label{tab:target-results}
	\begin{tabular}%
	{>{\centering\arraybackslash}p{0.5cm}%
	 >{\centering\arraybackslash}p{1.2cm}%
	 >{\centering\arraybackslash}p{0.9cm}%
 	 >{\raggedleft\arraybackslash}p{2.2cm}%
 	 >{\raggedright\arraybackslash}p{2.6cm}%
	 >{\raggedright\arraybackslash}p{2.6cm}}
    \toprule
    &           & Setting      & Mean Angle Error [rad] & Mean Absolute Angle Error [rad] &  Pearson Correlation \\ \midrule
    \multirow{6}{*}{\textit{G1}}
    &           & \textit{lo}  & $-0.02\pm0.37$ & $0.25\pm0.27$ & $0.75,~p < 0.001$ \\
    & Pan       & \textit{med} & $-0.01\pm0.37$ & $0.26\pm0.27$ & $0.77,~p < 0.001$ \\
    &           & \textit{hi}  & $-0.03\pm0.39$ & $0.26\pm0.29$ & $0.72,~p < 0.001$ \\ \cline{2-6}
    &           & \textit{lo}  & $-0.12\pm0.51$ & $0.42\pm0.31$ & $0.36,~p < 0.001$ \\
    & Elevation & \textit{med} & $-0.11\pm0.41$ & $0.44\pm0.24$ & $0.49,~p < 0.001$ \\
    &           & \textit{hi}  & $-0.15\pm0.44$ & $0.36\pm0.29$ & $0.48,~p < 0.001$ \\ \midrule
    \multirow{6}{*}{\textit{G2}}
    &           & \textit{lo}  & $-0.01\pm0.37$ & $0.48\pm0.31$ & $0.10,~p = 0.03$  \\
    & Pan       & \textit{med} & $ 0.04\pm0.53$ & $0.45\pm0.27$ & $0.13,~p = 0.01$  \\
    &           & \textit{hi}  & $ 0.03\pm0.48$ & $0.36\pm0.22$ & $0.21,~p < 0.001$ \\\cline{2-6}
    &           & \textit{lo}  & $-0.30\pm0.59$ & $0.49\pm0.39$ & $0.03,~p = 0.48$  \\
    & Elevation & \textit{med} & $-0.42\pm0.45$ & $0.42\pm0.33$ & $0.31,~p < 0.001$ \\
    &           & \textit{hi}  & $-0.37\pm0.43$ & $0.36\pm0.32$ & $0.40,~p < 0.001$ \\ 
    \bottomrule
  \end{tabular}
\end{table}

The Shapiro-Wilkes test for normality reveals that none of these distributions are normally spread. Therefore, the Pearson test is used to investigate the correlation between the actual target location and participants' pointing location.
These results are included in \cref{tab:target-results}.
\hl{The Pearson correlation scores for Group~\textit{G1} indicate a moderate to strong positive correlation between the target and the selected locations ($r_{pan} \in [0.72, 0.77],~p < 0.001;~r_{elevation} \in [0.36, 0.49],~p < 0.001$), showing that both the pan and elevation cues in general worked as expected.}
However, the correlation scores for Group~\textit{G2} are significantly weaker, with a pan angle correlation $r_{pan} \in [0.1, 0.21],~p < 0.03$.
With the exception of the \textit{lo} setting ($p_{lo} = 0.48$), the elevation correlation is generally stronger, with $r_{elevation} \in [0.31, 0.40],~p < 0.001$.

The repeated-measures procedure that was used for these experiments requires the data for each participant to be grouped together for each setting.
The medians of these data groupings are then used as individual samples that represent an individual participant's performance for each setting.
\cref{fig:target-boxplot-error} shows these median data collected from each participant as a set of box-plots, while \cref{fig:target-boxplot-absolute-errors} shows the collection of absolute errors.

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/boxplot_target_search_absolute_median_error.png}
  \caption{Distributions of the absolute angular errors in the pan and elevation dimensions for the 3 different pitch gradient settings. }\label{fig:target-boxplot-absolute-errors}
  \Description{Distributions of the absolute angular errors in the pan and elevation dimensions for the 3 different pitch gradient settings. }
\end{figure}

The box-plots in \cref{fig:target-boxplot-error} show that the error in the pan dimension is approximately centred around \SI{0}{\radian} for both groups, with some divergence between the groups for the different settings.
However, using the Friedman test for repeated measures on the medians of absolute errors, these divergences are found to be not significant ($p_{G1} = 0.17,~p_{G2} = 0.09$), showing that spatial perception and accuracy are not affected by changes in the tone's pitch.
This is further demonstrated in the box-plots in \cref{fig:target-boxplot-absolute-errors}, which demonstrates relatively consistent error levels in the pan dimension for both groups and across all three settings. 

Regarding the errors in the elevation dimension in \cref{fig:target-boxplot-error}, we observe in Group~\textit{G1} a narrowing distribution between the \textit{lo}, \textit{med} and \textit{hi} settings, respectively, and a median gradually approaching \SI{0}{\radian}.
A similar trend is observed for Group~\textit{G2}, but the improvement across the settings is more subtle and not as linear as for Group~\textit{G1}.
\cref{fig:target-boxplot-absolute-errors} shows a clearer improvement, i.e.\ approaching \SI{0}{\radian}, for the elevation data between the three settings in both groups, with the \textit{hi} setting producing the smallest error in both cases.
Further analysis of the medians of the absolute elevation error with the Friedman test reveals that the results for the different settings are significantly different from each other only for Group~\textit{G1} ($p_{G1} = 0.002,~p_{G2} = 0.32$).

A post-hoc analysis using the Wilcoxon signed rank test, with a Holm-Bonferroni correction applied to the commonly used 0.05 threshold, was used to investigate the setting relationships more closely. 
This analysis reveals that there is a significant difference between the errors generated by the \textit{lo} and \textit{med} settings, as well as the \textit{lo} and \textit{hi} settings, for Group~\textit{G1} ($p_{lo-med} = 0.003,~p_{lo-hi} < 0.001$), showing that the \textit{lo} setting clearly produces the highest error, while it is not clear which one of the \textit{med} and \textit{hi} settings is better for Group~\textit{G1}. 
Based on the current data, it is impossible to conclude which setting produces the smallest angular error for Group~\textit{G2}, but this may because of the relatively small sample size for each setting. 
\hl{Furthermore, the significant variance in the error results for Group \textit{G2} could possibly be because of the higher mean age compared to Group \textit{G1} and their general inexperience with mobile phones.
Indeed, previous works have suggested that these demographics may have a measurable effect on target-finding performance~\citep{millar1994understanding,pring2008psychological}.
However, comparing the distributions for each setting between the two groups with the Kruskal-Wallis test for non-parametric data, we see that the differences between the distributions for all three settings are not significantly different, for both groups and both pan and elevation (the $p$-values are summarised in \cref{tab:inter-group-results}).}
The similarity between the distributions is further highlighted by a two-sample Kolmogorov-Smirnov test (the $p$-values are listed in~\cref{tab:inter-group-results}).
\hl{This result confirms that the performance of the blindfolded participants and those with severe sight impairments are statistically similar despite the seemingly clear differences in error performance.
Therefore, groups from a different population can reasonably be expected to produce similar errors under similar experimental conditions. 
We also note that there is a significant negative error bias in the elevation data for all the settings and groups, possibly caused by a cognitive constraint introduced by the floor, below which the participants believed the targets could not appear.
Since this bias seems to be constant, it could be easily addressed in a future version of the audio interface by adjusting its frequency parameters to shift the bias upwards by a constant offset. 
Finally, we can conclude that the \textit{hi} setting, which generates the significantly smallest elevation error, is the best audio pitch level to guide a user in a pointing task, and that the pan error is completely independent of such setting choice. }

\begin{table}
  \centering
  \caption{A summary of the $p$-values from the Kruskal-Wallis and Kolmogorov-Smirnov tests comparing the distributions of the different settings' error data for each group in both the pan and elevation dimensions. }\label{tab:inter-group-results}
  \begin{tabular}{lcccc}
    \toprule
		   & \multicolumn{2}{c}{Kruskal-Wallis} & \multicolumn{2}{c}{Kolmogorov-Smirnov} \\
		   & Pan      & Elevation & Pan      & Elevation \\ \midrule
      \textit{lo}  & $p=0.18$ & $p=0.90$  & $p=0.07$ & $p=0.91$  \\
      \textit{med} & $p=0.86$ & $p=0.34$  & $p=0.90$ & $p=0.36$  \\
      \textit{hi}  & $p=0.28$ & $p=0.38$  & $p=0.61$ & $p=0.31$  \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Time-to-Target}

To investigate if the interface generates a Fitts-like response from the participants, we plot the time to find the target as a function of the targets' indices of difficulty, as defined by \cref{eq:fitts-base}.
The data is binned in intervals of the effective target width~($w_e$), given by \cref{eq:fitts-we}, and are plotted for each pitch gradient setting. 
A logarithmic line is fitted through the bins' median values by regression and all the results are presented in \cref{fig:fitts-results}.

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/fitts_fit.png}
  \caption{Plots showing the Fitts relationship between the time it took the participants to find a target and the target's index of difficulty. }\label{fig:fitts-results}
  \Description{Plots showing the Fitts relationship between the time it took the participants to find a target and the target's index of difficulty. }
\end{figure}

For Group~\textit{G1}, a Fitts relationship can be observed and the logarithmic line of best fit closely approximates the median values of the binned data for all three settings. 
\hl{This is confirmed by strong Pearson correlation scores for each setting ($r_{lo} = 0.98,~p_{lo}<0.001;~r_{med}=0.94,~p_{med}<0.001;~r_{hi} = 0.97,~p_{hi}<0.001$).
Regarding Group~\textit{G2} we observe larger spreads for each binned data interval, indicating less consistency in the time-to-target results for participants with severe sight impairments.
This could be due to each participant's result being taken as a single datum and to the smaller population size in Group~\textit{G2}, as well as the mean age and general level of expertise with mobile devices of this particular group.
Nevertheless, all three settings exhibit strong Pearson correlation scores ($r_{lo}=0.71,~p=0.005;r_{med}=0.85,~p_{med}<0.001;~r_{hi}=0.84,~p_{hi}<0.001$).}

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/fitts_ips.png}
  \caption{Plots showing the Fitts relationship between the time it took the participants to find a target and the target's index of difficulty. }\label{fig:fitts-ips}
  \Description{Plots showing the Fitts relationship between the time it took the participants to find a target and the target's index of difficulty. }
\end{figure}

These results allows us to calculate and plot in \cref{fig:fitts-ips} an index of performance, as given by \cref{eq:fitts-performance}, for each audio setting.
The results are summarised in \cref{tab:fitts-results}.
For Group~\textit{G1}, there is a fairly consistent level of performance between the three settings, with \textit{med} producing the highest indices of performance overall (i.e.\ the participants found the targets with the smallest error and in the least amount of time).
\hl{This is supported by the results from the Friedman test, showing that there is a significant difference in performance between the settings ($p < 0.001$), as well as post-hoc Wilcoxon tests with Holm-Bonferroni corrections, which show that the \textit{med} setting is significantly different to the \textit{lo} and \textit{hi} settings ($p_{med-lo} < 0.001,~p_{med-hi} < 0.001$). 
The \textit{lo} and \textit{hi} settings, instead, are not significantly different from each other ($p_{lo-hi} = 0.11$).
The results for Group~\textit{G2} show generally lower indices of performance for each setting, which is expected given the increased times to target observed in \cref{fig:fitts-results}.
%However, the \emph{hi} setting's result is an exception here and has a far smaller gap between it and its counterpart in Group \emph{G1}.
Following visual inspection of the distribution and significant results from the Friedman test ($p < 0.001$), it is clear that the \textit{hi} setting produces the highest performance by a large margin, compared to the \textit{lo} and \textit{med} settings. 
This is supported by Wilcoxon tests ($p_{lo-hi}=0.01$, $p_{med-hi}=0.01$), which also show that the results for the \textit{lo} and \textit{med} settings are not different ($p_{lo-med}=0.09$).}

%This seems to indicate that, for both groups, the \textit{lo} setting produces the highest level of performance, followed by the \textit{hi} setting.

\begin{table}
  \centering
  \caption{The average target acquisition error in the pan and elevation dimensions for each participant group. }\label{tab:fitts-results}
  \begin{tabular}{p{0.5cm}p{1.5cm}p{2.5cm}}
    \toprule
                & Setting      & Mean Index of Performance \\ \midrule
                & \textit{lo}  & $0.054\pm0.005$ \\
    \textit{G1} & \textit{med} & $0.057\pm0.006$ \\
		& \textit{hi}  & $0.054\pm0.007$ \\ \cline{2-3}
                & \textit{lo}  & $0.039\pm0.007$ \\
    \textit{G2} & \textit{med} & $0.038\pm0.007$ \\
                & \textit{hi}  & $0.048\pm0.008$ \\
    \bottomrule
  \end{tabular}
\end{table}

\cref{fig:fitts-ips} shows a significant difference between the IPs for each group's respective settings, with \textit{G2} producing significantly lower indices of performance.
This is further supported by a Kruskal-Wallis tests that reveals that each setting's distribution is indeed significantly different from its counterpart in the other group.
The significant difference between the blindfolded group and the group of participants with visual impairments seems to indicate that the latter require significantly more time to find the target. 
However, it is unclear whether this is a systematic cause or a difference in search strategy between the two groups, e.g.\ \textit{G2} preferring, on average, a slower and more methodical approach.

\subsection{Discussion}

\hl{Since Fitts's model we used here is based on the target estimation errors, it is reasonable to expect that the accuracy and time performances will follow similar trends. 
Indeed, this seems to be the case, with the \textit{hi} pitch setting produces the lowest target acquisition error, followed by the \textit{med} and \textit{lo} settings, respectively. 
This trend continues in the time-to-target results obtained with the Fitts model analysis, where the \textit{med} setting gave the marginally highest level of performance in Group \textit{G1} and \textit{hi} for Group \textit{G2}.
However, the improvement of the latter settings are far clearer in Group \textit{G2} than for \textit{G1}.
Indeed, Group \textit{G2} seems to respond better to the increased movement resolution that the \textit{hi} affords the use, allowing for finer adjustments to be made to the device's orientation to get closer to the target.
With the Fitts model discussed here, we can empirically evaluate any changes we make to the audio interface and optimise the parameters it to produce the desired output. }
%However, this trend is almost wholly reversed in the time-to-target results from the Fitts model, where the \textit{lo} setting gives the highest level of performance, followed by the \textit{hi} and \textit{med} settings respectively. 
%Since Fitts's model takes the angular error into account, one might reasonably expect that the results for both experiments follow a similar trend. 
%However, the Fitts model does not account for changing stimuli and different strategies.
%We therefore hypothesise that the reason for this divergence in performance is due to the increased resolution of the \textit{hi} setting, which allows for finer adjustments of the device's orientation, getting it closer to the correct target, but at the cost of a higher average time-to-target. 
%This seems to indicate a speed/accuracy trade-off in finding the targets.
%With the Fitts model discussed here, we can modify the interface to prioritise different metrics and produce the correct output. 

Regarding target acquisition, the progressive improvement from the \textit{lo}, \textit{med} and \textit{hi} settings (see \cref{tab:target-results}) seems to indicate that simply increasing the pitch gradient leads to better target-pointing performance.
However, \cref{fig:pitch-thresholds-hist} shows that the frequency difference between the ``on-target'' tone and the selected one with the \textit{hi} setting approaches the cut-off frequency of Group~\textit{G1}, indicating an inflection point where increasing the gradient reduces the final performance. 
Indeed, the participants from Group~\textit{G2} seem to go beyond this threshold and reach a saturation point where they can no longer reliably distinguish different tones.  

\section{Conclusion}\label{sec:conclusion}

In this paper we investigated the use of spatialised audio interface with varying pitch to guide a user with visual impairments in a target pointing task.
We found that the blindfolded participants and those with severe sight impairments performed similarly in localizing sound sources and differentiating between tones. 
We also found that both groups were able to find a randomly distributed set of virtual targets with similar levels of accuracy.
However, the blindfolded group outperformed the one with severe sight impairments in terms of time-to-target. 
We further tested different pitch settings and found that the user performance in the pan dimension, based on spatialised cues, is independent of such settings.
Moreover, we noticed a speed/accuracy trade-off between the settings, where a higher pitch setting produces a smaller angular error, but at the cost of reducing the time performance (i.e.\ more time to reach the target). 
These results, together with an analysis done with Fitts's Law that confirms its applicability to this type of audio interface, provide a useful baseline to improve and refine the latter in future applications, prioritising speed or accuracy to produce the desired output.

This work identified a number of uncertainties that can be the focus of future work.
These include questions such as what caused the observed difference in time performance between the groups and whether the constant negative bias observed in \cref{fig:target-boxplot-error} is indeed caused by a cognitive bias, or whether there is a more complex underlying reason for the behaviour.
Furthermore, casual post-experiment conversations with the participants revealed that some felt that one setting was easier to understand than the others and it can therefore be beneficial to investigate the possibility of adding an auto-adaptation component to the audio signal.
For example, the pitch gradient can be automatically adjusted over time by the device to provide a better match between the human and computer and increase overall target-finding performance.
Indeed, the work by~\citet{gallina2015progressive} may serve as a good guideline for such a system.
Additional feedback modes may also be added to allow for a clearer guidance experience for the user, e.g.\ vibration signals or another tone~\citep{marentakis2006effects}, to explicitly inform them when they are pointing to the target or by adjusting the volume to expand the system to three dimensions.
With this kind of audio interface now better understood, it is ready to be implemented into a fully implemented guidance system for people with visual impairments.

\bibliographystyle{ACM-Reference-Format}
\bibliography{bib}

\end{document}
